# GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒæŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
3. [è®­ç»ƒè„šæœ¬ä½¿ç”¨](#è®­ç»ƒè„šæœ¬ä½¿ç”¨)
4. [å‚æ•°é…ç½®è¯¦è§£](#å‚æ•°é…ç½®è¯¦è§£)
5. [è®­ç»ƒä¼˜åŒ–æŒ‡å—](#è®­ç»ƒä¼˜åŒ–æŒ‡å—)
6. [è¯„ä¼°å’Œæµ‹è¯•](#è¯„ä¼°å’Œæµ‹è¯•)
7. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
8. [è¿›é˜¶ä¼˜åŒ–](#è¿›é˜¶ä¼˜åŒ–)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

é¦–å…ˆç¡®ä¿æ‚¨å·²ç»å®Œæˆäº†åŸºç¡€ç¯å¢ƒæ­å»ºï¼š

```bash
# 1. æ„å»ºC++æ ¸å¿ƒåº“
# åœ¨VS Codeä¸­è¿è¡Œä»»åŠ¡: "Build Main Application"
# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ:
cd d:\Coding\Projects\ai-devour-evolve
D:\qt\Tools\CMake_64\bin\cmake.exe --build build --config Release

# 2. å®‰è£…Pythonä¾èµ–
pip install stable-baselines3
pip install gymnasium
pip install numpy
pip install rich  # ç”¨äºç¾åŒ–ç•Œé¢
```

### 2. å¿«é€Ÿè®­ç»ƒ

```bash
# è¿›å…¥è„šæœ¬ç›®å½•
cd scripts

# è¿è¡Œç®€åŒ–ç•Œé¢è®­ç»ƒï¼ˆæ¨èåˆå­¦è€…ï¼‰
python train_simple_ui.py

# æˆ–è¿è¡ŒRichç¾åŒ–ç•Œé¢è®­ç»ƒ
python train_rich_ui.py

# æˆ–è¿è¡ŒåŸºç¡€è®­ç»ƒè„šæœ¬
python train_rl_agent.py
```

### 3. éªŒè¯è®­ç»ƒæ•ˆæœ

```bash
# æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
python test_agent_score.py
```

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### Pythonç¯å¢ƒè®¾ç½®

é¡¹ç›®ä½¿ç”¨ä»¥ä¸‹æ ¸å¿ƒåº“ï¼š

- **stable-baselines3**: PPOå¼ºåŒ–å­¦ä¹ ç®—æ³•
- **gymnasium**: æ ‡å‡†åŒ–RLç¯å¢ƒæ¥å£
- **numpy**: æ•°å€¼è®¡ç®—
- **rich**: ç»ˆç«¯ç¾åŒ–ç•Œé¢ï¼ˆå¯é€‰ï¼‰

### C++æ„å»ºç¡®è®¤

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
```
build/Release/
â”œâ”€â”€ gobigger_env.pyd          # Pythonç»‘å®šåº“
â”œâ”€â”€ ai-devour-evolve.exe      # ä¸»ç¨‹åº
â””â”€â”€ *.dll                     # ä¾èµ–åº“
```

---

## ğŸ® è®­ç»ƒè„šæœ¬ä½¿ç”¨

### 1. train_simple_ui.pyï¼ˆæ¨èåˆå­¦è€…ï¼‰

**ç‰¹ç‚¹**: å…¼å®¹æ€§å¼ºï¼ŒASCIIç¾åŒ–ç•Œé¢ï¼Œå®æ—¶è¿›åº¦æ˜¾ç¤º

```bash
python train_simple_ui.py
```

**ç•Œé¢ç¤ºä¾‹**:
```
ğŸ® GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
è®­ç»ƒè¿›åº¦: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% (20000/20000)
Episodes: 15 | æœ€ä½³åˆ†æ•°: 1250 | å¹³å‡åˆ†æ•°: 890
FPS: 1200 | æ—¶é•¿: 18s
```

### 2. train_rich_ui.pyï¼ˆæ¨èæœ‰Richç¯å¢ƒï¼‰

**ç‰¹ç‚¹**: ç²¾ç¾è¡¨æ ¼ç•Œé¢ï¼Œå®æ—¶ç»Ÿè®¡ï¼Œå¤šçº¿ç¨‹æ›´æ–°

```bash
python train_rich_ui.py
```

**ç•Œé¢ç‰¹è‰²**:
- ğŸ¯ å®æ—¶åˆ†æ•°ç»Ÿè®¡è¡¨æ ¼
- ğŸ“Š è®­ç»ƒæŒ‡æ ‡ç›‘æ§
- ğŸ“ˆ å¯è§†åŒ–è¿›åº¦æ¡
- ğŸ† æœ€ä½³æˆç»©è¿½è¸ª

### 3. train_rl_agent.pyï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰

**ç‰¹ç‚¹**: å‘½ä»¤è¡Œè¾“å‡ºï¼Œç®€å•ç›´æ¥

```bash
python train_rl_agent.py
```

---

## âš™ï¸ å‚æ•°é…ç½®è¯¦è§£

### 1. ç¯å¢ƒå‚æ•°é…ç½®

åœ¨ `scripts/gobigger_gym_env.py` ä¸­ï¼š

```python
# ç¯å¢ƒåŸºç¡€é…ç½®
default_config = {
    'max_episode_steps': 1500,     # æ¯å±€æœ€å¤§æ­¥æ•°
}

# å¥–åŠ±å‡½æ•°å‚æ•°
class GoBiggerEnv:
    def __init__(self, config=None):
        # å¥–åŠ±æƒé‡
        self.score_reward_scale = 0.01      # åˆ†æ•°å¥–åŠ±ç¼©æ”¾
        self.food_reward = 0.5              # åƒé£Ÿç‰©å¥–åŠ±
        self.survival_reward = 0.001        # å­˜æ´»å¥–åŠ±
        self.death_penalty = -5.0           # æ­»äº¡æƒ©ç½š
        self.border_penalty = -0.1          # æ’è¾¹ç•Œæƒ©ç½š
```

#### å…³é”®å‚æ•°è¯´æ˜ï¼š

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒæ•´å»ºè®® |
|------|--------|------|----------|
| `max_episode_steps` | 1500 | æ¯å±€æ¸¸æˆæœ€å¤§æ­¥æ•° | å¢åŠ è®­ç»ƒæ›´ä¹…ï¼Œå‡å°‘è®­ç»ƒæ›´å¿« |
| `score_reward_scale` | 0.01 | åˆ†æ•°å˜åŒ–çš„å¥–åŠ±ç¼©æ”¾ | æé«˜è®©æ™ºèƒ½ä½“æ›´å…³æ³¨åˆ†æ•° |
| `food_reward` | 0.5 | æˆåŠŸåƒåˆ°é£Ÿç‰©çš„é¢å¤–å¥–åŠ± | å¢åŠ é¼“åŠ±è§…é£Ÿè¡Œä¸º |
| `survival_reward` | 0.001 | æ¯æ­¥å­˜æ´»çš„åŸºç¡€å¥–åŠ± | é¿å…æ™ºèƒ½ä½“è¿‡æ—©æ­»äº¡ |
| `death_penalty` | -5.0 | æ­»äº¡æ—¶çš„æƒ©ç½š | å¢åŠ ç»å¯¹å€¼è®©æ™ºèƒ½ä½“æ›´è°¨æ… |

### 2. PPOç®—æ³•å‚æ•°é…ç½®

åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼š

```python
model = PPO(
    "MlpPolicy",                    # ç­–ç•¥ç½‘ç»œç±»å‹
    env,
    learning_rate=3e-4,             # å­¦ä¹ ç‡
    n_steps=1024,                   # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
    batch_size=64,                  # æ‰¹å¤„ç†å¤§å°
    n_epochs=10,                    # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
    gamma=0.99,                     # æŠ˜æ‰£å› å­
    gae_lambda=0.95,                # GAEå‚æ•°
    clip_range=0.2,                 # PPOè£å‰ªèŒƒå›´
    ent_coef=0.01,                  # ç†µæ­£åˆ™åŒ–ç³»æ•°
    vf_coef=0.5,                    # ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°
    verbose=1
)
```

#### PPOå‚æ•°è°ƒæ•´æŒ‡å—ï¼š

| å‚æ•° | é»˜è®¤å€¼ | ä½œç”¨ | è°ƒæ•´ç­–ç•¥ |
|------|--------|------|----------|
| `learning_rate` | 3e-4 | å­¦ä¹ é€Ÿåº¦ | è¿‡æ…¢å¢åŠ ï¼Œä¸ç¨³å®šå‡å°‘ |
| `n_steps` | 1024 | ç»éªŒæ”¶é›†é‡ | å¢åŠ æå‡ç¨³å®šæ€§ï¼Œå‡å°‘åŠ å¿«è®­ç»ƒ |
| `batch_size` | 64 | æ‰¹å¤„ç†å¤§å° | æ ¹æ®å†…å­˜å’Œç¨³å®šæ€§è°ƒæ•´ |
| `n_epochs` | 10 | æ¯æ‰¹æ•°æ®è®­ç»ƒè½®æ•° | å¢åŠ æå‡å­¦ä¹ ï¼Œä½†å¯èƒ½è¿‡æ‹Ÿåˆ |
| `gamma` | 0.99 | æœªæ¥å¥–åŠ±æŠ˜æ‰£ | æ¥è¿‘1é‡è§†é•¿æœŸï¼Œè¿œç¦»1é‡è§†çŸ­æœŸ |
| `clip_range` | 0.2 | ç­–ç•¥æ›´æ–°é™åˆ¶ | å‡å°‘æ›´ä¿å®ˆï¼Œå¢åŠ æ›´æ¿€è¿› |

### 3. è®­ç»ƒæ€»æ­¥æ•°é…ç½®

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä¿®æ”¹
total_timesteps = 20000    # æ€»è®­ç»ƒæ­¥æ•°
```

**å»ºè®®è®¾ç½®**:
- **å¿«é€Ÿæµ‹è¯•**: 5,000 - 10,000 æ­¥
- **æ­£å¸¸è®­ç»ƒ**: 20,000 - 50,000 æ­¥  
- **æ·±åº¦è®­ç»ƒ**: 100,000+ æ­¥

---

## ğŸ“ˆ è®­ç»ƒä¼˜åŒ–æŒ‡å—

### 1. åˆå­¦è€…ä¼˜åŒ–è·¯å¾„

#### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®­ç»ƒ
```python
# ä¿å®ˆé…ç½®ï¼Œç¡®ä¿ç¨³å®šè®­ç»ƒ
model = PPO(
    "MlpPolicy", env,
    learning_rate=1e-4,        # è¾ƒå°å­¦ä¹ ç‡
    n_steps=512,               # è¾ƒå°‘æ­¥æ•°
    batch_size=32,             # è¾ƒå°æ‰¹æ¬¡
    n_epochs=5,                # è¾ƒå°‘è½®æ•°
    verbose=1
)
```

#### ç¬¬äºŒé˜¶æ®µï¼šå‚æ•°è°ƒä¼˜
è§‚å¯Ÿè®­ç»ƒæ•ˆæœåè°ƒæ•´ï¼š
- å¦‚æœè®­ç»ƒå¤ªæ…¢ â†’ å¢åŠ  `learning_rate` åˆ° 3e-4
- å¦‚æœä¸ç¨³å®š â†’ å‡å°‘ `learning_rate` åˆ° 1e-5
- å¦‚æœå†…å­˜å……è¶³ â†’ å¢åŠ  `n_steps` åˆ° 1024

#### ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§ä¼˜åŒ–
```python
# ä¼˜åŒ–åé…ç½®
model = PPO(
    "MlpPolicy", env,
    learning_rate=3e-4,
    n_steps=2048,              # æ›´å¤šç»éªŒ
    batch_size=128,            # æ›´å¤§æ‰¹æ¬¡
    n_epochs=15,               # æ›´å¤šè®­ç»ƒ
    gamma=0.995,               # æ›´é‡è§†é•¿æœŸå¥–åŠ±
    gae_lambda=0.98,
    ent_coef=0.005,            # å‡å°‘æ¢ç´¢
    verbose=1
)
```

### 2. ç¯å¢ƒå‚æ•°ä¼˜åŒ–

#### å¥–åŠ±å‡½æ•°è°ƒä¼˜
```python
# åœ¨ gobigger_gym_env.py ä¸­ä¿®æ”¹
class GoBiggerEnv:
    def __init__(self, config=None):
        # é˜¶æ®µ1ï¼šåŸºç¡€å¥–åŠ±
        self.score_reward_scale = 0.01
        self.food_reward = 0.5
        
        # é˜¶æ®µ2ï¼šå¦‚æœæ™ºèƒ½ä½“ä¸å¤Ÿç§¯æè§…é£Ÿ
        self.score_reward_scale = 0.02     # å¢åŠ åˆ†æ•°å¥–åŠ±
        self.food_reward = 1.0             # å¢åŠ é£Ÿç‰©å¥–åŠ±
        
        # é˜¶æ®µ3ï¼šå¦‚æœæ™ºèƒ½ä½“è¿‡äºå†’é™©
        self.death_penalty = -10.0         # å¢åŠ æ­»äº¡æƒ©ç½š
        self.border_penalty = -0.5         # å¢åŠ è¾¹ç•Œæƒ©ç½š
```

#### Episodeé•¿åº¦è°ƒä¼˜
```python
# çŸ­è®­ç»ƒï¼šå¿«é€ŸéªŒè¯ç®—æ³•
config = {'max_episode_steps': 500}

# æ ‡å‡†è®­ç»ƒï¼šå¹³è¡¡æ•ˆæœå’Œæ—¶é—´
config = {'max_episode_steps': 1500}

# é•¿è®­ç»ƒï¼šæ·±åº¦ç­–ç•¥å­¦ä¹ 
config = {'max_episode_steps': 3000}
```

### 3. ç›‘æ§è®­ç»ƒæ•ˆæœ

#### å…³é”®æŒ‡æ ‡è§‚å¯Ÿ
1. **Episodeåˆ†æ•°è¶‹åŠ¿**ï¼šåº”è¯¥é€æ¸ä¸Šå‡
2. **Episodeé•¿åº¦**ï¼šåº”è¯¥è¶‹äºç¨³å®šæˆ–å¢é•¿
3. **å¥–åŠ±å‡å€¼**ï¼šåº”è¯¥æŒç»­æ”¹å–„
4. **ç­–ç•¥ç†µ**ï¼šåˆæœŸé«˜ï¼ŒåæœŸé€æ¸é™ä½

#### è®­ç»ƒæ•ˆæœåˆ¤æ–­
```bash
# å¥½çš„è®­ç»ƒè¿¹è±¡ï¼š
âœ… å¹³å‡åˆ†æ•°æŒç»­ä¸Šå‡
âœ… æ™ºèƒ½ä½“èƒ½ç¨³å®šåƒåˆ°é£Ÿç‰© 
âœ… Episodeé•¿åº¦é€æ¸å¢åŠ 
âœ… æ­»äº¡é¢‘ç‡é€æ¸é™ä½

# éœ€è¦è°ƒæ•´çš„è¿¹è±¡ï¼š
âŒ åˆ†æ•°é•¿æœŸä¸å˜æˆ–ä¸‹é™
âŒ æ™ºèƒ½ä½“é¢‘ç¹æ­»äº¡
âŒ è®­ç»ƒlossä¸æ”¶æ•›
âŒ æ™ºèƒ½ä½“è¡Œä¸ºå¼‚å¸¸ï¼ˆå¦‚åªç§»åŠ¨ä¸è§…é£Ÿï¼‰
```

---

## ğŸ§ª è¯„ä¼°å’Œæµ‹è¯•

### 1. å¿«é€Ÿè¯„ä¼°è„šæœ¬

```bash
# æµ‹è¯•æ™ºèƒ½ä½“åˆ†æ•°è¡¨ç°
python test_agent_score.py

# è§‚å¯Ÿæ™ºèƒ½ä½“æ¸¸æˆè¿‡ç¨‹
python debug_observation.py
```

### 2. è‡ªå®šä¹‰è¯„ä¼°

```python
# åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°è„šæœ¬
from stable_baselines3 import PPO
from gobigger_gym_env import GoBiggerEnv

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = PPO.load("models/PPO_gobigger.zip")
env = GoBiggerEnv()

# è¿è¡Œè¯„ä¼°
episodes = 10
total_scores = []

for i in range(episodes):
    obs, _ = env.reset()
    episode_score = 0
    done = False
    
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        if 'final_score' in info:
            episode_score = info['final_score']
    
    total_scores.append(episode_score)
    print(f"Episode {i+1}: Score = {episode_score:.0f}")

print(f"\nå¹³å‡åˆ†æ•°: {np.mean(total_scores):.0f}")
print(f"æœ€ä½³åˆ†æ•°: {np.max(total_scores):.0f}")
```

### 3. æ€§èƒ½åŸºå‡†

#### æ™ºèƒ½ä½“æ°´å¹³åˆ¤æ–­ï¼š
- **åˆå­¦è€…çº§åˆ«**: å¹³å‡åˆ†æ•° < 500
- **ä¸­ç­‰æ°´å¹³**: å¹³å‡åˆ†æ•° 500-1500
- **é«˜çº§æ°´å¹³**: å¹³å‡åˆ†æ•° 1500-3000
- **ä¸“å®¶çº§åˆ«**: å¹³å‡åˆ†æ•° > 3000

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. å¯¼å…¥é”™è¯¯
```bash
# é—®é¢˜ï¼šModuleNotFoundError: No module named 'gobigger_env'
# è§£å†³ï¼šç¡®ä¿C++åº“å·²æ­£ç¡®æ„å»º
cd d:\Coding\Projects\ai-devour-evolve
D:\qt\Tools\CMake_64\bin\cmake.exe --build build --config Release
```

#### 2. Richåº“å®‰è£…é—®é¢˜
```bash
# é—®é¢˜ï¼šå¯¼å…¥richå¤±è´¥
# è§£å†³ï¼š
pip install rich

# æˆ–ä½¿ç”¨conda
conda install rich -c conda-forge
```

#### 3. è®­ç»ƒè¿‡æ…¢æˆ–ä¸ç¨³å®š
```python
# é—®é¢˜ï¼šè®­ç»ƒé€Ÿåº¦å¤ªæ…¢
# è§£å†³ï¼šå‡å°‘n_stepså’Œbatch_size
model = PPO("MlpPolicy", env, n_steps=256, batch_size=32)

# é—®é¢˜ï¼šè®­ç»ƒä¸ç¨³å®š
# è§£å†³ï¼šé™ä½å­¦ä¹ ç‡
model = PPO("MlpPolicy", env, learning_rate=1e-5)
```

#### 4. æ™ºèƒ½ä½“ä¸å­¦ä¹ 
```python
# æ£€æŸ¥å¥–åŠ±å‡½æ•°
# åœ¨gobigger_gym_env.pyä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
def step(self, action):
    # ...existing code...
    print(f"Debug: reward={reward}, score_change={score_change}")
    return obs, reward, terminated, truncated, info
```

#### 5. å†…å­˜ä¸è¶³
```python
# å‡å°‘å¹¶è¡Œç¯å¢ƒæ•°é‡
env = make_vec_env(lambda: create_env(), n_envs=1)  # ä»4æ”¹ä¸º1

# å‡å°‘ç»éªŒç¼“å†²åŒºå¤§å°
model = PPO("MlpPolicy", env, n_steps=512, batch_size=32)
```

---

## ğŸš€ è¿›é˜¶ä¼˜åŒ–

### 1. ç½‘ç»œæ¶æ„ä¼˜åŒ–

```python
# è‡ªå®šä¹‰ç­–ç•¥ç½‘ç»œ
from stable_baselines3.ppo import MlpPolicy
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch.nn as nn

class CustomFeatureExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        self.linear = nn.Sequential(
            nn.Linear(observation_space.shape[0], 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, features_dim),
            nn.ReLU()
        )
    
    def forward(self, observations):
        return self.linear(observations)

# ä½¿ç”¨è‡ªå®šä¹‰ç½‘ç»œ
policy_kwargs = dict(
    features_extractor_class=CustomFeatureExtractor,
    features_extractor_kwargs=dict(features_dim=256),
    net_arch=[dict(pi=[256, 128], vf=[256, 128])]
)

model = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs)
```

### 2. è¯¾ç¨‹å­¦ä¹ 

```python
# æ¸è¿›å¼éš¾åº¦è®­ç»ƒ
class CurriculumEnv(GoBiggerEnv):
    def __init__(self, stage=1):
        super().__init__()
        self.stage = stage
        
    def reset(self, **kwargs):
        obs, info = super().reset(**kwargs)
        
        # æ ¹æ®é˜¶æ®µè°ƒæ•´éš¾åº¦
        if self.stage == 1:
            # ç®€å•æ¨¡å¼ï¼šæ›´å¤šé£Ÿç‰©ï¼Œæ›´å°åœ°å›¾
            pass
        elif self.stage == 2:
            # ä¸­ç­‰æ¨¡å¼ï¼šæ ‡å‡†é…ç½®
            pass
        elif self.stage == 3:
            # å›°éš¾æ¨¡å¼ï¼šæ›´å°‘é£Ÿç‰©ï¼Œæ›´å¤§åœ°å›¾
            pass
            
        return obs, info

# åˆ†é˜¶æ®µè®­ç»ƒ
for stage in [1, 2, 3]:
    print(f"Training Stage {stage}")
    env = CurriculumEnv(stage=stage)
    model.set_env(env)
    model.learn(total_timesteps=10000)
```

### 3. å¤šæ™ºèƒ½ä½“è®­ç»ƒ

```python
# åŒæ—¶è®­ç»ƒå¤šä¸ªæ™ºèƒ½ä½“
def create_multi_agent_env():
    """åˆ›å»ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒ"""
    # å¯ä»¥æ‰©å±•ä¸ºå¤šç©å®¶å¯¹æˆ˜ç¯å¢ƒ
    pass

# ä½¿ç”¨è‡ªåšå¼ˆæå‡æ€§èƒ½
def self_play_training():
    """è‡ªåšå¼ˆè®­ç»ƒ"""
    # è®©æ™ºèƒ½ä½“ä¸è‡ªå·±çš„å†å²ç‰ˆæœ¬å¯¹æˆ˜
    pass
```

### 4. è¶…å‚æ•°ä¼˜åŒ–

```python
# ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°æœç´¢
import optuna

def objective(trial):
    # å®šä¹‰æœç´¢ç©ºé—´
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_categorical("n_steps", [512, 1024, 2048])
    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128])
    
    # è®­ç»ƒæ¨¡å‹
    model = PPO("MlpPolicy", env, 
                learning_rate=learning_rate,
                n_steps=n_steps, 
                batch_size=batch_size)
    
    model.learn(total_timesteps=10000)
    
    # è¯„ä¼°æ€§èƒ½
    mean_reward = evaluate_model(model)
    return mean_reward

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)
```

### 5. é«˜çº§å¥–åŠ±è®¾è®¡

```python
# åŸºäºè¡Œä¸ºçš„å¥–åŠ±å¡‘é€ 
class AdvancedRewardEnv(GoBiggerEnv):
    def calculate_reward(self, old_obs, action, new_obs, info):
        reward = 0
        
        # åŸºç¡€åˆ†æ•°å¥–åŠ±
        if info.get('score_change', 0) > 0:
            reward += info['score_change'] * 0.01
        
        # ç§»åŠ¨æ•ˆç‡å¥–åŠ±
        if self.is_moving_towards_food(old_obs, action, new_obs):
            reward += 0.1
        
        # æ¢ç´¢å¥–åŠ±
        if self.is_exploring_new_area(new_obs):
            reward += 0.05
        
        # åˆ†è£‚æ—¶æœºå¥–åŠ±
        if action == 4 and self.is_good_split_timing(new_obs):
            reward += 0.5
        
        return reward
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§å’Œæ—¥å¿—

### 1. TensorBoardæ—¥å¿—

```bash
# å¯åŠ¨TensorBoardç›‘æ§
tensorboard --logdir=./tensorboard_logs/

# åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ï¼šhttp://localhost:6006
```

### 2. è‡ªå®šä¹‰æ—¥å¿—è®°å½•

```python
import logging

# è®¾ç½®è®­ç»ƒæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

class LoggingCallback(BaseCallback):
    def _on_step(self):
        if self.n_calls % 1000 == 0:
            logging.info(f"Step {self.n_calls}: Mean reward = {self.locals.get('rewards', [0])}")
        return True
```

### 3. æ¨¡å‹æ£€æŸ¥ç‚¹

```python
# å®šæœŸä¿å­˜æ¨¡å‹
class CheckpointCallback(BaseCallback):
    def __init__(self, save_freq, save_path):
        super().__init__()
        self.save_freq = save_freq
        self.save_path = save_path
        
    def _on_step(self):
        if self.n_calls % self.save_freq == 0:
            self.model.save(f"{self.save_path}/model_{self.n_calls}")
        return True

# ä½¿ç”¨å›è°ƒ
checkpoint_callback = CheckpointCallback(save_freq=5000, save_path="./checkpoints/")
model.learn(total_timesteps=50000, callback=checkpoint_callback)
```

---

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### 1. è®­ç»ƒæµç¨‹å»ºè®®

1. **ä»ç®€å•å¼€å§‹**: ä½¿ç”¨é»˜è®¤å‚æ•°è¿›è¡ŒçŸ­æœŸè®­ç»ƒ
2. **è§‚å¯Ÿè¡Œä¸º**: ç¡®ä¿æ™ºèƒ½ä½“åŸºæœ¬è¡Œä¸ºæ­£å¸¸
3. **é€æ­¥è°ƒä¼˜**: ä¸€æ¬¡åªæ”¹å˜ä¸€ä¸ªå‚æ•°
4. **è®°å½•å®éªŒ**: ä¿å­˜æ¯æ¬¡å®éªŒçš„é…ç½®å’Œç»“æœ
5. **å®šæœŸè¯„ä¼°**: æ¯è½®è®­ç»ƒåéƒ½è¦æµ‹è¯•æ•ˆæœ

### 2. å‚æ•°è°ƒä¼˜ç­–ç•¥

- **å­¦ä¹ ç‡**: ä» 1e-4 å¼€å§‹ï¼Œæ ¹æ®æ”¶æ•›æƒ…å†µè°ƒæ•´
- **ç»éªŒæ­¥æ•°**: å†…å­˜å…è®¸çš„æƒ…å†µä¸‹å°½é‡å¤§
- **è®­ç»ƒè½®æ•°**: å¹³è¡¡è®­ç»ƒæ—¶é—´å’Œæ•ˆæœ
- **å¥–åŠ±å‡½æ•°**: ç¡®ä¿æœ‰æ˜ç¡®çš„è¿›åº¦ä¿¡å·

### 3. è°ƒè¯•æŠ€å·§

- ä½¿ç”¨ `verbose=1` è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹
- æ·»åŠ è°ƒè¯•è¾“å‡ºåˆ°å¥–åŠ±å‡½æ•°
- å®šæœŸè¿è¡Œè¯„ä¼°è„šæœ¬
- è§‚å¯Ÿæ™ºèƒ½ä½“çš„å®é™…æ¸¸æˆè¡Œä¸º

---

## ğŸ“ æ”¯æŒå’Œå¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå»ºè®®ï¼š

1. æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦æ­£ç¡®å®‰è£…
3. ç¡®è®¤C++åº“æ„å»ºæˆåŠŸ
4. å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®
5. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—å’ŒTensorBoard

ç¥æ‚¨è®­ç»ƒæ„‰å¿«ï¼ğŸ®ğŸ¤–
