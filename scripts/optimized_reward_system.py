#!/usr/bin/env python3
"""
ä¼˜åŒ–å¥–åŠ±å‡½æ•°è®¾è®¡ - è§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
===============================================

å®ç°ä¸‰å¤§æ ¸å¿ƒä¼˜åŒ–ï¼š
1. æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆNovelty Bonusï¼‰- é¼“åŠ±å¤šæ–¹å‘æ¢ç´¢
2. æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ± - å‡å°‘ç›²ç›®åˆ†è£‚ï¼Œé¼“åŠ±é€šè¿‡åˆ†è£‚è·å¾—æ›´å¤šé£Ÿç‰©
3. ä¼˜åŒ–è®­ç»ƒè¶…å‚æ•° - æé«˜ç†µç³»æ•°ï¼Œå¢å¼ºæ¢ç´¢æ€§

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025å¹´7æœˆ1æ—¥
"""

import numpy as np
import math
from typing import Dict, Tuple, Set, Optional, List, Any
from dataclasses import dataclass
from collections import deque, defaultdict

@dataclass
class ExplorationState:
    """æ¢ç´¢çŠ¶æ€è¿½è¸ª"""
    visited_cells: Set[Tuple[int, int]]
    cell_visit_count: Dict[Tuple[int, int], int]
    cell_size: int = 100  # æ¯ä¸ªæ ¼å­çš„åƒç´ å¤§å°
    max_visits_bonus: int = 3  # æœ€å¤§è®¿é—®æ¬¡æ•°å¥–åŠ±
    
class OptimizedRewardCalculator:
    """ä¼˜åŒ–çš„å¥–åŠ±è®¡ç®—å™¨ - ä¸“æ³¨è§£å†³æ¢ç´¢å’Œç­–ç•¥é—®é¢˜"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        
        # === æ ¸å¿ƒå¥–åŠ±æƒé‡é…ç½® ===
        self.weights = {
            # åŸºç¡€å¥–åŠ±
            'score_growth': 1.0,           # åˆ†æ•°å¢é•¿åŸºç¡€å¥–åŠ±
            'survival': 0.01,              # ç”Ÿå­˜å¥–åŠ±
            'death_penalty': -50.0,        # æ­»äº¡é‡æƒ©ç½š
            
            # === æ¢ç´¢å¥–åŠ±ç³»ç»Ÿ ===
            'exploration_bonus': 2.0,      # æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆå…³é”®ï¼‰
            'diversity_bonus': 1.0,        # ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±
            'backtrack_penalty': -0.5,     # åå¤å›åˆ°åŒä¸€åŒºåŸŸæƒ©ç½š
            
            # === æ™ºèƒ½åˆ†è£‚å¥–åŠ±ç³»ç»Ÿ ===
            'smart_split_bonus': 3.0,      # æ™ºèƒ½åˆ†è£‚å¥–åŠ±
            'waste_split_penalty': -2.0,   # æµªè´¹åˆ†è£‚æƒ©ç½š
            'split_efficiency': 2.0,       # åˆ†è£‚åé£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±
            
            # === å…¶ä»–ç­–ç•¥å¥–åŠ± ===
            'food_efficiency': 1.5,        # é£Ÿç‰©è·å–æ•ˆç‡
            'size_management': 1.0,         # å¤§å°ç®¡ç†å¥–åŠ±
        }
        
        # === æ¢ç´¢çŠ¶æ€è¿½è¸ª ===
        self.exploration = ExplorationState(
            visited_cells=set(),
            cell_visit_count=defaultdict(int),
            cell_size=self.config.get('exploration_cell_size', 100)
        )
        
        # === ç§»åŠ¨æ–¹å‘è¿½è¸ªï¼ˆè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"ï¼‰ ===
        self.movement_history = deque(maxlen=20)  # è®°å½•æœ€è¿‘20æ­¥çš„ç§»åŠ¨æ–¹å‘
        self.direction_diversity_window = 10      # æ–¹å‘å¤šæ ·æ€§è®¡ç®—çª—å£
        
        # === åˆ†è£‚ç­–ç•¥è¿½è¸ª ===
        self.split_history = []
        self.last_split_step = -1
        self.split_efficiency_window = 50  # åˆ†è£‚æ•ˆç‡è¯„ä¼°çª—å£
        
        # === å†å²çŠ¶æ€ç¼“å­˜ ===
        self.history = deque(maxlen=100)
        self.step_count = 0
        
        # === ç»Ÿè®¡ä¿¡æ¯ ===
        self.stats = {
            'total_exploration_bonus': 0.0,
            'total_smart_split_bonus': 0.0,
            'unique_cells_visited': 0,
            'direction_changes': 0,
            'efficient_splits': 0,
            'wasted_splits': 0,
        }
    
    def calculate_reward(self, current_state, previous_state, action, step_info: Dict = None) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—ä¼˜åŒ–çš„å¥–åŠ±
        
        Args:
            current_state: å½“å‰æ¸¸æˆçŠ¶æ€
            previous_state: å‰ä¸€æ¸¸æˆçŠ¶æ€  
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            step_info: é¢å¤–çš„æ­¥éª¤ä¿¡æ¯
            
        Returns:
            (æ€»å¥–åŠ±, å¥–åŠ±åˆ†è§£å­—å…¸)
        """
        self.step_count += 1
        reward_components = {}
        
        # 1. åŸºç¡€å¥–åŠ±ï¼ˆåˆ†æ•°å¢é•¿ã€ç”Ÿå­˜ã€æ­»äº¡ï¼‰
        reward_components.update(self._calculate_basic_rewards(current_state, previous_state))
        
        # 2. ğŸ”¥ æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"çš„æ ¸å¿ƒï¼‰
        reward_components.update(self._calculate_exploration_rewards(current_state))
        
        # 3. ğŸ”¥ ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±
        reward_components.update(self._calculate_movement_diversity_rewards(action))
        
        # 4. ğŸ”¥ æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ±
        reward_components.update(self._calculate_smart_split_rewards(current_state, previous_state, action))
        
        # 5. é£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±
        reward_components.update(self._calculate_food_efficiency_rewards(current_state, previous_state))
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = sum(
            self.weights.get(component, 1.0) * value 
            for component, value in reward_components.items()
        )
        
        # æ›´æ–°å†å²å’Œç»Ÿè®¡
        self._update_history(current_state, reward_components)
        self._update_stats(reward_components)
        
        return total_reward, reward_components
    
    def _calculate_basic_rewards(self, current_state, previous_state) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€å¥–åŠ±ï¼šåˆ†æ•°å¢é•¿ã€ç”Ÿå­˜ã€æ­»äº¡"""
        rewards = {}
        
        # åˆ†æ•°å¢é•¿å¥–åŠ±
        if previous_state is not None:
            score_delta = getattr(current_state, 'score', 0) - getattr(previous_state, 'score', 0)
            if score_delta > 0:
                # éçº¿æ€§åˆ†æ•°å¥–åŠ±ï¼Œé¼“åŠ±æ›´å¤§çš„å¢é•¿
                rewards['score_growth'] = math.sqrt(score_delta) / 20.0
            else:
                rewards['score_growth'] = score_delta / 100.0
        else:
            rewards['score_growth'] = 0.0
        
        # ç”Ÿå­˜å¥–åŠ±
        rewards['survival'] = 1.0
        
        # æ­»äº¡æ£€æµ‹ï¼ˆå‡è®¾çŠ¶æ€åŒ…å«æ­»äº¡ä¿¡æ¯ï¼‰
        if hasattr(current_state, 'is_dead') and current_state.is_dead:
            rewards['death_penalty'] = 1.0  # æƒé‡ä¸­å·²è®¾ä¸ºè´Ÿå€¼
        else:
            rewards['death_penalty'] = 0.0
            
        return rewards
    
    def _calculate_exploration_rewards(self, current_state) -> Dict[str, float]:
        """
        ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼šè®¡ç®—æ¢ç´¢å¥–åŠ±ï¼Œè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
        
        å®ç°æ€è·¯ï¼š
        1. å°†åœ°å›¾åˆ’åˆ†ä¸ºç½‘æ ¼
        2. è¿½è¸ªè®¿é—®è¿‡çš„æ ¼å­
        3. ç»™è®¿é—®æ–°æ ¼å­å¤§é¢å¥–åŠ±
        4. å¯¹é‡å¤è®¿é—®ç»™äºˆé€’å‡å¥–åŠ±æˆ–æƒ©ç½š
        """
        rewards = {}
        
        # è·å–å½“å‰ä½ç½®
        if hasattr(current_state, 'rectangle') and len(current_state.rectangle) >= 4:
            center_x = (current_state.rectangle[0] + current_state.rectangle[2]) / 2
            center_y = (current_state.rectangle[1] + current_state.rectangle[3]) / 2
        elif hasattr(current_state, 'position'):
            center_x, center_y = current_state.position[0], current_state.position[1]
        else:
            # æ— æ³•è·å–ä½ç½®ï¼Œè·³è¿‡æ¢ç´¢å¥–åŠ±
            rewards['exploration_bonus'] = 0.0
            rewards['backtrack_penalty'] = 0.0
            return rewards
        
        # è®¡ç®—å½“å‰æ ¼å­åæ ‡
        cell_x = int(center_x // self.exploration.cell_size)
        cell_y = int(center_y // self.exploration.cell_size)
        cell_coord = (cell_x, cell_y)
        
        # æ›´æ–°è®¿é—®è®¡æ•°
        self.exploration.cell_visit_count[cell_coord] += 1
        visit_count = self.exploration.cell_visit_count[cell_coord]
        
        # æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±
        if cell_coord not in self.exploration.visited_cells:
            # ğŸ”¥ é¦–æ¬¡è®¿é—®æ–°åŒºåŸŸï¼šå¤§é¢å¥–åŠ±
            rewards['exploration_bonus'] = 1.0  # åŸºç¡€æ¢ç´¢å¥–åŠ±
            self.exploration.visited_cells.add(cell_coord)
            self.stats['unique_cells_visited'] += 1
            
            # é¢å¤–å¥–åŠ±ï¼šå¦‚æœè¿™ä¸ªæ–°åŒºåŸŸè·ç¦»ä¹‹å‰è®¿é—®çš„åŒºåŸŸè¾ƒè¿œ
            if len(self.exploration.visited_cells) > 1:
                min_distance = min(
                    abs(cell_x - other_x) + abs(cell_y - other_y)
                    for other_x, other_y in self.exploration.visited_cells
                    if (other_x, other_y) != cell_coord
                )
                if min_distance >= 3:  # è·ç¦»è¶³å¤Ÿè¿œ
                    rewards['exploration_bonus'] += 0.5  # è¿œè·ç¦»æ¢ç´¢é¢å¤–å¥–åŠ±
        else:
            # å·²è®¿é—®åŒºåŸŸï¼šæ ¹æ®è®¿é—®æ¬¡æ•°ç»™äºˆé€’å‡å¥–åŠ±æˆ–æƒ©ç½š
            if visit_count <= self.exploration.max_visits_bonus:
                # å°‘é‡è®¿é—®ä»æœ‰å°å¥–åŠ±
                rewards['exploration_bonus'] = 0.1 / visit_count
            else:
                # è¿‡åº¦è®¿é—®ç»™äºˆæƒ©ç½š
                rewards['backtrack_penalty'] = min(visit_count - self.exploration.max_visits_bonus, 5) * 0.1
                rewards['exploration_bonus'] = 0.0
        
        return rewards
    
    def _calculate_movement_diversity_rewards(self, action) -> Dict[str, float]:
        """
        ğŸ”¥ è®¡ç®—ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±ï¼Œè¿›ä¸€æ­¥è§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
        """
        rewards = {'diversity_bonus': 0.0}
        
        # è®°å½•å½“å‰ç§»åŠ¨æ–¹å‘
        if len(action) >= 2:
            current_direction = np.array([action[0], action[1]])
            direction_magnitude = np.linalg.norm(current_direction)
            
            if direction_magnitude > 0.1:  # æœ‰æ˜æ˜¾ç§»åŠ¨
                normalized_direction = current_direction / direction_magnitude
                self.movement_history.append(normalized_direction)
                
                # è®¡ç®—æ–¹å‘å¤šæ ·æ€§ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„å†å²è®°å½•ï¼‰
                if len(self.movement_history) >= self.direction_diversity_window:
                    recent_directions = list(self.movement_history)[-self.direction_diversity_window:]
                    
                    # è®¡ç®—æ–¹å‘å˜åŒ–çš„æ–¹å·®ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
                    direction_changes = []
                    for i in range(1, len(recent_directions)):
                        # è®¡ç®—è¿ç»­æ–¹å‘é—´çš„è§’åº¦å˜åŒ–
                        dot_product = np.clip(np.dot(recent_directions[i-1], recent_directions[i]), -1, 1)
                        angle_change = np.arccos(dot_product)
                        direction_changes.append(angle_change)
                    
                    if direction_changes:
                        # æ–¹å‘å˜åŒ–è¶Šå¤§ï¼Œå¤šæ ·æ€§å¥–åŠ±è¶Šé«˜
                        avg_angle_change = np.mean(direction_changes)
                        diversity_score = avg_angle_change / np.pi  # æ ‡å‡†åŒ–åˆ°[0,1]
                        
                        if diversity_score > 0.3:  # è¶³å¤Ÿçš„æ–¹å‘å˜åŒ–
                            rewards['diversity_bonus'] = diversity_score * 0.5
                            self.stats['direction_changes'] += 1
        
        return rewards
    
    def _calculate_smart_split_rewards(self, current_state, previous_state, action) -> Dict[str, float]:
        """
        ğŸ”¥ è®¡ç®—æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ±
        
        å‡å°‘ç›²ç›®åˆ†è£‚ï¼Œé¼“åŠ±åœ¨æœ‰è¶³å¤Ÿé£Ÿç‰©æ—¶åˆ†è£‚
        """
        rewards = {
            'smart_split_bonus': 0.0,
            'waste_split_penalty': 0.0,
            'split_efficiency': 0.0
        }
        
        # æ£€æµ‹æ˜¯å¦è¿›è¡Œäº†åˆ†è£‚
        split_action = False
        if len(action) >= 3:
            split_action = action[2] > 1.5  # å‡è®¾ç¬¬ä¸‰ä¸ªåŠ¨ä½œæ˜¯åˆ†è£‚ä¿¡å·
        
        if split_action:
            current_cells = 1  # é»˜è®¤å€¼
            food_nearby = 0
            current_score = 0
            
            # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
            if hasattr(current_state, 'clone') and isinstance(current_state.clone, list):
                current_cells = len(current_state.clone)
            if hasattr(current_state, 'food') and isinstance(current_state.food, list):
                food_nearby = len(current_state.food)
            if hasattr(current_state, 'score'):
                current_score = current_state.score
            
            # è¯„ä¼°åˆ†è£‚æ™ºèƒ½ç¨‹åº¦
            steps_since_last_split = self.step_count - self.last_split_step
            
            # æ™ºèƒ½åˆ†è£‚æ¡ä»¶ï¼š
            # 1. é™„è¿‘æœ‰è¶³å¤Ÿé£Ÿç‰©ï¼ˆè‡³å°‘3ä¸ªï¼‰
            # 2. è·ç¦»ä¸Šæ¬¡åˆ†è£‚è¶³å¤Ÿä¹…ï¼ˆè‡³å°‘30æ­¥ï¼‰
            # 3. å½“å‰åˆ†æ•°è¶³å¤Ÿé«˜ï¼ˆèƒ½å¤Ÿæ‰¿å—åˆ†è£‚ï¼‰
            # 4. å½“å‰ç»†èƒæ•°ä¸è¿‡å¤šï¼ˆé¿å…è¿‡åº¦åˆ†è£‚ï¼‰
            
            is_smart_split = (
                food_nearby >= 3 and  # æ¡ä»¶1ï¼šé™„è¿‘æœ‰è¶³å¤Ÿé£Ÿç‰©
                steps_since_last_split >= 30 and  # æ¡ä»¶2ï¼šåˆ†è£‚é—´éš”è¶³å¤Ÿ
                current_score >= 2000 and  # æ¡ä»¶3ï¼šåˆ†æ•°è¶³å¤Ÿ
                current_cells <= 8  # æ¡ä»¶4ï¼šé¿å…è¿‡åº¦åˆ†è£‚
            )
            
            if is_smart_split:
                # æ™ºèƒ½åˆ†è£‚å¥–åŠ±
                food_density_bonus = min(food_nearby / 10.0, 1.0)  # é£Ÿç‰©å¯†åº¦å¥–åŠ±
                timing_bonus = min(steps_since_last_split / 100.0, 0.5)  # æ—¶æœºå¥–åŠ±
                rewards['smart_split_bonus'] = 1.0 + food_density_bonus + timing_bonus
                
                self.stats['efficient_splits'] += 1
            else:
                # æµªè´¹åˆ†è£‚æƒ©ç½š
                penalty_factors = []
                if food_nearby < 3:
                    penalty_factors.append(0.5)  # é£Ÿç‰©ä¸è¶³æƒ©ç½š
                if steps_since_last_split < 30:
                    penalty_factors.append(0.3)  # åˆ†è£‚è¿‡é¢‘æƒ©ç½š
                if current_cells > 8:
                    penalty_factors.append(0.4)  # è¿‡åº¦åˆ†è£‚æƒ©ç½š
                
                rewards['waste_split_penalty'] = sum(penalty_factors)
                self.stats['wasted_splits'] += 1
            
            # è®°å½•åˆ†è£‚äº‹ä»¶
            self.last_split_step = self.step_count
            self.split_history.append({
                'step': self.step_count,
                'food_nearby': food_nearby,
                'cells_before': current_cells,
                'smart_split': is_smart_split
            })
        
        # åˆ†è£‚åæ•ˆç‡å¥–åŠ±ï¼šè¯„ä¼°åˆ†è£‚åæ˜¯å¦è·å¾—äº†æ›´å¤šé£Ÿç‰©
        if previous_state and hasattr(current_state, 'score') and hasattr(previous_state, 'score'):
            score_growth = current_state.score - previous_state.score
            
            # å¦‚æœæœ€è¿‘è¿›è¡Œäº†åˆ†è£‚ä¸”åˆ†æ•°æœ‰æ˜¾è‘—å¢é•¿
            recent_splits = [s for s in self.split_history if self.step_count - s['step'] <= self.split_efficiency_window]
            if recent_splits and score_growth > 0:
                # æ ¹æ®åˆ†è£‚åçš„åˆ†æ•°å¢é•¿ç»™äºˆæ•ˆç‡å¥–åŠ±
                efficiency_ratio = score_growth / len(recent_splits)
                rewards['split_efficiency'] = min(efficiency_ratio / 500.0, 1.0)
        
        return rewards
    
    def _calculate_food_efficiency_rewards(self, current_state, previous_state) -> Dict[str, float]:
        """è®¡ç®—é£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±"""
        rewards = {'food_efficiency': 0.0}
        
        if previous_state is None:
            return rewards
        
        # è®¡ç®—åˆ†æ•°å¢é•¿ï¼ˆä¸»è¦æ¥è‡ªé£Ÿç‰©ï¼‰
        score_growth = getattr(current_state, 'score', 0) - getattr(previous_state, 'score', 0)
        
        if score_growth > 0:
            # åŸºäºåˆ†æ•°å¢é•¿çš„æ•ˆç‡å¥–åŠ±
            efficiency = min(score_growth / 100.0, 2.0)  # é™åˆ¶æœ€å¤§å¥–åŠ±
            rewards['food_efficiency'] = efficiency
        
        return rewards
    
    def _update_history(self, current_state, reward_components):
        """æ›´æ–°å†å²è®°å½•"""
        self.history.append({
            'step': self.step_count,
            'state': current_state,
            'rewards': reward_components.copy(),
            'total_reward': sum(
                self.weights.get(k, 1.0) * v for k, v in reward_components.items()
            )
        })
    
    def _update_stats(self, reward_components):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if 'exploration_bonus' in reward_components and reward_components['exploration_bonus'] > 0:
            self.stats['total_exploration_bonus'] += reward_components['exploration_bonus']
        
        if 'smart_split_bonus' in reward_components and reward_components['smart_split_bonus'] > 0:
            self.stats['total_smart_split_bonus'] += reward_components['smart_split_bonus']
    
    def get_exploration_stats(self) -> Dict[str, Any]:
        """è·å–æ¢ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'unique_cells_visited': len(self.exploration.visited_cells),
            'total_cell_visits': sum(self.exploration.cell_visit_count.values()),
            'exploration_coverage': len(self.exploration.visited_cells),  # å¯ä»¥åç»­æ ‡å‡†åŒ–
            'avg_visits_per_cell': (
                sum(self.exploration.cell_visit_count.values()) / len(self.exploration.visited_cells)
                if self.exploration.visited_cells else 0
            ),
            'direction_diversity_score': self.stats['direction_changes'] / max(self.step_count, 1),
        }
    
    def get_split_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†è£‚ç­–ç•¥ç»Ÿè®¡ä¿¡æ¯"""
        total_splits = len(self.split_history)
        return {
            'total_splits': total_splits,
            'efficient_splits': self.stats['efficient_splits'],
            'wasted_splits': self.stats['wasted_splits'],
            'split_efficiency_ratio': (
                self.stats['efficient_splits'] / total_splits
                if total_splits > 0 else 0
            ),
            'avg_split_interval': (
                self.step_count / total_splits if total_splits > 0 else float('inf')
            ),
        }
    
    def reset(self):
        """é‡ç½®å¥–åŠ±è®¡ç®—å™¨ï¼ˆæ–°episodeæ—¶è°ƒç”¨ï¼‰"""
        self.exploration.visited_cells.clear()
        self.exploration.cell_visit_count.clear()
        self.movement_history.clear()
        self.split_history.clear()
        self.history.clear()
        self.step_count = 0
        self.last_split_step = -1
        
        # é‡ç½®ç»Ÿè®¡
        for key in self.stats:
            self.stats[key] = 0
        self.stats['unique_cells_visited'] = 0
        self.stats['direction_changes'] = 0
        self.stats['efficient_splits'] = 0
        self.stats['wasted_splits'] = 0


def create_optimized_training_config():
    """
    åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®
    
    é‡ç‚¹ï¼šæé«˜ç†µç³»æ•°ä»¥å¢å¼ºæ¢ç´¢æ€§
    """
    return {
        # ğŸ”¥ å¼ºåŒ–å­¦ä¹ è¶…å‚æ•°ä¼˜åŒ–ï¼ˆé‡ç‚¹æ˜¯ç†µç³»æ•°ï¼‰
        'learning_rate': 3e-4,
        'ent_coef': 0.05,  # ğŸ”¥ æé«˜ç†µç³»æ•°ï¼šä»é»˜è®¤0.01æå‡åˆ°0.05ï¼Œå¢å¼ºæ¢ç´¢
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'n_epochs': 10,
        'batch_size': 64,
        
        # ğŸ”¥ å¥–åŠ±ç³»ç»Ÿé…ç½®
        'use_optimized_reward': True,
        'reward_config': {
            'exploration_cell_size': 80,  # é€‚ä¸­çš„æ ¼å­å¤§å°ï¼Œä¸ä¼šå¤ªç¨€ç–ä¹Ÿä¸ä¼šå¤ªå¯†é›†
            'direction_diversity_window': 15,  # æ–¹å‘å¤šæ ·æ€§è¯„ä¼°çª—å£
            'split_efficiency_window': 60,    # åˆ†è£‚æ•ˆç‡è¯„ä¼°çª—å£
        },
        
        # ç¯å¢ƒé…ç½®
        'max_episode_steps': 3000,  # æ›´é•¿çš„episodeä»¥ä¾¿å……åˆ†æ¢ç´¢
        'frame_skip': 1,
        
        # è®­ç»ƒé…ç½®
        'total_timesteps': 1000000,
        'eval_freq': 10000,
        'save_freq': 50000,
    }


if __name__ == "__main__":
    # æµ‹è¯•å¥–åŠ±è®¡ç®—å™¨
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–å¥–åŠ±è®¡ç®—å™¨...")
    
    calculator = OptimizedRewardCalculator()
    
    # æ¨¡æ‹Ÿä¸€äº›çŠ¶æ€ç”¨äºæµ‹è¯•
    class MockState:
        def __init__(self, score, position, food_count=0):
            self.score = score
            self.rectangle = [position[0]-10, position[1]-10, position[0]+10, position[1]+10]
            self.food = [f"food_{i}" for i in range(food_count)]
            self.clone = ["main_cell"]
    
    # æµ‹è¯•æ¢ç´¢å¥–åŠ±
    print("\nğŸ“ æµ‹è¯•æ¢ç´¢å¥–åŠ±:")
    state1 = MockState(1000, (100, 100), 2)  # æ–°ä½ç½®
    state2 = MockState(1100, (200, 100), 3)  # å¦ä¸€ä¸ªæ–°ä½ç½®
    state3 = MockState(1200, (100, 100), 1)  # å›åˆ°ç¬¬ä¸€ä¸ªä½ç½®
    
    action = [0.5, 0.3, 0]  # å‘å³ä¸Šç§»åŠ¨ï¼Œä¸åˆ†è£‚
    
    reward1, components1 = calculator.calculate_reward(state1, None, action)
    print(f"ç¬¬ä¸€ä¸ªä½ç½®: æ€»å¥–åŠ±={reward1:.3f}, æ¢ç´¢å¥–åŠ±={components1.get('exploration_bonus', 0):.3f}")
    
    reward2, components2 = calculator.calculate_reward(state2, state1, action)
    print(f"æ–°ä½ç½®: æ€»å¥–åŠ±={reward2:.3f}, æ¢ç´¢å¥–åŠ±={components2.get('exploration_bonus', 0):.3f}")
    
    reward3, components3 = calculator.calculate_reward(state3, state2, action)
    print(f"å›åˆ°è€ä½ç½®: æ€»å¥–åŠ±={reward3:.3f}, æ¢ç´¢å¥–åŠ±={components3.get('exploration_bonus', 0):.3f}, å›é€€æƒ©ç½š={components3.get('backtrack_penalty', 0):.3f}")
    
    # æµ‹è¯•æ™ºèƒ½åˆ†è£‚
    print("\nğŸ”€ æµ‹è¯•æ™ºèƒ½åˆ†è£‚:")
    split_action = [0.1, 0.1, 2.0]  # åˆ†è£‚åŠ¨ä½œ
    state_with_food = MockState(3000, (300, 300), 5)  # é«˜åˆ†+å¤šé£Ÿç‰©
    reward_split, components_split = calculator.calculate_reward(state_with_food, state3, split_action)
    print(f"æ™ºèƒ½åˆ†è£‚: æ€»å¥–åŠ±={reward_split:.3f}, åˆ†è£‚å¥–åŠ±={components_split.get('smart_split_bonus', 0):.3f}")
    
    # æµ‹è¯•æµªè´¹åˆ†è£‚
    bad_split_state = MockState(1500, (400, 400), 1)  # ä½åˆ†+å°‘é£Ÿç‰©
    reward_bad, components_bad = calculator.calculate_reward(bad_split_state, state_with_food, split_action)
    print(f"æµªè´¹åˆ†è£‚: æ€»å¥–åŠ±={reward_bad:.3f}, åˆ†è£‚æƒ©ç½š={components_bad.get('waste_split_penalty', 0):.3f}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“Š æ¢ç´¢ç»Ÿè®¡:", calculator.get_exploration_stats())
    print("ğŸ“Š åˆ†è£‚ç»Ÿè®¡:", calculator.get_split_stats())
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
