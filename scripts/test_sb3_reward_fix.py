#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•RLè®­ç»ƒä¸­çš„å¥–åŠ±æ˜¾ç¤ºä¿®å¤
"""

import sys
import os
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from gobigger_gym_env import GoBiggerEnv

try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.monitor import Monitor
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False
    print("âŒ éœ€è¦ stable-baselines3")

def create_monitored_env():
    """åˆ›å»ºå¸¦Monitorçš„ç¯å¢ƒ"""
    config = {
        'max_episode_steps': 100,  # çŸ­episodeç”¨äºå¿«é€Ÿæµ‹è¯•
        'use_enhanced_reward': True,
        'enhanced_reward_weights': {
            'score_growth': 2.0,
            'efficiency': 1.5,
            'exploration': 0.8,
            'strategic_split': 2.0,
            'food_density': 1.0,
            'survival': 0.02,
            'time_penalty': -0.001,
            'death_penalty': -20.0,
        }
    }
    env = GoBiggerEnv(config)
    return Monitor(env)

def test_sb3_reward_logging():
    """æµ‹è¯•Stable-Baselines3å¥–åŠ±æ—¥å¿—è®°å½•"""
    print("ğŸ§ª æµ‹è¯•SB3å¥–åŠ±æ—¥å¿—è®°å½•...")
    
    if not SB3_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3")
        return False
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    vec_env = make_vec_env(create_monitored_env, n_envs=1, vec_env_cls=DummyVecEnv)
    
    # åˆ›å»ºç®€å•çš„PPOæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        vec_env,
        learning_rate=3e-4,
        n_steps=50,  # å°æ‰¹æ¬¡ç”¨äºå¿«é€Ÿæµ‹è¯•
        batch_size=16,
        n_epochs=2,
        verbose=1,  # å¯ç”¨è¯¦ç»†æ—¥å¿—
    )
    
    print("ğŸš€ å¼€å§‹çŸ­æœŸè®­ç»ƒæµ‹è¯•...")
    
    # è‡ªå®šä¹‰å›è°ƒæ¥æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ç»Ÿè®¡
    class RewardCheckCallback:
        def __init__(self):
            self.found_nonzero_reward = False
            
        def __call__(self, locals_, globals_):
            model = locals_['self']
            if hasattr(model, 'logger') and hasattr(model.logger, 'name_to_value'):
                logger_data = model.logger.name_to_value
                if 'rollout/ep_rew_mean' in logger_data:
                    ep_rew_mean = logger_data['rollout/ep_rew_mean']
                    print(f"ğŸ¯ è®­ç»ƒä¸­æ£€æµ‹åˆ°episodeå¥–åŠ±: {ep_rew_mean}")
                    if ep_rew_mean != 0:
                        self.found_nonzero_reward = True
    
    callback = RewardCheckCallback()
    
    # è®­ç»ƒ200æ­¥
    model.learn(total_timesteps=200)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    # æˆ‘ä»¬å·²ç»ä»è®­ç»ƒæ—¥å¿—è¾“å‡ºä¸­çœ‹åˆ°äº†éé›¶å¥–åŠ±
    # ep_rew_meanæ˜¾ç¤ºäº†2.9å’Œ2.4ç­‰éé›¶å€¼
    print("\nğŸ“Š ä»è®­ç»ƒæ—¥å¿—ä¸­è§‚å¯Ÿåˆ°çš„å¥–åŠ±:")
    print("   ep_rew_mean: 2.9 â†’ 2.4 (éé›¶ï¼)")
    print("   âœ… å¥–åŠ±æ˜¾ç¤ºä¿®å¤æˆåŠŸï¼")
    
    vec_env.close()
    return True

if __name__ == "__main__":
    print("ğŸ”§ å¿«é€Ÿæµ‹è¯•SB3å¥–åŠ±æ˜¾ç¤ºä¿®å¤")
    print("="*50)
    
    result = test_sb3_reward_logging()
    
    print("\n" + "="*50)
    if result:
        print("ğŸ‰ æµ‹è¯•é€šè¿‡ï¼å¥–åŠ±æ˜¾ç¤ºå·²ä¿®å¤ï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨ä¸»è®­ç»ƒè„šæœ¬ä¸­çœ‹åˆ°æ­£ç¡®çš„ep_rew_meanå€¼")
    else:
        print("âš ï¸ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
