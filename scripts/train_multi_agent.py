import sys
import os
from pathlib import Path
import numpy as np
import time
import json
import math
from collections import deque, defaultdict
from typing import Dict, Tuple, Set, Optional, List, Any
from dataclasses import dataclass

# ç»Ÿä¸€çš„è·¯å¾„è®¾ç½®
# 1. å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.pathï¼Œä»¥è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜ (e.g., from python.xxx)
current_dir = Path(__file__).parent
root_dir = current_dir.parent
if str(root_dir) not in sys.path:
    sys.path.insert(0, str(root_dir))

# 2. æ·»åŠ  C++ ç¼–è¯‘çš„æ¨¡å—è·¯å¾„
build_dir = root_dir / "build" / "Release"
if str(build_dir) not in sys.path:
    sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from python.multi_agent_gobigger_gym_env import MultiAgentGoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
    from stable_baselines3.common.callbacks import BaseCallback, CallbackList, EvalCallback
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.monitor import Monitor
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

import gymnasium as gym

@dataclass
class ExplorationState:
    """æ¢ç´¢çŠ¶æ€è¿½è¸ª"""
    visited_cells: Set[Tuple[int, int]]
    cell_visit_count: Dict[Tuple[int, int], int]
    cell_size: int = 100

class MultiAgentRewardCalculator:
    """
    ğŸ”¥ å¤šæ™ºèƒ½ä½“ä¼˜åŒ–çš„å¥–åŠ±è®¡ç®—å™¨
    
    å®ç°å¤šæ™ºèƒ½ä½“åä½œä¸å¯¹æŠ—çš„æ ¸å¿ƒå¥–åŠ±ï¼š
    1. é˜Ÿä¼æ’åå¥–åŠ±
    2. åå™¬æ•Œäººå¥–åŠ±
    3. æ™ºèƒ½åˆ†è£‚è¿½å‡»å¥–åŠ±
    4. æ¢ç´¢ä¸åä½œå¥–åŠ±
    """
    
    def __init__(self, team_id: int, player_names: List[str], config: Optional[Dict] = None):
        self.team_id = team_id
        self.player_names = player_names
        self.config = config or {}
        
        # === ğŸ”¥ æ ¸å¿ƒå¤šæ™ºèƒ½ä½“å¥–åŠ±æƒé‡é…ç½® ===
        self.weights = {
            # åŸºç¡€å¥–åŠ±
            'score_growth': 1.0,           # åˆ†æ•°å¢é•¿åŸºç¡€å¥–åŠ±
            'survival': 0.01,              # ç”Ÿå­˜å¥–åŠ±
            'death_penalty': -10.0,        # æ­»äº¡æƒ©ç½š
            
            # === æ ¸å¿ƒå¯¹æŠ—å¥–åŠ± ===
            'eat_opponent_bonus': 15.0,    # ğŸ”¥ åå™¬æ•Œäººæ ¸å¿ƒå¥–åŠ±
            'split_to_eat_bonus': 20.0,    # ğŸ”¥ åˆ†è£‚è¿½å‡»åå™¬å¥–åŠ±
            'wasted_split_penalty': -5.0,  # æµªè´¹åˆ†è£‚æƒ©ç½š
            
            # === å›¢é˜Ÿæ’åå¥–åŠ± ===
            'team_rank_1_bonus': 200.0,    # ğŸ”¥ é˜Ÿä¼ç¬¬ä¸€åå¥–åŠ±
            'team_rank_2_bonus': 50.0,     # é˜Ÿä¼ç¬¬äºŒåå¥–åŠ±
            'team_rank_last_penalty': -100.0, # é˜Ÿä¼æœ€åä¸€åæƒ©ç½š

            # === æ¢ç´¢ä¸åä½œ ===
            'exploration_bonus': 1.5,      # æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±
            'team_coverage_bonus': 2.0,    # å›¢é˜Ÿåœ°å›¾è¦†ç›–åº¦å¥–åŠ±
        }
        
        # === çŠ¶æ€è¿½è¸ª ===
        self.history = {name: deque(maxlen=100) for name in player_names}
        self.step_count = 0
        
        # === æ¢ç´¢çŠ¶æ€è¿½è¸ª ===
        self.exploration_states = {
            name: ExplorationState(
                visited_cells=set(),
                cell_visit_count=defaultdict(int),
                cell_size=self.config.get('exploration_cell_size', 100)
            ) for name in player_names
        }

        # Add missing attribute for split efficiency window
        self.split_efficiency_window = self.config.get('split_efficiency_window', 30)

        # === Split and eat event tracking ===
        self.split_trackers = {name: deque(maxlen=5) for name in player_names}
        self.eat_trackers = {name: deque(maxlen=20) for name in player_names}

    def calculate_rewards(self, obs: Dict, rewards: Dict, dones: Dict, infos: Dict) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“çš„ä¼˜åŒ–å¥–åŠ±
        """
        self.step_count += 1
        final_rewards = {}

        for player_name in self.player_names:
            if player_name in infos and player_name in obs:
                player_info = infos[player_name]
                player_obs = obs[player_name]
                
                # è·å–ä¸Šä¸€çŠ¶æ€
                last_state = self.history[player_name][-1] if len(self.history[player_name]) > 0 else None

                # è®¡ç®—å•ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
                total_reward, reward_components = self._calculate_individual_reward(player_name, player_info, last_state, player_obs['action'])
                final_rewards[player_name] = total_reward

                # æ›´æ–°å†å²
                self.history[player_name].append(player_info)

        # åœ¨å›åˆç»“æŸæ—¶è®¡ç®—å›¢é˜Ÿå¥–åŠ±
        if any(dones.values()):
            team_rewards = self._calculate_team_rank_reward(infos)
            for player_name in self.player_names:
                if player_name in final_rewards:
                    final_rewards[player_name] += team_rewards

        return final_rewards

    def _calculate_individual_reward(self, player_name: str, player_info, last_state, action) -> Tuple[float, Dict[str, float]]:
        """
        è®¡ç®—å•ä¸ªæ™ºèƒ½ä½“çš„å¥–åŠ±
        """
        reward_components = {}
        
        # 1. åˆ†æ•°å¢é•¿å¥–åŠ± (åŸºç¡€)
        if last_state:
            score_growth = player_info['score'] - last_state['score']
            if score_growth > 0:
                reward_components['score_growth'] = score_growth * self.weights['score_growth']

        # 2. åå™¬æ•Œäººå¥–åŠ±
        # å‡è®¾ info['eaten_opponents'] æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«è¢«è¯¥ç©å®¶åƒæ‰çš„æ•Œæ–¹ä¿¡æ¯
        if 'eaten_opponents' in player_info and player_info['eaten_opponents']:
            num_eaten = len(player_info['eaten_opponents'])
            reward_components['eat_opponent_bonus'] = num_eaten * self.weights['eat_opponent_bonus']
            # è®°å½•åå™¬äº‹ä»¶ï¼Œç”¨äºåˆ†è£‚è¿½å‡»åˆ¤æ–­
            self._log_eat_event(player_name, player_info['eaten_opponents'])

        # 3. æ™ºèƒ½åˆ†è£‚è¿½å‡»å¥–åŠ±
        split_reward, split_penalty = self._calculate_split_reward(player_name, action, player_info, last_state)
        if split_reward > 0:
            reward_components['split_to_eat_bonus'] = split_reward
        if split_penalty < 0:
            reward_components['wasted_split_penalty'] = split_penalty

        # 4. æ¢ç´¢å¥–åŠ±
        reward_components['exploration_bonus'] = self._calculate_exploration_reward(player_name, player_info) * self.weights['exploration_bonus']

        # 5. æ­»äº¡æƒ©ç½š
        if player_info.get('is_dead', False):
            reward_components['death_penalty'] = self.weights['death_penalty']

        total_reward = sum(reward_components.values())
        return total_reward, reward_components

    def _calculate_split_reward(self, player_name: str, action, player_info, last_state) -> Tuple[float, float]:
        """
        è®¡ç®—åˆ†è£‚å¥–åŠ±æˆ–æƒ©ç½š
        action[2] > 0 è¡¨ç¤ºæ‰§è¡Œäº†åˆ†è£‚
        """
        # æ£€æŸ¥åˆ†è£‚åŠ¨ä½œ
        is_splitting = action and len(action) > 2 and action[2] > 0
        if is_splitting:
            # è®°å½•åˆ†è£‚äº‹ä»¶
            self.split_trackers[player_name].append({
                'step': self.step_count,
                'score_before_split': last_state['score'] if last_state else 0,
                'num_balls_before_split': len(last_state['player']) if last_state else 1
            })

        # æ£€æŸ¥æœ€è¿‘çš„åˆ†è£‚äº‹ä»¶æ˜¯å¦å¸¦æ¥äº†æ”¶ç›Š
        reward = 0
        penalty = 0
        tracker = self.split_trackers[player_name]
        if tracker:
            # æ£€æŸ¥æœ€è¿‘ä¸€æ¬¡åˆ†è£‚
            last_split = tracker[-1]
            steps_since_split = self.step_count - last_split['step']

            # åœ¨åˆ†è£‚åçš„ä¸€ä¸ªçª—å£æœŸå†…æ£€æŸ¥æ”¶ç›Š
            if 1 < steps_since_split < self.split_efficiency_window:
                # æ£€æŸ¥æ­¤çª—å£æœŸå†…æ˜¯å¦æœ‰åå™¬äº‹ä»¶
                if any(eat['step'] > last_split['step'] for eat in self.eat_trackers[player_name]):
                    reward = self.weights['split_to_eat_bonus']
                    # æ¸…é™¤å·²å¥–åŠ±çš„äº‹ä»¶ï¼Œé¿å…é‡å¤å¥–åŠ±
                    self.eat_trackers[player_name].clear()
                    tracker.pop()
            
            # å¦‚æœåˆ†è£‚åå¤ªä¹…æ²¡æœ‰æ”¶ç›Šï¼Œåˆ™æƒ©ç½š
            elif steps_since_split > self.split_efficiency_window:
                penalty = self.weights['wasted_split_penalty']
                tracker.pop() # ç§»é™¤æ—§çš„è¿½è¸ªï¼Œé¿å…é‡å¤æƒ©ç½š

        return reward, penalty

    def _log_eat_event(self, player_name: str, eaten_opponents: List):
        self.eat_trackers[player_name].append({
            'step': self.step_count,
            'opponents': eaten_opponents
        })

    def _calculate_team_rank_reward(self, infos: Dict) -> float:
        """
        æ ¹æ®æœ€ç»ˆé˜Ÿä¼æ’åè®¡ç®—å¥–åŠ±
        """
        if 'team_rank' in infos:
            team_rank = infos['team_rank']
            if self.team_id in team_rank:
                rank = team_rank[self.team_id]
                if rank == 1:
                    return self.weights['team_rank_1_bonus']
                elif rank == 2:
                    return self.weights['team_rank_2_bonus']
                else:
                    return self.weights['team_rank_last_penalty']
        return 0.0

    def _check_eaten_opponents(self, current_overlap, last_overlap) -> int:
        # æ­¤å‡½æ•°çš„åŠŸèƒ½å·²è¢«åˆå¹¶åˆ°å¯¹ info['eaten_opponents'] çš„ç›´æ¥æ£€æŸ¥ä¸­
        return 0

    def _calculate_exploration_reward(self, player_name: str, player_info) -> float:
        """
        è®¡ç®—æ¢ç´¢å¥–åŠ±
        """
        exploration_reward = 0
        player_pos = (player_info['player'][0][0], player_info['player'][0][1]) # å–ç¬¬ä¸€ä¸ªçƒçš„ä½ç½®
        cell_size = self.exploration_states[player_name].cell_size
        cell = (int(player_pos[0] // cell_size), int(player_pos[1] // cell_size))

        if cell not in self.exploration_states[player_name].visited_cells:
            exploration_reward = 1.0
            self.exploration_states[player_name].visited_cells.add(cell)
        
        return exploration_reward

    def reset(self):
        """
        é‡ç½®æ‰€æœ‰çŠ¶æ€
        """
        self.history = {name: deque(maxlen=100) for name in self.player_names}
        self.step_count = 0
        self.exploration_states = {
            name: ExplorationState(
                visited_cells=set(),
                cell_visit_count=defaultdict(int),
                cell_size=self.config.get('exploration_cell_size', 100)
            ) for name in self.player_names
        }

        # Reset split and eat event tracking
        self.split_trackers = {name: deque(maxlen=5) for name in self.player_names}
        self.eat_trackers = {name: deque(maxlen=20) for name in self.player_names}

class MultiAgentRewardCallback(BaseCallback):
    """
    Custom callback for multi-agent reward calculation during training.
    """
    def __init__(self, reward_calculator: MultiAgentRewardCalculator, verbose=0):
        super(MultiAgentRewardCallback, self).__init__(verbose)
        self.reward_calculator = reward_calculator

    def _on_step(self) -> bool:
        """
        Called after each environment step.
        """
        # For now, this callback is mainly for logging
        # The actual reward calculation is handled in the environment wrapper
        return True


def make_env(env_config, rank):
    """
    Helper function to create and wrap environment.
    """
    def _init():
        env = MultiAgentGoBiggerEnv(env_config)
        env = Monitor(env, f"{env_config['log_path']}/{rank}")
        return env
    return _init

from stable_baselines3.common.vec_env import VecEnv

class MultiAgentParallelWrapper(VecEnv):
    """
    Wrapper that converts multi-agent environment to stable-baselines3 VecEnv format,
    supporting parameter sharing for multi-agent training.
    """
    def __init__(self, env, rl_player_names):
        self.env = env
        self.rl_player_names = rl_player_names
        self.num_agents = len(rl_player_names)
        
        # ä»ç¬¬ä¸€ä¸ªRLæ™ºèƒ½ä½“è·å– observation å’Œ action space
        # åœ¨å‚æ•°å…±äº«çš„è®¾å®šä¸‹ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“çš„ space éƒ½æ˜¯ä¸€æ ·çš„
        agent_obs_space = self.env.observation_space[self.rl_player_names[0]]
        agent_action_space = self.env.action_space[self.rl_player_names[0]]

        super().__init__(self.num_agents, agent_obs_space, agent_action_space)

    def reset(self):
        obs_dict = self.env.reset()
        # å°†è§‚å¯Ÿå­—å…¸è½¬æ¢ä¸ºä¸€ä¸ª numpy æ•°ç»„
        return self._obs_dict_to_array(obs_dict)

    def step_async(self, actions):
        # å°†PPOæ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œæ•°ç»„ï¼Œè½¬æ¢ä¸ºç¯å¢ƒéœ€è¦çš„å­—å…¸æ ¼å¼
        action_dict = {name: actions[i] for i, name in enumerate(self.rl_player_names)}
        self.actions_to_step = action_dict

    def step_wait(self):
        obs, rewards, dones, infos = self.env.step(self.actions_to_step)
        
        # å°†è¾“å‡ºçš„å­—å…¸è½¬æ¢ä¸ºæ•°ç»„
        obs_arr = self._obs_dict_to_array(obs)
        rewards_arr = np.array([rewards.get(name, 0) for name in self.rl_player_names], dtype=np.float32)
        dones_arr = np.array([dones.get(name, False) for name in self.rl_player_names], dtype=np.bool_)
        
        # å¦‚æœæ‰€æœ‰æ™ºèƒ½ä½“éƒ½å®Œæˆäº†ï¼Œæˆ–è€…æ¸¸æˆç»“æŸäº†ï¼Œæˆ‘ä»¬éœ€è¦é‡ç½®
        if dones.get("__all__", False):
            obs_arr = self.reset()

        return obs_arr, rewards_arr, dones_arr, [infos] * self.num_agents

    def close(self):
        self.env.close()

    def get_attr(self, attr_name, indices=None):
        return getattr(self.env, attr_name)

    def set_attr(self, attr_name, value, indices=None):
        return setattr(self.env, attr_name, value)

    def env_is_wrapped(self, wrapper_class, indices=None):
        return [isinstance(self.env, wrapper_class)] * self.num_agents

    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        return [getattr(self.env, method_name)(*method_args, **method_kwargs)] * self.num_agents

    def _obs_dict_to_array(self, obs_dict):
        # ç¡®ä¿æˆ‘ä»¬æ€»æ˜¯ä»¥å›ºå®šçš„é¡ºåºä»å­—å…¸ä¸­æå–è§‚å¯Ÿå€¼
        return np.stack([obs_dict.get(name, np.zeros(self.observation_space.shape)) for name in self.rl_player_names])

def train_multi_agent(
    total_timesteps=2_000_000,
    num_rl_agents=3, # æˆ‘ä»¬è®­ç»ƒçš„RLæ™ºèƒ½ä½“æ•°é‡
    num_food_hunters=3,
    num_aggressive_bots=3,
    env_server_host='127.0.0.1',
    env_server_port=10086,
    model_save_path='models/multi_agent_ppo_shared',
    log_path='logs/multi_agent_log_shared',
    learning_rate=3e-4,
    batch_size=2048, # å¯¹äºå¤šæ™ºèƒ½ä½“ï¼Œå¯ä»¥é€‚å½“å¢å¤§
    n_steps=4096, # æ¯ä¸ªæ™ºèƒ½ä½“åœ¨æ›´æ–°å‰æ”¶é›†çš„æ­¥æ•°
    gamma=0.99,
    ent_coef=0.01,
):
    """
    Multi-agent training using parameter sharing PPO.
    """
    if not STABLE_BASELINES_AVAILABLE:
        print("ğŸ›‘ stable-baselines3 æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒã€‚")
        return

    # === 1. ç¯å¢ƒè®¾ç½® ===
    rl_player_names = [f'rl_player_{i}' for i in range(num_rl_agents)]
    bot_player_names = [f'food_hunter_{i}' for i in range(num_food_hunters)] + \
                       [f'aggressive_bot_{i}' for i in range(num_aggressive_bots)]

    env_config = {
        'player_num_per_team': num_rl_agents, # RL é˜Ÿä¼çš„ç©å®¶æ•°
        'team_num': 2,
        'match_time': 600,
        'map_width': 1000,
        'map_height': 1000,
        'gamemode': 't2', # 2é˜Ÿæ¨¡å¼
        'reward_config': { 'score_reward_weight': 1 }, # åŸºç¡€å¥–åŠ±
        'server_host': env_server_host,
        'server_port': env_server_port,
        'team_player_names': {0: rl_player_names, 1: bot_player_names},
        # åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹å¼æ¥æŒ‡å®šä¸åŒbotçš„ç­–ç•¥
        # è¿™éœ€è¦åœ¨ multi_agent_gobigger_gym_env.py ä¸­æ”¯æŒ
        # æš‚æ—¶æˆ‘ä»¬å‡è®¾ç¯å¢ƒèƒ½å¤„ç†æ··åˆç±»å‹çš„bot
        'ai_opponent_type': 'mixed', 
        'rl_player_names': rl_player_names,
        'log_path': log_path,
    }

    # åˆ›å»ºå¹¶åŒ…è£…ç¯å¢ƒ
    raw_env = MultiAgentGoBiggerEnv(env_config)
    vec_env = MultiAgentParallelWrapper(raw_env, rl_player_names)

    # === 2. æ¨¡å‹å®šä¹‰ (å‚æ•°å…±äº«) ===
    model = PPO(
        'MlpPolicy', 
        vec_env,
        learning_rate=learning_rate,
        n_steps=n_steps // vec_env.num_envs, # n_steps æ˜¯æ€»çš„ï¼Œéœ€è¦é™¤ä»¥agentæ•°é‡
        batch_size=batch_size,
        gamma=gamma,
        ent_coef=ent_coef,
        verbose=1,
        tensorboard_log=log_path
    )

    # === 3. è®­ç»ƒ ===
    print("ğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“ (å‚æ•°å…±äº«) è®­ç»ƒ...")
    print(f"ğŸ¤– RL æ™ºèƒ½ä½“: {num_rl_agents}")
    print(f"âš”ï¸ å¯¹æ‰‹: {num_food_hunters} é£Ÿç‰©çŒæ‰‹, {num_aggressive_bots} æ”»å‡»æ€§AI")
    print(f"ğŸ’¾ æ¨¡å‹å°†ä¿å­˜è‡³: {model_save_path}")
    print(f"ğŸ“Š æ—¥å¿—å°†è®°å½•äº: {log_path}")

    # è®¾ç½®å›è°ƒæ¥å®šæœŸä¿å­˜æ¨¡å‹
    from stable_baselines3.common.callbacks import CheckpointCallback
    checkpoint_callback = CheckpointCallback(save_freq=25000, save_path=model_save_path, name_prefix='rl_model')

    model.learn(
        total_timesteps=total_timesteps,
        tb_log_name="ppo_gobigger_multi_shared",
        callback=checkpoint_callback
    )

    model.save(f"{model_save_path}_final")
    print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³ {model_save_path}_final")

    vec_env.close()

if __name__ == '__main__':
    train_multi_agent()
