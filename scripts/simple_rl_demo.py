#!/usr/bin/env python3
"""
å¿«é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¼”ç¤º
ä½¿ç”¨ç®€åŒ–é…ç½®å¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹
"""
import sys
import os
from pathlib import Path
import numpy as np
import time

# è·¯å¾„è®¾ç½®
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

from gobigger_gym_env import GoBiggerEnv

def simple_q_learning_demo():
    """ç®€å•çš„Q-learningæ¼”ç¤º"""
    print("ğŸ§  ç®€å•Q-learningæ¼”ç¤º")
    
    # åˆ›å»ºç¯å¢ƒ
    env = GoBiggerEnv({'max_episode_steps': 200})
    
    # ç®€åŒ–çŠ¶æ€ç©ºé—´ï¼šåªä½¿ç”¨å‰20ä¸ªç‰¹å¾
    state_dim = 20
    action_dim = 3  # [direction_x, direction_y, action_type]
    
    # ç®€å•çš„Qè¡¨ï¼ˆç¦»æ•£åŒ–çŠ¶æ€å’ŒåŠ¨ä½œï¼‰
    q_table = {}
    
    # è¶…å‚æ•°
    learning_rate = 0.1
    discount_factor = 0.95
    epsilon = 1.0  # æ¢ç´¢ç‡
    epsilon_decay = 0.995
    epsilon_min = 0.01
    episodes = 50
    
    def discretize_state(obs):
        """å°†è¿ç»­çŠ¶æ€ç¦»æ•£åŒ–"""
        # åªå–å‰20ä¸ªç‰¹å¾å¹¶é‡åŒ–åˆ°[-2, 2]èŒƒå›´å†…çš„10ä¸ªç¦»æ•£å€¼
        discrete_obs = np.clip(obs[:state_dim], -2, 2)
        discrete_obs = np.round(discrete_obs * 2).astype(int)  # è½¬æ¢ä¸ºæ•´æ•°
        return tuple(discrete_obs)
    
    def discretize_action(action_idx):
        """å°†åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºè¿ç»­åŠ¨ä½œ"""
        actions = [
            [0.0, 0.0, 0],      # ä¸åŠ¨
            [1.0, 0.0, 0],      # å³ç§»
            [-1.0, 0.0, 0],     # å·¦ç§»
            [0.0, 1.0, 0],      # ä¸Šç§»
            [0.0, -1.0, 0],     # ä¸‹ç§»
            [0.7, 0.7, 1],      # å³ä¸Šåçƒ
            [0.0, 0.0, 2],      # åˆ†è£‚
        ]
        return actions[action_idx % len(actions)]
    
    episode_rewards = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        state = discretize_state(obs)
        total_reward = 0
        steps = 0
        
        while steps < 200:
            # Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            if np.random.random() < epsilon:
                action_idx = np.random.randint(0, 7)  # éšæœºåŠ¨ä½œ
            else:
                # é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
                if state in q_table:
                    action_idx = np.argmax(q_table[state])
                else:
                    action_idx = np.random.randint(0, 7)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            action = discretize_action(action_idx)
            next_obs, reward, terminated, truncated, info = env.step(action)
            next_state = discretize_state(next_obs)
            
            # æ›´æ–°Qè¡¨
            if state not in q_table:
                q_table[state] = np.zeros(7)
            
            if next_state not in q_table:
                q_table[next_state] = np.zeros(7)
            
            # Q-learningæ›´æ–°å…¬å¼
            best_next_action = np.argmax(q_table[next_state])
            td_target = reward + discount_factor * q_table[next_state][best_next_action]
            td_error = td_target - q_table[state][action_idx]
            q_table[state][action_idx] += learning_rate * td_error
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(total_reward)
        
        # è¡°å‡æ¢ç´¢ç‡
        epsilon = max(epsilon_min, epsilon * epsilon_decay)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, Îµ={epsilon:.3f}, Qè¡¨å¤§å°={len(q_table)}")
    
    print(f"\nğŸ“Š è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-10:]):.3f}")
    print(f"ğŸ§  å­¦ä¹ åˆ°çš„çŠ¶æ€æ•°: {len(q_table)}")
    
    return q_table, episode_rewards

def genetic_algorithm_demo():
    """ç®€å•çš„é—ä¼ ç®—æ³•æ¼”ç¤º"""
    print("ğŸ§¬ é—ä¼ ç®—æ³•æ¼”ç¤º")
    
    env = GoBiggerEnv({'max_episode_steps': 100})
    
    # ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆç®€åŒ–ä¸ºçº¿æ€§æ¨¡å‹ï¼‰
    input_dim = 20  # ä½¿ç”¨å‰20ä¸ªç‰¹å¾
    hidden_dim = 10
    output_dim = 3  # [direction_x, direction_y, action_type]
    
    population_size = 20
    generations = 10
    mutation_rate = 0.1
    
    def create_individual():
        """åˆ›å»ºä¸€ä¸ªä¸ªä½“ï¼ˆç¥ç»ç½‘ç»œæƒé‡ï¼‰"""
        w1 = np.random.randn(input_dim, hidden_dim) * 0.5
        b1 = np.random.randn(hidden_dim) * 0.1
        w2 = np.random.randn(hidden_dim, output_dim) * 0.5
        b2 = np.random.randn(output_dim) * 0.1
        return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}
    
    def forward(individual, state):
        """å‰å‘ä¼ æ’­"""
        x = state[:input_dim]
        h = np.tanh(np.dot(x, individual['w1']) + individual['b1'])
        output = np.dot(h, individual['w2']) + individual['b2']
        
        # è¾“å‡ºè½¬æ¢ä¸ºåŠ¨ä½œ
        direction_x = np.tanh(output[0])  # [-1, 1]
        direction_y = np.tanh(output[1])  # [-1, 1]
        action_type = int(np.clip(output[2], 0, 2))  # [0, 1, 2]
        
        return [direction_x, direction_y, action_type]
    
    def evaluate_individual(individual, episodes=3):
        """è¯„ä¼°ä¸ªä½“æ€§èƒ½"""
        total_rewards = []
        
        for _ in range(episodes):
            obs, info = env.reset()
            total_reward = 0
            steps = 0
            
            while steps < 100:
                action = forward(individual, obs)
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                steps += 1
                
                if terminated or truncated:
                    break
            
            total_rewards.append(total_reward)
        
        return np.mean(total_rewards)
    
    def crossover(parent1, parent2):
        """äº¤å‰æ“ä½œ"""
        child = {}
        for key in parent1.keys():
            mask = np.random.random(parent1[key].shape) > 0.5
            child[key] = np.where(mask, parent1[key], parent2[key])
        return child
    
    def mutate(individual):
        """å˜å¼‚æ“ä½œ"""
        mutated = {}
        for key in individual.keys():
            mutated[key] = individual[key].copy()
            mask = np.random.random(individual[key].shape) < mutation_rate
            mutated[key][mask] += np.random.randn(*individual[key].shape)[mask] * 0.1
        return mutated
    
    # åˆå§‹åŒ–ç§ç¾¤
    population = [create_individual() for _ in range(population_size)]
    
    best_scores = []
    
    for generation in range(generations):
        # è¯„ä¼°æ‰€æœ‰ä¸ªä½“
        scores = [evaluate_individual(ind) for ind in population]
        
        # è®°å½•æœ€ä½³æ€§èƒ½
        best_score = max(scores)
        best_scores.append(best_score)
        avg_score = np.mean(scores)
        
        print(f"Generation {generation + 1}: æœ€ä½³={best_score:.3f}, å¹³å‡={avg_score:.3f}")
        
        # é€‰æ‹©ï¼ˆé”¦æ ‡èµ›é€‰æ‹©ï¼‰
        def tournament_selection():
            tournament_size = 3
            tournament = np.random.choice(len(population), tournament_size, replace=False)
            tournament_scores = [scores[i] for i in tournament]
            winner_idx = tournament[np.argmax(tournament_scores)]
            return population[winner_idx]
        
        # ç”Ÿæˆæ–°ä¸€ä»£
        new_population = []
        
        # ä¿ç•™æœ€ä½³ä¸ªä½“
        best_idx = np.argmax(scores)
        new_population.append(population[best_idx])
        
        # äº¤å‰å’Œå˜å¼‚
        while len(new_population) < population_size:
            parent1 = tournament_selection()
            parent2 = tournament_selection()
            child = crossover(parent1, parent2)
            child = mutate(child)
            new_population.append(child)
        
        population = new_population
    
    print(f"\nğŸ† è¿›åŒ–å®Œæˆï¼æœ€ä½³å¾—åˆ†: {max(best_scores):.3f}")
    return population[np.argmax([evaluate_individual(ind) for ind in population])]

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ é€‰æ‹©æ¼”ç¤ºç®—æ³•:")
    print("1. Q-learning (åŸºäºè¡¨æ ¼)")
    print("2. é—ä¼ ç®—æ³• (ç¥ç»è¿›åŒ–)")
    print("3. éšæœºæ™ºèƒ½ä½“å¯¹æ¯”")
    
    choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
    
    if choice == '1':
        start_time = time.time()
        q_table, rewards = simple_q_learning_demo()
        print(f"â±ï¸  ç”¨æ—¶: {time.time() - start_time:.2f}ç§’")
        
    elif choice == '2':
        start_time = time.time()
        best_individual = genetic_algorithm_demo()
        print(f"â±ï¸  ç”¨æ—¶: {time.time() - start_time:.2f}ç§’")
        
    elif choice == '3':
        print("ğŸ² éšæœºæ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•")
        env = GoBiggerEnv({'max_episode_steps': 200})
        
        random_rewards = []
        for episode in range(20):
            obs, info = env.reset()
            total_reward = 0
            steps = 0
            
            while steps < 200:
                action = np.random.uniform(env.action_space_low, env.action_space_high)
                action[2] = int(action[2])
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                steps += 1
                
                if terminated or truncated:
                    break
            
            random_rewards.append(total_reward)
        
        print(f"ğŸ² éšæœºç­–ç•¥å¹³å‡å¥–åŠ±: {np.mean(random_rewards):.3f} Â± {np.std(random_rewards):.3f}")
        
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ’¡ è¿™äº›ç®€å•ç®—æ³•å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬åŸç†")
    print("ğŸ’¡ æ›´å¤æ‚çš„ç®—æ³•ï¼ˆå¦‚PPOã€DQNï¼‰éœ€è¦å®‰è£… stable-baselines3")

if __name__ == "__main__":
    main()
