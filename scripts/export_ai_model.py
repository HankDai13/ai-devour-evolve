#!/usr/bin/env python3
"""
AIæ¨¡å‹å¯¼å‡ºå·¥å…·
å°†è®­ç»ƒå¥½çš„PPOæ¨¡å‹å¯¼å‡ºä¸ºC++å¯ç”¨çš„æ ¼å¼
"""
import os
import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

from gobigger_gym_env import GoBiggerEnv

try:
    from stable_baselines3 import PPO
    import onnx
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€éœ€åº“")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install stable-baselines3 onnx")
    sys.exit(1)

class ModelExporter:
    """AIæ¨¡å‹å¯¼å‡ºå·¥å…·ç±»"""
    
    def __init__(self):
        self.observation_space_size = 400  # GoBiggerç¯å¢ƒçš„è§‚å¯Ÿç©ºé—´å¤§å°
        
    def find_best_model(self, search_dirs=["models", "checkpoints"]):
        """æŸ¥æ‰¾æœ€ä½³å¯ç”¨æ¨¡å‹"""
        model_files = []
        
        for search_dir in search_dirs:
            if not os.path.exists(search_dir):
                continue
                
            for file in os.listdir(search_dir):
                if file.endswith('.zip') and 'PPO' in file:
                    model_path = os.path.join(search_dir, file)
                    model_files.append({
                        'path': model_path,
                        'name': file,
                        'size': os.path.getsize(model_path),
                        'modified': os.path.getmtime(model_path)
                    })
        
        if not model_files:
            return None
            
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        model_files.sort(key=lambda x: x['modified'], reverse=True)
        return model_files[0]['path']
    
    def load_model(self, model_path=None):
        """åŠ è½½PPOæ¨¡å‹"""
        if model_path is None:
            model_path = self.find_best_model()
            
        if model_path is None or not os.path.exists(model_path):
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
            return None
            
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        try:
            model = PPO.load(model_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {os.path.basename(model_path)}")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def create_dummy_input(self):
        """åˆ›å»ºç”¨äºå¯¼å‡ºçš„è™šæ‹Ÿè¾“å…¥"""
        # åˆ›å»ºç¬¦åˆGoBiggerè§‚å¯Ÿç©ºé—´çš„è™šæ‹Ÿè¾“å…¥
        return torch.randn(1, self.observation_space_size).float()
    
    def export_to_onnx(self, model, output_path="exported_models/ai_model.onnx"):
        """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
        print("ğŸ”„ å¼€å§‹å¯¼å‡ºONNXæ¨¡å‹...")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_input = self.create_dummy_input()
            
            # ğŸ”¥ ç¡®ä¿æ¨¡å‹å’Œè¾“å…¥åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼ˆCPUï¼‰
            device = torch.device('cpu')
            model.policy = model.policy.to(device)
            dummy_input = dummy_input.to(device)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            model.policy.eval()
            
            # å¯¼å‡ºONNX
            torch.onnx.export(
                model.policy,  # æ¨¡å‹
                dummy_input,   # è™šæ‹Ÿè¾“å…¥
                output_path,   # è¾“å‡ºè·¯å¾„
                export_params=True,        # å¯¼å‡ºå‚æ•°
                opset_version=11,          # ONNXæ“ä½œé›†ç‰ˆæœ¬
                do_constant_folding=True,  # å¸¸é‡æŠ˜å ä¼˜åŒ–
                input_names=['observation'],  # è¾“å…¥åç§°
                output_names=['action_logits', 'value'],  # è¾“å‡ºåç§°
                dynamic_axes={
                    'observation': {0: 'batch_size'},
                    'action_logits': {0: 'batch_size'},
                    'value': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            
            # éªŒè¯ONNXæ¨¡å‹
            self.verify_onnx_model(output_path, dummy_input, model)
            
        except Exception as e:
            print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            return False
        
        return True
    
    def export_to_torchscript(self, model, output_path="exported_models/ai_model_traced.pt"):
        """å¯¼å‡ºæ¨¡å‹ä¸ºTorchScriptæ ¼å¼"""
        print("ğŸ”„ å¼€å§‹å¯¼å‡ºTorchScriptæ¨¡å‹...")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            dummy_input = self.create_dummy_input()
            
            # ğŸ”¥ ç¡®ä¿æ¨¡å‹å’Œè¾“å…¥åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼ˆCPUï¼‰
            device = torch.device('cpu')
            model.policy = model.policy.to(device)
            dummy_input = dummy_input.to(device)
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            model.policy.eval()
            
            # ä½¿ç”¨traceå¯¼å‡º
            with torch.no_grad():
                traced_model = torch.jit.trace(model.policy, dummy_input)
            
            # ä¿å­˜æ¨¡å‹
            traced_model.save(output_path)
            
            print(f"âœ… TorchScriptæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {output_path}")
            
            # éªŒè¯TorchScriptæ¨¡å‹
            self.verify_torchscript_model(output_path, dummy_input, model)
            
        except Exception as e:
            print(f"âŒ TorchScriptå¯¼å‡ºå¤±è´¥: {e}")
            return False
        
        return True
    
    def verify_onnx_model(self, onnx_path, test_input, original_model):
        """éªŒè¯ONNXæ¨¡å‹è¾“å‡ºä¸€è‡´æ€§"""
        try:
            import onnxruntime as ort
            
            # åˆ›å»ºONNXæ¨ç†ä¼šè¯
            ort_session = ort.InferenceSession(onnx_path)
            
            # è·å–åŸå§‹æ¨¡å‹è¾“å‡º
            with torch.no_grad():
                original_output = original_model.policy(test_input)
                if isinstance(original_output, tuple):
                    original_action_logits = original_output[0].numpy()
                    original_value = original_output[1].numpy()
                else:
                    original_action_logits = original_output.numpy()
                    original_value = None
            
            # è·å–ONNXæ¨¡å‹è¾“å‡º
            ort_inputs = {ort_session.get_inputs()[0].name: test_input.numpy()}
            ort_outputs = ort_session.run(None, ort_inputs)
            
            # æ¯”è¾ƒè¾“å‡º
            action_logits_diff = np.abs(original_action_logits - ort_outputs[0]).max()
            print(f"ğŸ“Š åŠ¨ä½œlogitsæœ€å¤§å·®å¼‚: {action_logits_diff:.6f}")
            
            if len(ort_outputs) > 1 and original_value is not None:
                value_diff = np.abs(original_value - ort_outputs[1]).max()
                print(f"ğŸ“Š ä»·å€¼å‡½æ•°æœ€å¤§å·®å¼‚: {value_diff:.6f}")
            
            if action_logits_diff < 1e-5:
                print("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
                return True
            else:
                print("âš ï¸  ONNXæ¨¡å‹è¾“å‡ºå·®å¼‚è¾ƒå¤§ï¼Œè¯·æ£€æŸ¥")
                return False
                
        except ImportError:
            print("âš ï¸  æœªå®‰è£…onnxruntimeï¼Œè·³è¿‡ONNXéªŒè¯")
            print("å®‰è£…å‘½ä»¤: pip install onnxruntime")
            return True
        except Exception as e:
            print(f"âŒ ONNXéªŒè¯å¤±è´¥: {e}")
            return False
    
    def verify_torchscript_model(self, script_path, test_input, original_model):
        """éªŒè¯TorchScriptæ¨¡å‹è¾“å‡ºä¸€è‡´æ€§"""
        try:
            # åŠ è½½TorchScriptæ¨¡å‹
            loaded_model = torch.jit.load(script_path)
            
            # è·å–åŸå§‹æ¨¡å‹è¾“å‡º
            with torch.no_grad():
                original_output = original_model.policy(test_input)
                if isinstance(original_output, tuple):
                    original_action_logits = original_output[0]
                    original_value = original_output[1]
                else:
                    original_action_logits = original_output
                    original_value = None
            
            # è·å–TorchScriptæ¨¡å‹è¾“å‡º
            with torch.no_grad():
                script_output = loaded_model(test_input)
                if isinstance(script_output, tuple):
                    script_action_logits = script_output[0]
                    script_value = script_output[1]
                else:
                    script_action_logits = script_output
                    script_value = None
            
            # æ¯”è¾ƒè¾“å‡º
            action_logits_diff = torch.abs(original_action_logits - script_action_logits).max().item()
            print(f"ğŸ“Š åŠ¨ä½œlogitsæœ€å¤§å·®å¼‚: {action_logits_diff:.6f}")
            
            if original_value is not None and script_value is not None:
                value_diff = torch.abs(original_value - script_value).max().item()
                print(f"ğŸ“Š ä»·å€¼å‡½æ•°æœ€å¤§å·®å¼‚: {value_diff:.6f}")
            
            if action_logits_diff < 1e-5:
                print("âœ… TorchScriptæ¨¡å‹éªŒè¯é€šè¿‡")
                return True
            else:
                print("âš ï¸  TorchScriptæ¨¡å‹è¾“å‡ºå·®å¼‚è¾ƒå¤§ï¼Œè¯·æ£€æŸ¥")
                return False
                
        except Exception as e:
            print(f"âŒ TorchScriptéªŒè¯å¤±è´¥: {e}")
            return False
    
    def create_model_info(self, model, output_dir="exported_models"):
        """åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶"""
        info = {
            "model_type": "PPO",
            "observation_space_size": self.observation_space_size,
            "action_space": {
                "type": "continuous",
                "shape": [3],  # [dx, dy, action_type]
                "low": [-1.0, -1.0, 0],
                "high": [1.0, 1.0, 2]
            },
            "input_format": "float32 array of shape (batch_size, 400)",
            "output_format": {
                "action_logits": "float32 array of shape (batch_size, 3)",
                "value": "float32 array of shape (batch_size, 1)"
            },
            "preprocessing": {
                "normalization": "Environment handles normalization",
                "feature_extraction": "Raw observation from GoBigger environment"
            },
            "usage_notes": [
                "Model expects observations from GoBiggerEnv with 400 features",
                "Action type should be rounded to nearest integer (0, 1, or 2)",
                "dx, dy should be clipped to [-1.0, 1.0] range",
                "Model was trained with Gemini-optimized hyperparameters"
            ]
        }
        
        import json
        info_path = os.path.join(output_dir, "model_info.json")
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å·²ä¿å­˜: {info_path}")
        return info_path
    
    def create_cpp_header(self, output_dir="exported_models"):
        """åˆ›å»ºC++é›†æˆæ‰€éœ€çš„å¤´æ–‡ä»¶"""
        header_content = '''#pragma once
#include <vector>
#include <string>
#include <memory>

namespace GoBigger {
namespace AI {

// AIæ¨¡å‹æ¨ç†æ¥å£
class ModelInference {
public:
    virtual ~ModelInference() = default;
    
    // åŠ è½½æ¨¡å‹
    virtual bool loadModel(const std::string& model_path) = 0;
    
    // æ‰§è¡Œæ¨ç†
    virtual std::vector<float> predict(const std::vector<float>& observation) = 0;
    
    // è·å–åŠ¨ä½œ
    virtual std::vector<float> getAction(const std::vector<float>& observation) = 0;
};

// ONNXæ¨¡å‹æ¨ç†å®ç°
class ONNXInference : public ModelInference {
private:
    class Impl;
    std::unique_ptr<Impl> pImpl;
    
public:
    ONNXInference();
    ~ONNXInference();
    
    bool loadModel(const std::string& model_path) override;
    std::vector<float> predict(const std::vector<float>& observation) override;
    std::vector<float> getAction(const std::vector<float>& observation) override;
};

// å¸¸é‡å®šä¹‰
constexpr int OBSERVATION_SIZE = 400;
constexpr int ACTION_SIZE = 3;

// åŠ¨ä½œç±»å‹
enum class ActionType {
    MOVE = 0,     // ç§»åŠ¨
    SPLIT = 1,    // åˆ†è£‚
    EJECT = 2     // å–·å°„
};

// åŠ¨ä½œç»“æ„
struct Action {
    float dx;        // xæ–¹å‘ç§»åŠ¨ [-1.0, 1.0]
    float dy;        // yæ–¹å‘ç§»åŠ¨ [-1.0, 1.0]
    ActionType type; // åŠ¨ä½œç±»å‹
    
    Action(float dx = 0.0f, float dy = 0.0f, ActionType type = ActionType::MOVE)
        : dx(dx), dy(dy), type(type) {}
};

} // namespace AI
} // namespace GoBigger
'''
        
        header_path = os.path.join(output_dir, "AIModelInterface.h")
        with open(header_path, 'w', encoding='utf-8') as f:
            f.write(header_content)
        
        print(f"ğŸ“„ C++å¤´æ–‡ä»¶å·²ç”Ÿæˆ: {header_path}")
        return header_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ AIæ¨¡å‹å¯¼å‡ºå·¥å…·")
    print("=" * 50)
    
    exporter = ModelExporter()
    
    # 1. æŸ¥æ‰¾å’ŒåŠ è½½æ¨¡å‹
    model = exporter.load_model()
    if model is None:
        return
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("exported_models", exist_ok=True)
    
    success_count = 0
    
    # 3. å¯¼å‡ºONNXæ ¼å¼
    if exporter.export_to_onnx(model):
        success_count += 1
    
    # 4. å¯¼å‡ºTorchScriptæ ¼å¼
    if exporter.export_to_torchscript(model):
        success_count += 1
    
    # 5. åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
    exporter.create_model_info(model)
    
    # 6. åˆ›å»ºC++å¤´æ–‡ä»¶
    exporter.create_cpp_header()
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š å¯¼å‡ºå®Œæˆï¼æˆåŠŸå¯¼å‡º {success_count}/2 ç§æ ¼å¼")
    print("\nğŸ“ å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨:")
    
    export_dir = "exported_models"
    if os.path.exists(export_dir):
        for file in os.listdir(export_dir):
            file_path = os.path.join(export_dir, file)
            size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            print(f"  â€¢ {file} ({size:.2f} MB)")
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("  1. ONNXæ ¼å¼ (.onnx): æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œè·¨å¹³å°å…¼å®¹æ€§å¥½")
    print("  2. TorchScriptæ ¼å¼ (.pt): é€‚ç”¨äºPyTorch C++å‰ç«¯")
    print("  3. æ¨¡å‹ä¿¡æ¯ (model_info.json): åŒ…å«é›†æˆæ‰€éœ€çš„è¯¦ç»†ä¿¡æ¯")
    print("  4. C++å¤´æ–‡ä»¶ (AIModelInterface.h): é›†æˆå‚è€ƒæ¥å£")
    print("\nğŸ”„ ä¸‹ä¸€æ­¥:")
    print("  â€¢ å°†å¯¼å‡ºçš„æ¨¡å‹æ–‡ä»¶æ‹·è´åˆ°C++é¡¹ç›®ä¸­")
    print("  â€¢ æ ¹æ®AIModelInterface.hå®ç°æ¨ç†æ¥å£")
    print("  â€¢ åœ¨æ–°çš„AIé›†æˆåˆ†æ”¯ä¸­å¼€å§‹é›†æˆå·¥ä½œ")

if __name__ == "__main__":
    main()
