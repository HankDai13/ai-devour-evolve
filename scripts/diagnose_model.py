"""
ONNXæ¨¡å‹è¯Šæ–­å·¥å…·
ç”¨äºåˆ†æå’Œä¿®å¤RLè®­ç»ƒæ¨¡å‹çš„å…¼å®¹æ€§é—®é¢˜
"""

import onnx
import onnxruntime as ort
import numpy as np
import sys
import os
from pathlib import Path

def diagnose_onnx_model(model_path):
    """è¯Šæ–­ONNXæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
    
    print(f"ğŸ” è¯Šæ–­ONNXæ¨¡å‹: {model_path}")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    # è·å–æ–‡ä»¶ä¿¡æ¯
    file_size = os.path.getsize(model_path)
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")
    
    try:
        # åŠ è½½å’ŒéªŒè¯ONNXæ¨¡å‹
        print("\nğŸ”§ åŠ è½½ONNXæ¨¡å‹...")
        model = onnx.load(model_path)
        
        print("âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # éªŒè¯æ¨¡å‹
        print("\nğŸ” éªŒè¯æ¨¡å‹ç»“æ„...")
        try:
            onnx.checker.check_model(model)
            print("âœ… æ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹ç»“æ„éªŒè¯è­¦å‘Š: {e}")
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        graph = model.graph
        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ¨¡å‹åç§°: {model.graph.name if model.graph.name else 'Unnamed'}")
        print(f"   ONNXç‰ˆæœ¬: {model.opset_import[0].version if model.opset_import else 'Unknown'}")
        print(f"   èŠ‚ç‚¹æ•°é‡: {len(graph.node)}")
        
        # åˆ†æè¾“å…¥
        print(f"\nğŸ“¥ è¾“å…¥ä¿¡æ¯ ({len(graph.input)}ä¸ª):")
        for i, input_tensor in enumerate(graph.input):
            print(f"   è¾“å…¥ {i}:")
            print(f"     åç§°: {input_tensor.name}")
            print(f"     ç±»å‹: {input_tensor.type.tensor_type.elem_type}")
            
            # è·å–å½¢çŠ¶
            shape = []
            for dim in input_tensor.type.tensor_type.shape.dim:
                if dim.dim_value:
                    shape.append(dim.dim_value)
                elif dim.dim_param:
                    shape.append(f"'{dim.dim_param}'")
                else:
                    shape.append("?")
            print(f"     å½¢çŠ¶: {shape}")
        
        # åˆ†æè¾“å‡º
        print(f"\nğŸ“¤ è¾“å‡ºä¿¡æ¯ ({len(graph.output)}ä¸ª):")
        for i, output_tensor in enumerate(graph.output):
            print(f"   è¾“å‡º {i}:")
            print(f"     åç§°: {output_tensor.name}")
            print(f"     ç±»å‹: {output_tensor.type.tensor_type.elem_type}")
            
            # è·å–å½¢çŠ¶
            shape = []
            for dim in output_tensor.type.tensor_type.shape.dim:
                if dim.dim_value:
                    shape.append(dim.dim_value)
                elif dim.dim_param:
                    shape.append(f"'{dim.dim_param}'")
                else:
                    shape.append("?")
            print(f"     å½¢çŠ¶: {shape}")
        
    except Exception as e:
        print(f"âŒ ONNXæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ONNX Runtime
    print(f"\nğŸš€ æµ‹è¯•ONNX Runtimeæ¨ç†...")
    try:
        # åˆ›å»ºæ¨ç†ä¼šè¯
        session = ort.InferenceSession(model_path)
        
        print("âœ… ONNX Runtimeä¼šè¯åˆ›å»ºæˆåŠŸ")
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        input_info = session.get_inputs()[0]
        output_info = session.get_outputs()[0]
        
        print(f"\nğŸ”§ Runtimeä¿¡æ¯:")
        print(f"   è¾“å…¥åç§°: {input_info.name}")
        print(f"   è¾“å…¥å½¢çŠ¶: {input_info.shape}")
        print(f"   è¾“å…¥ç±»å‹: {input_info.type}")
        print(f"   è¾“å‡ºåç§°: {output_info.name}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output_info.shape}")
        print(f"   è¾“å‡ºç±»å‹: {output_info.type}")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        input_shape = input_info.shape
        
        # å¤„ç†åŠ¨æ€å½¢çŠ¶
        test_shape = []
        for dim in input_shape:
            if isinstance(dim, str) or dim == -1:
                test_shape.append(1)  # åŠ¨æ€ç»´åº¦è®¾ä¸º1
            else:
                test_shape.append(dim)
        
        # å¦‚æœç¬¬äºŒä¸ªç»´åº¦è¿˜æ˜¯åŠ¨æ€çš„ï¼Œå‡è®¾ä¸º400ï¼ˆè§‚å¯Ÿç©ºé—´å¤§å°ï¼‰
        if len(test_shape) > 1 and test_shape[1] == 1:
            test_shape[1] = 400
        
        print(f"   æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_shape}")
        
        # åˆ›å»ºéšæœºæµ‹è¯•æ•°æ®
        test_input = np.random.randn(*test_shape).astype(np.float32)
        
        # è¿è¡Œæ¨ç†
        result = session.run([output_info.name], {input_info.name: test_input})
        
        print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ!")
        print(f"   è¾“å‡ºå½¢çŠ¶: {result[0].shape}")
        print(f"   è¾“å‡ºæ ·ä¾‹: {result[0].flatten()[:5]}")  # æ˜¾ç¤ºå‰5ä¸ªå€¼
        
        return True
        
    except Exception as e:
        print(f"âŒ ONNX Runtimeæµ‹è¯•å¤±è´¥: {e}")
        print(f"   è¯¦ç»†é”™è¯¯ä¿¡æ¯: {type(e).__name__}: {str(e)}")
        return False

def fix_model_compatibility(model_path, output_path=None):
    """å°è¯•ä¿®å¤æ¨¡å‹å…¼å®¹æ€§é—®é¢˜"""
    
    if output_path is None:
        name, ext = os.path.splitext(model_path)
        output_path = f"{name}_fixed{ext}"
    
    print(f"\nğŸ”§ å°è¯•ä¿®å¤æ¨¡å‹å…¼å®¹æ€§...")
    print(f"è¾“å…¥: {model_path}")
    print(f"è¾“å‡º: {output_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = onnx.load(model_path)
        
        # ç®€åŒ–æ¨¡å‹ï¼ˆç§»é™¤ä¸å¿…è¦çš„èŠ‚ç‚¹ï¼‰
        from onnx import optimizer
        
        # åº”ç”¨ä¼˜åŒ–passes
        passes = [
            'eliminate_deadend',
            'eliminate_identity',
            'eliminate_nop_dropout',
            'eliminate_nop_pad',
            'eliminate_unused_initializer',
            'extract_constant_to_initializer',
            'fuse_add_bias_into_conv',
            'fuse_bn_into_conv',
            'fuse_consecutive_concats',
            'fuse_consecutive_log_softmax',
            'fuse_consecutive_reduce_unsqueeze',
            'fuse_consecutive_squeezes',
            'fuse_consecutive_transposes',
            'fuse_matmul_add_bias_into_gemm',
            'fuse_pad_into_conv',
            'fuse_transpose_into_gemm',
            'lift_lexical_references',
        ]
        
        optimized_model = optimizer.optimize(model, passes)
        
        # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
        onnx.save(optimized_model, output_path)
        
        print(f"âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ: {output_path}")
        
        # éªŒè¯ä¿®å¤åçš„æ¨¡å‹
        if diagnose_onnx_model(output_path):
            print(f"âœ… ä¿®å¤åçš„æ¨¡å‹éªŒè¯é€šè¿‡!")
            return True
        else:
            print(f"âŒ ä¿®å¤åçš„æ¨¡å‹ä»æœ‰é—®é¢˜")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿®å¤å¤±è´¥: {e}")
        return False

def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python diagnose_model.py <model_path> [--fix]")
        print("ç¤ºä¾‹:")
        print("  python diagnose_model.py my_model.onnx")
        print("  python diagnose_model.py my_model.onnx --fix")
        return
    
    model_path = sys.argv[1]
    should_fix = "--fix" in sys.argv
    
    # è¯Šæ–­æ¨¡å‹
    success = diagnose_onnx_model(model_path)
    
    if not success and should_fix:
        print(f"\nğŸ”§ æ¨¡å‹æœ‰é—®é¢˜ï¼Œå°è¯•ä¿®å¤...")
        fix_model_compatibility(model_path)
    elif success:
        print(f"\nâœ… æ¨¡å‹å®Œå…¨å…¼å®¹ï¼å¯ä»¥åœ¨GoBigger AIä¸­ä½¿ç”¨")
    else:
        print(f"\nâŒ æ¨¡å‹æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ --fix å‚æ•°å°è¯•ä¿®å¤")
        print(f"\nğŸ’¡ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ:")
        print(f"   1. åŠ¨æ€è¾“å…¥å½¢çŠ¶: ç¡®ä¿æ¨¡å‹æ”¯æŒbatch_size=1")
        print(f"   2. è¾“å…¥å¤§å°ä¸åŒ¹é…: æ£€æŸ¥è§‚å¯Ÿç©ºé—´æ˜¯å¦ä¸º400ç»´")
        print(f"   3. è¾“å‡ºæ ¼å¼é—®é¢˜: ç¡®ä¿è¾“å‡ºä¸º3ç»´ [dx, dy, action_type]")
        print(f"   4. ONNXç‰ˆæœ¬é—®é¢˜: ä½¿ç”¨opset_version=11å¯¼å‡º")

if __name__ == "__main__":
    main()
