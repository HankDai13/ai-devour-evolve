#!/usr/bin/env python3
"""
è¯„ä¼°å’Œæµ‹è¯•è®­ç»ƒå¥½çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹
å¯è§†åŒ–æ™ºèƒ½ä½“çš„è¡Œä¸ºå’Œæ€§èƒ½
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import matplotlib.pyplot as plt

# è·¯å¾„è®¾ç½®
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

from gobigger_gym_env import GoBiggerEnv

try:
    from stable_baselines3 import PPO, DQN, A2C
    STABLE_BASELINES_AVAILABLE = True
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")

def evaluate_model(model_path, episodes=10, render=True, save_replay=False):
    """è¯¦ç»†è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3")
        return
    
    # åŠ è½½æ¨¡å‹
    try:
        if 'PPO' in model_path:
            model = PPO.load(model_path)
            print("âœ… åŠ è½½PPOæ¨¡å‹æˆåŠŸ")
        elif 'DQN' in model_path:
            model = DQN.load(model_path)
            print("âœ… åŠ è½½DQNæ¨¡å‹æˆåŠŸ")
        elif 'A2C' in model_path:
            model = A2C.load(model_path)
            print("âœ… åŠ è½½A2Cæ¨¡å‹æˆåŠŸ")
        else:
            print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹ï¼Œå°è¯•ç”¨PPOåŠ è½½...")
            model = PPO.load(model_path)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = GoBiggerEnv({'max_episode_steps': 2000})
    
    episode_stats = []
    action_stats = {'directions': [], 'action_types': []}
    
    print(f"\nğŸ® å¼€å§‹è¯„ä¼° {episodes} ä¸ªå›åˆ...")
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        score_history = []
        actions_taken = []
        
        print(f"\nğŸ“ å›åˆ {episode + 1}:")
        
        while steps < 2000:
            # é¢„æµ‹åŠ¨ä½œ
            action, _states = model.predict(obs, deterministic=True)
            actions_taken.append(action.copy())
            
            # è®°å½•åŠ¨ä½œç»Ÿè®¡
            action_stats['directions'].append([action[0], action[1]])
            action_stats['action_types'].append(action[2])
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            # è®°å½•åˆ†æ•°å˜åŒ–
            if hasattr(env, 'current_obs') and env.current_obs:
                if len(env.current_obs.player_states) > 0:
                    ps = list(env.current_obs.player_states.values())[0]
                    score_history.append(ps.score)
            
            # å¯é€‰æ¸²æŸ“
            if render and steps % 100 == 0:
                print(f"  æ­¥éª¤ {steps}: å¥–åŠ±={reward:.4f}, åŠ¨ä½œ=[{action[0]:.2f}, {action[1]:.2f}, {int(action[2])}]")
            
            if terminated or truncated:
                break
        
        # ç»Ÿè®¡æœ¬å›åˆç»“æœ
        final_score = score_history[-1] if score_history else 0
        episode_stats.append({
            'episode': episode + 1,
            'total_reward': total_reward,
            'steps': steps,
            'final_score': final_score,
            'score_history': score_history,
            'actions': actions_taken
        })
        
        print(f"  âœ… å®Œæˆ: æ€»å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}, æœ€ç»ˆåˆ†æ•°={final_score:.1f}")
    
    # åˆ†æç»“æœ
    analyze_results(episode_stats, action_stats)
    
    # å¯é€‰ä¿å­˜å›æ”¾æ•°æ®
    if save_replay:
        save_replay_data(episode_stats, model_path)
    
    return episode_stats

def analyze_results(episode_stats, action_stats):
    """åˆ†æè¯„ä¼°ç»“æœ"""
    print("\nğŸ“Š è¯„ä¼°ç»“æœåˆ†æ:")
    print("=" * 50)
    
    # åŸºç¡€ç»Ÿè®¡
    rewards = [ep['total_reward'] for ep in episode_stats]
    steps = [ep['steps'] for ep in episode_stats]
    scores = [ep['final_score'] for ep in episode_stats]
    
    print(f"ğŸ¯ å›åˆç»Ÿè®¡:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.3f} Â± {np.std(rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(steps):.1f} Â± {np.std(steps):.1f}")
    print(f"  å¹³å‡æœ€ç»ˆåˆ†æ•°: {np.mean(scores):.1f} Â± {np.std(scores):.1f}")
    print(f"  æœ€ä½³å¥–åŠ±: {max(rewards):.3f}")
    print(f"  æœ€é«˜åˆ†æ•°: {max(scores):.1f}")
    
    # åŠ¨ä½œåˆ†æ
    directions = np.array(action_stats['directions'])
    action_types = action_stats['action_types']
    
    print(f"\nğŸ® åŠ¨ä½œåˆ†æ:")
    print(f"  å¹³å‡ç§»åŠ¨æ–¹å‘: ({np.mean(directions[:, 0]):.3f}, {np.mean(directions[:, 1]):.3f})")
    print(f"  ç§»åŠ¨å¹…åº¦æ ‡å‡†å·®: ({np.std(directions[:, 0]):.3f}, {np.std(directions[:, 1]):.3f})")
    
    # åŠ¨ä½œç±»å‹åˆ†å¸ƒ
    action_counts = {0: 0, 1: 0, 2: 0}
    for act_type in action_types:
        action_counts[int(act_type)] += 1
    
    total_actions = len(action_types)
    print(f"  åŠ¨ä½œç±»å‹åˆ†å¸ƒ:")
    print(f"    æ— åŠ¨ä½œ (0): {action_counts[0]/total_actions*100:.1f}%")
    print(f"    åçƒ (1): {action_counts[1]/total_actions*100:.1f}%") 
    print(f"    åˆ†è£‚ (2): {action_counts[2]/total_actions*100:.1f}%")
    
    # ç»˜åˆ¶ç»“æœå›¾è¡¨
    plot_evaluation_results(episode_stats, action_stats)

def plot_evaluation_results(episode_stats, action_stats):
    """ç»˜åˆ¶è¯„ä¼°ç»“æœå›¾è¡¨"""
    print("\nğŸ“ˆ ç”Ÿæˆç»“æœå›¾è¡¨...")
    
    plt.figure(figsize=(15, 10))
    
    # 1. å›åˆå¥–åŠ±è¶‹åŠ¿
    plt.subplot(2, 3, 1)
    rewards = [ep['total_reward'] for ep in episode_stats]
    plt.plot(rewards, 'bo-')
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.grid(True)
    
    # 2. å›åˆæ­¥æ•°
    plt.subplot(2, 3, 2)
    steps = [ep['steps'] for ep in episode_stats]
    plt.plot(steps, 'go-')
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    plt.grid(True)
    
    # 3. åˆ†æ•°å˜åŒ–ï¼ˆç¬¬ä¸€ä¸ªå›åˆï¼‰
    plt.subplot(2, 3, 3)
    if episode_stats and episode_stats[0]['score_history']:
        score_history = episode_stats[0]['score_history']
        plt.plot(score_history)
        plt.title('Score Evolution (Episode 1)')
        plt.xlabel('Steps')
        plt.ylabel('Score')
        plt.grid(True)
    
    # 4. åŠ¨ä½œæ–¹å‘åˆ†å¸ƒ
    plt.subplot(2, 3, 4)
    directions = np.array(action_stats['directions'])
    plt.scatter(directions[:, 0], directions[:, 1], alpha=0.6)
    plt.title('Action Direction Distribution')
    plt.xlabel('Direction X')
    plt.ylabel('Direction Y')
    plt.grid(True)
    plt.axis('equal')
    
    # 5. åŠ¨ä½œç±»å‹åˆ†å¸ƒ
    plt.subplot(2, 3, 5)
    action_types = action_stats['action_types']
    action_counts = [0, 0, 0]
    for act_type in action_types:
        action_counts[int(act_type)] += 1
    
    plt.bar(['No Action', 'Eject', 'Split'], action_counts)
    plt.title('Action Type Distribution')
    plt.ylabel('Count')
    
    # 6. å¥–åŠ±ç›´æ–¹å›¾
    plt.subplot(2, 3, 6)
    plt.hist(rewards, bins=10, alpha=0.7)
    plt.title('Reward Distribution')
    plt.xlabel('Total Reward')
    plt.ylabel('Frequency')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('model_evaluation_results.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š ç»“æœå›¾è¡¨å·²ä¿å­˜åˆ° model_evaluation_results.png")

def save_replay_data(episode_stats, model_path):
    """ä¿å­˜å›æ”¾æ•°æ®"""
    import json
    
    replay_data = {
        'model_path': model_path,
        'timestamp': time.strftime('%Y-%m-%d_%H-%M-%S'),
        'episodes': episode_stats
    }
    
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    for ep in replay_data['episodes']:
        if 'actions' in ep:
            ep['actions'] = [action.tolist() if hasattr(action, 'tolist') else action 
                           for action in ep['actions']]
    
    replay_file = f"replay_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(replay_file, 'w') as f:
        json.dump(replay_data, f, indent=2)
    
    print(f"ğŸ’¾ å›æ”¾æ•°æ®å·²ä¿å­˜åˆ° {replay_file}")

def compare_models(model_paths, episodes=5):
    """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("ğŸ” æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 50)
    
    results = {}
    
    for model_path in model_paths:
        print(f"\nè¯„ä¼°æ¨¡å‹: {model_path}")
        stats = evaluate_model(model_path, episodes=episodes, render=False)
        
        if stats:
            rewards = [ep['total_reward'] for ep in stats]
            scores = [ep['final_score'] for ep in stats]
            
            results[model_path] = {
                'mean_reward': np.mean(rewards),
                'std_reward': np.std(rewards),
                'mean_score': np.mean(scores),
                'std_score': np.std(scores)
            }
    
    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    print("\nğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:")
    print("-" * 80)
    print(f"{'æ¨¡å‹è·¯å¾„':<30} {'å¹³å‡å¥–åŠ±':<12} {'å¹³å‡åˆ†æ•°':<12} {'å¥–åŠ±ç¨³å®šæ€§':<12}")
    print("-" * 80)
    
    for model_path, result in results.items():
        model_name = Path(model_path).name
        print(f"{model_name:<30} {result['mean_reward']:<12.3f} {result['mean_score']:<12.1f} {result['std_reward']:<12.3f}")

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    print("ğŸ§ª GoBigger æ¨¡å‹è¯„ä¼°å™¨")
    print("=" * 50)
    
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£… stable-baselines3 æ¥è¯„ä¼°æ¨¡å‹")
        return
    
    print("ğŸ¯ é€‰æ‹©è¯„ä¼°æ¨¡å¼:")
    print("1. è¯„ä¼°å•ä¸ªæ¨¡å‹")
    print("2. æ¯”è¾ƒå¤šä¸ªæ¨¡å‹")
    print("3. è¯„ä¼°æœ€æ–°çš„PPOæ¨¡å‹")
    
    choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
    
    if choice == '1':
        model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
        if os.path.exists(model_path):
            episodes = int(input("è¯„ä¼°å›åˆæ•° (é»˜è®¤10): ") or 10)
            evaluate_model(model_path, episodes=episodes, render=True, save_replay=True)
        else:
            print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            
    elif choice == '2':
        print("è¯·è¾“å…¥å¤šä¸ªæ¨¡å‹è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸï¼‰:")
        model_paths = []
        while True:
            path = input().strip()
            if not path:
                break
            if os.path.exists(path):
                model_paths.append(path)
            else:
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        if model_paths:
            episodes = int(input("æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°å›åˆæ•° (é»˜è®¤5): ") or 5)
            compare_models(model_paths, episodes=episodes)
        else:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„")
            
    elif choice == '3':
        # æŸ¥æ‰¾æœ€æ–°çš„PPOæ¨¡å‹
        model_files = []
        
        # æ£€æŸ¥modelsç›®å½•
        if os.path.exists("models"):
            for file in os.listdir("models"):
                if file.endswith(".zip") and "PPO" in file:
                    model_files.append(os.path.join("models", file))
        
        # æ£€æŸ¥checkpointsç›®å½•
        if os.path.exists("checkpoints"):
            for file in os.listdir("checkpoints"):
                if file.endswith(".zip"):
                    model_files.append(os.path.join("checkpoints", file))
        
        if model_files:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
            latest_model = max(model_files, key=os.path.getmtime)
            print(f"ğŸ¯ æ‰¾åˆ°æœ€æ–°æ¨¡å‹: {latest_model}")
            
            episodes = int(input("è¯„ä¼°å›åˆæ•° (é»˜è®¤10): ") or 10)
            evaluate_model(latest_model, episodes=episodes, render=True, save_replay=True)
        else:
            print("âŒ æœªæ‰¾åˆ°PPOæ¨¡å‹æ–‡ä»¶")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()
