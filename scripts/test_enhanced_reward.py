#!/usr/bin/env python3
"""
å¢å¼ºå¥–åŠ±ç³»ç»Ÿæµ‹è¯•è„šæœ¬
===================

æµ‹è¯•å¢å¼ºå¥–åŠ±ç³»ç»Ÿçš„å„ä¸ªç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Œ
å¹¶æä¾›è¯¦ç»†çš„å¥–åŠ±åˆ†è§£åˆ†æã€‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024å¹´
"""

import sys
import os
import numpy as np

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from gobigger_gym_env import GoBiggerEnv
from enhanced_reward_system import EnhancedRewardCalculator

def test_enhanced_reward_system():
    """æµ‹è¯•å¢å¼ºå¥–åŠ±ç³»ç»Ÿ"""
    print("ğŸ§ª å¢å¼ºå¥–åŠ±ç³»ç»Ÿæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºå¯ç”¨å¢å¼ºå¥–åŠ±çš„ç¯å¢ƒ
    config = {
        'max_episode_steps': 100,
        'use_enhanced_reward': True,
        'enhanced_reward_weights': {
            'score_growth': 2.0,
            'efficiency': 1.5,
            'exploration': 0.8,
            'strategic_split': 2.0,
            'food_density': 1.0,
            'survival': 0.02,
            'time_penalty': -0.001,
            'death_penalty': -20.0,
        }
    }
    
    print("âœ… åˆ›å»ºç¯å¢ƒï¼ˆå¢å¼ºå¥–åŠ±æ¨¡å¼ï¼‰")
    env = GoBiggerEnv(config)
    
    print("âœ… é‡ç½®ç¯å¢ƒ")
    obs = env.reset()
    
    # è®°å½•å¥–åŠ±å†å²
    reward_history = []
    component_history = []
    score_history = []
    
    print("âœ… å¼€å§‹æµ‹è¯•æ­¥éª¤...")
    
    for step in range(20):  # æµ‹è¯•20æ­¥
        # ç”ŸæˆéšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        
        # è®°å½•æ­¥éª¤å‰çš„åˆ†æ•°
        if env.current_obs and env.current_obs.player_states:
            score_before = list(env.current_obs.player_states.values())[0].score
        else:
            score_before = 0.0
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        
        # è®°å½•æ­¥éª¤åçš„åˆ†æ•°
        if env.current_obs and env.current_obs.player_states:
            score_after = list(env.current_obs.player_states.values())[0].score
        else:
            score_after = 0.0
        
        score_delta = score_after - score_before
        
        # è®°å½•æ•°æ®
        reward_history.append(reward)
        score_history.append(score_after)
        
        # è·å–å¥–åŠ±ç»„ä»¶åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(env, 'reward_components_history') and env.reward_components_history:
            latest_components = env.reward_components_history[-1]['components']
            component_history.append(latest_components)
        else:
            component_history.append({})
        
        # æ˜¾ç¤ºæ­¥éª¤ä¿¡æ¯
        print(f"æ­¥éª¤ {step + 1:2d}: "
              f"åŠ¨ä½œ={[f'{a:.2f}' for a in action]}, "
              f"åˆ†æ•°={score_after:.2f} (Î”{score_delta:+.2f}), "
              f"å¥–åŠ±={reward:.4f}")
        
        # å¦‚æœæœ‰å¥–åŠ±ç»„ä»¶ï¼Œæ˜¾ç¤ºåˆ†è§£
        if component_history[-1]:
            print("         å¥–åŠ±ç»„ä»¶:")
            for component, value in component_history[-1].items():
                weighted_value = config['enhanced_reward_weights'].get(component, 1.0) * value
                print(f"           {component}: {value:.4f} (æƒé‡å: {weighted_value:.4f})")
        
        # å¦‚æœepisodeç»“æŸï¼Œé€€å‡º
        if terminated or truncated:
            print(f"ğŸ Episodeç»“æŸäºæ­¥éª¤ {step + 1}")
            break
    
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“")
    print("-" * 50)
    
    if reward_history:
        total_reward = sum(reward_history)
        avg_reward = np.mean(reward_history)
        max_reward = max(reward_history)
        min_reward = min(reward_history)
        
        print(f"æ€»æ­¥æ•°: {len(reward_history)}")
        print(f"æ€»å¥–åŠ±: {total_reward:.4f}")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        print(f"æœ€å¤§å¥–åŠ±: {max_reward:.4f}")
        print(f"æœ€å°å¥–åŠ±: {min_reward:.4f}")
        
        if score_history:
            initial_score = score_history[0] if len(score_history) > 1 else 0
            final_score = score_history[-1]
            score_growth = final_score - initial_score
            print(f"åˆ†æ•°å¢é•¿: {score_growth:+.2f} ({initial_score:.2f} â†’ {final_score:.2f})")
        
        # å¥–åŠ±ç»„ä»¶ç»Ÿè®¡
        if component_history and any(component_history):
            print("\nå¥–åŠ±ç»„ä»¶å¹³å‡å€¼:")
            all_components = set()
            for components in component_history:
                all_components.update(components.keys())
            
            for component in sorted(all_components):
                values = [comp.get(component, 0.0) for comp in component_history if component in comp]
                if values:
                    avg_value = np.mean(values)
                    weighted_avg = avg_value * config['enhanced_reward_weights'].get(component, 1.0)
                    print(f"  {component}: {avg_value:.4f} (æƒé‡å: {weighted_avg:.4f})")
    
    env.close()
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

def test_reward_calculator_directly():
    """ç›´æ¥æµ‹è¯•å¥–åŠ±è®¡ç®—å™¨"""
    print("\nğŸ”¬ ç›´æ¥æµ‹è¯•å¢å¼ºå¥–åŠ±è®¡ç®—å™¨")
    print("=" * 50)
    
    calculator = EnhancedRewardCalculator()
    
    # æ¨¡æ‹Ÿä¸€äº›æ¸¸æˆçŠ¶æ€
    print("âœ… åˆ›å»ºå¥–åŠ±è®¡ç®—å™¨")
    print("âœ… æ¨¡æ‹Ÿæ¸¸æˆçŠ¶æ€...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å•å…ƒæµ‹è¯•
    # ç”±äºéœ€è¦çœŸå®çš„æ¸¸æˆçŠ¶æ€å¯¹è±¡ï¼Œæš‚æ—¶è·³è¿‡
    print("âš ï¸  éœ€è¦çœŸå®æ¸¸æˆçŠ¶æ€å¯¹è±¡ï¼Œè·³è¿‡ç›´æ¥æµ‹è¯•")
    
    # æ˜¾ç¤ºå¥–åŠ±è®¡ç®—å™¨é…ç½®
    print("\nå¥–åŠ±æƒé‡é…ç½®:")
    for component, weight in calculator.weights.items():
        print(f"  {component}: {weight}")

def compare_simple_vs_enhanced():
    """æ¯”è¾ƒç®€å•å¥–åŠ±å’Œå¢å¼ºå¥–åŠ±"""
    print("\nâš–ï¸  ç®€å•å¥–åŠ± vs å¢å¼ºå¥–åŠ±å¯¹æ¯”")
    print("=" * 50)
    
    results = {}
    
    for system_name, use_enhanced in [("ç®€å•å¥–åŠ±", False), ("å¢å¼ºå¥–åŠ±", True)]:
        print(f"\nğŸ§ª æµ‹è¯•{system_name}ç³»ç»Ÿ...")
        
        config = {
            'max_episode_steps': 50,
            'use_enhanced_reward': use_enhanced
        }
        
        env = GoBiggerEnv(config)
        
        # è¿è¡Œ5ä¸ªepisode
        all_rewards = []
        all_scores = []
        
        for episode in range(3):
            obs = env.reset()
            episode_reward = 0.0
            
            if env.current_obs and env.current_obs.player_states:
                initial_score = list(env.current_obs.player_states.values())[0].score
            else:
                initial_score = 0.0
            
            for step in range(20):  # é™åˆ¶æ­¥æ•°ä»¥åŠ å¿«æµ‹è¯•
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward
                
                if terminated or truncated:
                    break
            
            final_score = info.get('final_score', initial_score)
            all_rewards.append(episode_reward)
            all_scores.append(final_score)
            
            print(f"  Episode {episode + 1}: å¥–åŠ±={episode_reward:.4f}, åˆ†æ•°={final_score:.2f}")
        
        env.close()
        
        results[system_name] = {
            'avg_reward': np.mean(all_rewards),
            'avg_score': np.mean(all_scores),
            'reward_std': np.std(all_rewards),
            'score_std': np.std(all_scores)
        }
    
    print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"{'æŒ‡æ ‡':<15} {'ç®€å•å¥–åŠ±':<15} {'å¢å¼ºå¥–åŠ±':<15} {'æ”¹è¿›':<10}")
    print("-" * 60)
    
    simple = results["ç®€å•å¥–åŠ±"]
    enhanced = results["å¢å¼ºå¥–åŠ±"]
    
    metrics = [
        ('å¹³å‡å¥–åŠ±', 'avg_reward'),
        ('å¹³å‡åˆ†æ•°', 'avg_score'),
        ('å¥–åŠ±æ ‡å‡†å·®', 'reward_std'),
        ('åˆ†æ•°æ ‡å‡†å·®', 'score_std')
    ]
    
    for name, key in metrics:
        simple_val = simple[key]
        enhanced_val = enhanced[key]
        
        if simple_val != 0:
            if key in ['avg_reward', 'avg_score']:
                improvement = f"{((enhanced_val - simple_val) / abs(simple_val) * 100):+.1f}%"
            else:
                improvement = f"{((simple_val - enhanced_val) / abs(simple_val) * 100):+.1f}%"
        else:
            improvement = "N/A"
        
        print(f"{name:<15} {simple_val:<15.4f} {enhanced_val:<15.4f} {improvement:<10}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¢å¼ºå¥–åŠ±ç³»ç»Ÿå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1: å¢å¼ºå¥–åŠ±ç³»ç»ŸåŸºæœ¬åŠŸèƒ½
        test_enhanced_reward_system()
        
        # æµ‹è¯•2: ç›´æ¥æµ‹è¯•å¥–åŠ±è®¡ç®—å™¨
        test_reward_calculator_directly()
        
        # æµ‹è¯•3: ç³»ç»Ÿå¯¹æ¯”
        compare_simple_vs_enhanced()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
