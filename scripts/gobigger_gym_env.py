#!/usr/bin/env python3
"""
GoBigger Gymnasium Environment Wrapper
å°†C++æ ¸å¿ƒå¼•æ“åŒ…è£…æˆæ ‡å‡†çš„Gymnasiumç¯å¢ƒ
"""
import sys
import os
import time
from pathlib import Path
import numpy as np

def setup_qt_environment():
    """Setup Qt environment variables for proper plugin loading"""
    qt_dir = r"D:\qt\6.9.1\msvc2022_64"
    
    # Set Qt environment variables
    os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = os.path.join(qt_dir, "plugins", "platforms")
    os.environ['QT_PLUGIN_PATH'] = os.path.join(qt_dir, "plugins")
    
    # Add Qt bin to PATH
    qt_bin = os.path.join(qt_dir, "bin")
    if qt_bin not in os.environ.get('PATH', ''):
        os.environ['PATH'] = qt_bin + os.pathsep + os.environ.get('PATH', '')

# Setup Qt environment before any imports that might use Qt
setup_qt_environment()

# æ™ºèƒ½è·¯å¾„æœç´¢ï¼šå¯»æ‰¾æœ‰æ•ˆçš„æ„å»ºç›®å½•
root_dir = Path(__file__).parent.parent
build_dir_candidates = [
    root_dir / "build" / "Release",
    root_dir / "build" / "Debug", 
    root_dir / "build",
    root_dir / "build-msvc" / "Release",
    root_dir / "build-msvc" / "Debug",
]

build_dir_found = None
for build_dir in build_dir_candidates:
    if build_dir.exists() and any(build_dir.iterdir()):
        sys.path.insert(0, str(build_dir))
        os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"
        build_dir_found = build_dir
        print(f"âœ… æ‰¾åˆ°æ„å»ºç›®å½•: {build_dir}")
        break

# æ·»åŠ pythonç›®å½•åˆ°æœç´¢è·¯å¾„ï¼ˆç”¨äºå¤šæ™ºèƒ½ä½“æ¨¡å—ï¼‰
python_dir = root_dir / "python"
if python_dir.exists():
    sys.path.insert(0, str(python_dir))
    print(f"âœ… æ·»åŠ Pythonæ¨¡å—ç›®å½•: {python_dir}")

if not build_dir_found:
    raise ImportError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ„å»ºç›®å½•ã€‚è¯·ç¡®ä¿é¡¹ç›®å·²ç¼–è¯‘ã€‚")

try:
    import gobigger_env
    print("âœ… æˆåŠŸå¯¼å…¥ gobigger_env æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ gobigger_env å¤±è´¥: {e}")
    print(f"æœç´¢è·¯å¾„: {build_dir_found}")
    raise

# å°è¯•å¯¼å…¥å¤šæ™ºèƒ½ä½“æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    import gobigger_multi_env
    MULTI_AGENT_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥ gobigger_multi_env æ¨¡å—ï¼ˆå¤šæ™ºèƒ½ä½“æ”¯æŒï¼‰")
except ImportError:
    MULTI_AGENT_AVAILABLE = False
    print("âš ï¸  gobigger_multi_env ä¸å¯ç”¨ï¼Œä»…æ”¯æŒå•æ™ºèƒ½ä½“æ¨¡å¼")

try:
    import gymnasium as gym
    from gymnasium import spaces
    GYMNASIUM_AVAILABLE = True
    print("âœ… ä½¿ç”¨ gymnasium åº“")
except ImportError:
    try:
        import gym
        from gym import spaces
        GYMNASIUM_AVAILABLE = False
        print("âœ… ä½¿ç”¨ gym åº“")
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… gymnasium æˆ– gym")
        raise

class GoBiggerEnv(gym.Env if GYMNASIUM_AVAILABLE else gym.Env):
    """
    GoBiggerç¯å¢ƒçš„Gymnasiumé£æ ¼åŒ…è£…å™¨
    æä¾›æ ‡å‡†çš„reset()ã€step()ã€render()æ¥å£
    """
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–ç¯å¢ƒ"""
        super().__init__()
        
        self.engine = gobigger_env.GameEngine()
        self.config = config or {}
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´å’Œè§‚å¯Ÿç©ºé—´
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0, 0], dtype=np.float32),
            high=np.array([1.0, 1.0, 2], dtype=np.float32),
            dtype=np.float32
        )
        
        # è§‚å¯Ÿç©ºé—´å¤§å°éœ€è¦ä¸ç‰¹å¾æå–ä¿æŒä¸€è‡´
        self.observation_space_shape = (400,)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=self.observation_space_shape,
            dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´å®šä¹‰ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
        self.action_space_low = np.array([-1.0, -1.0, 0], dtype=np.float32)
        self.action_space_high = np.array([1.0, 1.0, 2], dtype=np.float32)
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_obs = None
        self.episode_step = 0
        self.max_episode_steps = self.config.get('max_episode_steps', 2000)  # å¢åŠ é»˜è®¤æ­¥æ•°
        
        # å¥–åŠ±å‡½æ•°ä¼˜åŒ–ï¼šè¿½è¸ªåˆ†æ•°å˜åŒ–
        self.last_score = 0.0
        self.initial_score = 0.0  # è®°å½•åˆå§‹åˆ†æ•°
        self.final_score = 0.0    # è®°å½•æœ€ç»ˆåˆ†æ•°
        self.prev_observation = None
        
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        if seed is not None:
            # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æœå¼•æ“æ”¯æŒï¼‰
            np.random.seed(seed)
        
        self.episode_step = 0
        self.episode_reward = 0.0  # é‡ç½®episodeç´¯ç§¯å¥–åŠ±
        self.current_obs = self.engine.reset()
        
        # åˆå§‹åŒ–å¥–åŠ±å‡½æ•°çŠ¶æ€
        if self.current_obs and self.current_obs.player_states:
            ps = list(self.current_obs.player_states.values())[0]
            self.last_score = ps.score
            self.initial_score = ps.score  # è®°å½•åˆå§‹åˆ†æ•°
        else:
            self.last_score = 0.0
            self.initial_score = 0.0

        self.final_score = 0.0  # é‡ç½®æœ€ç»ˆåˆ†æ•°
        self.prev_observation = self.current_obs
        
        # é‡ç½®å¢å¼ºå¥–åŠ±è®¡ç®—å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, 'enhanced_reward_calculator'):
            self.enhanced_reward_calculator.reset()
            self.reward_components_history = []
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_features(), {}
        else:
            return self._extract_features()
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        # ç¡®ä¿åŠ¨ä½œæ ¼å¼æ­£ç¡®å¹¶è¿›è¡Œè£å‰ª
        if isinstance(action, (list, tuple, np.ndarray)):
            if len(action) >= 3:
                # è£å‰ªåŠ¨ä½œåˆ°æœ‰æ•ˆèŒƒå›´
                move_x = float(np.clip(action[0], -1.0, 1.0))
                move_y = float(np.clip(action[1], -1.0, 1.0))
                action_type = int(np.round(np.clip(action[2], 0, 2)))
                cpp_action = gobigger_env.Action(move_x, move_y, action_type)
            else:
                cpp_action = gobigger_env.Action(0.0, 0.0, 0)
        else:
            cpp_action = action
        
        # æ‰§è¡ŒåŠ¨ä½œ
        self.prev_observation = self.current_obs
        self.current_obs = self.engine.step(cpp_action)
        self.episode_step += 1
        
        # è®¡ç®—å¥–åŠ±å’Œç»ˆæ­¢æ¡ä»¶ï¼ˆä¼ é€’åŸå§‹actionç”¨äºå¢å¼ºå¥–åŠ±è®¡ç®—ï¼‰
        reward = self._calculate_reward(action)
        terminated = self.engine.is_done()
        truncated = self.episode_step >= self.max_episode_steps
        
        # å‡†å¤‡infoå­—å…¸
        info = {}
        
        # ç´¯ç§¯episodeæ€»å¥–åŠ±ï¼ˆç”¨äºMonitorç»Ÿè®¡ï¼‰
        if not hasattr(self, 'episode_reward'):
            self.episode_reward = 0.0
        self.episode_reward += reward
        
        # å¦‚æœepisodeç»“æŸï¼Œè®°å½•æœ€ç»ˆåˆ†æ•°
        if terminated or truncated:
            if self.current_obs and self.current_obs.player_states:
                ps = list(self.current_obs.player_states.values())[0]
                self.final_score = ps.score
            else:
                self.final_score = self.last_score
            
            # MonitoræœŸæœ›çš„æ ‡å‡†é”®
            info['episode'] = {
                'r': self.episode_reward,                     # episodeæ€»å¥–åŠ±ï¼ˆç´¯ç§¯çš„stepå¥–åŠ±ï¼‰
                'l': self.episode_step,                       # episodeé•¿åº¦
                't': time.time()                              # æ—¶é—´æˆ³
            }
            
            # é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯
            info['final_score'] = self.final_score
            info['score_delta'] = self.final_score - self.initial_score
            info['episode_length'] = self.episode_step
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_features(), reward, terminated, truncated, info
        else:
            return self._extract_features(), reward, terminated or truncated, info
    
    def _extract_features(self):
        """ä»C++è§‚å¯Ÿä¸­æå–ç‰¹å¾å‘é‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if not self.current_obs or not self.current_obs.player_states:
            return np.zeros(self.observation_space_shape, dtype=np.float32)
        
        # è·å–ç¬¬ä¸€ä¸ªç©å®¶çš„çŠ¶æ€
        ps = list(self.current_obs.player_states.values())[0]
        
        features = []
        
        # å…¨å±€ç‰¹å¾ (10ç»´) - æ”¹è¿›å½’ä¸€åŒ–
        gs = self.current_obs.global_state
        features.extend([
            gs.total_frame / self.max_episode_steps,  # å½’ä¸€åŒ–å¸§æ•°
            len(gs.leaderboard) / 10.0,               # å½’ä¸€åŒ–é˜Ÿä¼æ•°é‡
            ps.score / 10000.0,                       # å½’ä¸€åŒ–åˆ†æ•°
            float(ps.can_eject),                      # èƒ½å¦åçƒ
            float(ps.can_split),                      # èƒ½å¦åˆ†è£‚
        ])
        features.extend([0.0] * 5)  # é¢„ç•™5ä¸ªå…¨å±€ç‰¹å¾
        
        # è§†é‡çŸ©å½¢ (4ç»´) - æ”¹è¿›å½’ä¸€åŒ–
        features.extend([coord / 2000.0 for coord in ps.rectangle])
        
        # æ‰å¹³åŒ–å¯¹è±¡ç‰¹å¾ - æ”¹è¿›å½’ä¸€åŒ–å’Œæ•°æ®å¤„ç†
        # é£Ÿç‰©: 50ä¸ª Ã— 4ç»´ = 200ç»´
        food_count = 0
        for food in ps.food:
            if food_count >= 50:
                break
            # å½’ä¸€åŒ–åæ ‡å’ŒåŠå¾„
            features.extend([
                food[0] / 2000.0,   # xåæ ‡å½’ä¸€åŒ–
                food[1] / 2000.0,   # yåæ ‡å½’ä¸€åŒ–  
                food[2] / 10.0,     # åŠå¾„å½’ä¸€åŒ–
                2.0                 # ç±»å‹æ ‡è¯†
            ])
            food_count += 1
        # å¡«å……å‰©ä½™çš„é£Ÿç‰©æ§½ä½
        features.extend([0.0] * ((50 - food_count) * 4))
        
        # è†æ£˜: 20ä¸ª Ã— 4ç»´ = 80ç»´
        thorns_count = 0
        for thorns in ps.thorns:
            if thorns_count >= 20:
                break
            features.extend([
                thorns[0] / 2000.0,  # xåæ ‡å½’ä¸€åŒ–
                thorns[1] / 2000.0,  # yåæ ‡å½’ä¸€åŒ–
                thorns[2] / 100.0,   # åŠå¾„å½’ä¸€åŒ–
                3.0                  # ç±»å‹æ ‡è¯†
            ])
            thorns_count += 1
        # å¡«å……å‰©ä½™çš„è†æ£˜æ§½ä½
        features.extend([0.0] * ((20 - thorns_count) * 4))
        
        # å­¢å­: 10ä¸ª Ã— 4ç»´ = 40ç»´
        spore_count = 0
        for spore in ps.spore:
            if spore_count >= 10:
                break
            features.extend([
                spore[0] / 2000.0,   # xåæ ‡å½’ä¸€åŒ–
                spore[1] / 2000.0,   # yåæ ‡å½’ä¸€åŒ–
                spore[2] / 20.0,     # åŠå¾„å½’ä¸€åŒ–
                4.0                  # ç±»å‹æ ‡è¯†
            ])
            spore_count += 1
        # å¡«å……å‰©ä½™çš„å­¢å­æ§½ä½
        features.extend([0.0] * ((10 - spore_count) * 4))
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        # æ€»è®¡: 10 + 4 + 200 + 80 + 40 = 334ç»´ï¼Œå¡«å……åˆ°400ç»´
        target_length = self.observation_space_shape[0]
        if len(features) > target_length:
            features = features[:target_length]
        elif len(features) < target_length:
            features.extend([0.0] * (target_length - len(features)))
        
        return np.array(features, dtype=np.float32)
    
    def _calculate_reward(self, action=None):
        """
        è®¡ç®—å¥–åŠ±ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šç®€å•æ¨¡å¼ï¼ˆåŸç‰ˆï¼‰å’Œå¢å¼ºæ¨¡å¼
        """
        # é€‰æ‹©å¥–åŠ±è®¡ç®—æ¨¡å¼
        use_enhanced_reward = self.config.get('use_enhanced_reward', False)
        
        if use_enhanced_reward and hasattr(self, 'enhanced_reward_calculator'):
            return self._calculate_enhanced_reward(action)
        else:
            return self._calculate_simple_reward(action)
    
    def _calculate_simple_reward(self, action=None):
        """
        è®¡ç®—ç®€å•å¥–åŠ±ï¼ˆåŸç‰ˆé€»è¾‘ + Geminiä¼˜åŒ–çš„äº‹ä»¶é©±åŠ¨å¥–åŠ±ï¼‰
        æ ¸å¿ƒæ€æƒ³ï¼šå¥–åŠ±åˆ†æ•°å¢é‡è€Œéç»å¯¹åˆ†æ•°ï¼Œé¿å…å¥–åŠ±ç¨€ç–æ€§é—®é¢˜
        ğŸ”¥ æ–°å¢ï¼šå¤§å¹…å¥–åŠ±Splitå’ŒEjecté«˜çº§åŠ¨ä½œï¼Œè§£å†³ç­–ç•¥å´©å¡Œé—®é¢˜
        """
        if not self.current_obs or not self.current_obs.player_states:
            return -5.0  # æ— æ•ˆçŠ¶æ€æƒ©ç½š

        ps = list(self.current_obs.player_states.values())[0]
        
        # 1. åˆ†æ•°å¢é‡å¥–åŠ±ï¼ˆä¸»è¦å¥–åŠ±æ¥æºï¼‰
        score_delta = ps.score - self.last_score
        score_reward = score_delta / 100.0  # ç¼©æ”¾ç³»æ•°ï¼Œé¿å…å¥–åŠ±è¿‡å¤§
        
        # 2. æ—¶é—´æƒ©ç½šï¼ˆé¼“åŠ±å¿«é€Ÿå†³ç­–ï¼‰
        time_penalty = -0.001
        
        # 3. æ­»äº¡æƒ©ç½šï¼ˆå…³é”®æƒ©ç½šï¼‰
        death_penalty = 0.0
        if self.engine.is_done():
            death_penalty = -10.0
        
        # 4. ç”Ÿå­˜å¥–åŠ±ï¼ˆå°å¹…æ­£å¥–åŠ±ï¼‰
        survival_reward = 0.01 if not self.engine.is_done() else 0.0
        
        # ğŸ”¥ 5. äº‹ä»¶é©±åŠ¨çš„é«˜çº§åŠ¨ä½œå¥–åŠ±ï¼ˆGeminiæ¨èï¼‰
        advanced_action_reward = 0.0
        if action is not None and len(action) >= 3:
            action_type = int(np.round(np.clip(action[2], 0, 2)))
            
            # SplitåŠ¨ä½œå¥–åŠ±ï¼ˆåŠ¨ä½œç±»å‹1ï¼‰
            if action_type == 1:
                base_split_reward = 2.0  # åŸºç¡€Splitå¥–åŠ±
                # å¦‚æœSplitååˆ†æ•°ç¡®å®å¢åŠ ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
                if score_delta > 0:
                    advanced_action_reward += base_split_reward + score_delta / 50.0
                    if hasattr(self, 'debug_rewards'):
                        print(f"ğŸ¯ SplitæˆåŠŸ! å¥–åŠ±: {base_split_reward + score_delta / 50.0:.3f}")
                else:
                    # å³ä½¿Splitæ²¡æœ‰ç«‹å³è·å¾—åˆ†æ•°ï¼Œä¹Ÿç»™äºˆå°å¥–åŠ±é¼“åŠ±æ¢ç´¢
                    advanced_action_reward += base_split_reward * 0.3
                    if hasattr(self, 'debug_rewards'):
                        print(f"ğŸ”„ Splitå°è¯•! å¥–åŠ±: {base_split_reward * 0.3:.3f}")
            
            # EjectåŠ¨ä½œå¥–åŠ±ï¼ˆåŠ¨ä½œç±»å‹2ï¼‰
            elif action_type == 2:
                base_eject_reward = 1.5  # åŸºç¡€Ejectå¥–åŠ±
                # å¦‚æœEjectååˆ†æ•°å¢åŠ ï¼ˆå¯èƒ½å–‚é£Ÿé˜Ÿå‹æˆ–æˆ˜ç•¥ä½¿ç”¨ï¼‰ï¼Œç»™äºˆå¥–åŠ±
                if score_delta >= 0:  # ä¸æƒ©ç½šEjectï¼Œé¼“åŠ±æˆ˜ç•¥ä½¿ç”¨
                    advanced_action_reward += base_eject_reward
                    if hasattr(self, 'debug_rewards'):
                        print(f"ğŸ’¨ Ejectä½¿ç”¨! å¥–åŠ±: {base_eject_reward:.3f}")
                else:
                    # å³ä½¿æŸå¤±åˆ†æ•°ï¼Œä¹Ÿç»™å°å¥–åŠ±é¿å…è¿‡åº¦æƒ©ç½š
                    advanced_action_reward += base_eject_reward * 0.2
        
        # 6. å°ºå¯¸å¥–åŠ±ï¼ˆåŸºäºç»†èƒæ•°é‡å˜åŒ–ï¼‰
        size_reward = 0.0
        if self.prev_observation and self.prev_observation.player_states:
            prev_ps = list(self.prev_observation.player_states.values())[0]
            # åŸºäºcloneåˆ—è¡¨é•¿åº¦çš„å¥–åŠ±ï¼ˆç»†èƒæ•°é‡å˜åŒ–ï¼‰
            if hasattr(ps, 'clone') and hasattr(prev_ps, 'clone'):
                current_cells = len(ps.clone) if isinstance(ps.clone, list) else 1
                prev_cells = len(prev_ps.clone) if isinstance(prev_ps.clone, list) else 1
                cell_delta = current_cells - prev_cells
                size_reward = cell_delta * 0.1
        
        # æ€»å¥–åŠ±è®¡ç®—
        total_reward = (score_reward + time_penalty + death_penalty + 
                       survival_reward + size_reward + advanced_action_reward)
        
        # æ›´æ–°ä¸Šä¸€å¸§åˆ†æ•°
        self.last_score = ps.score
        
        return total_reward
        
        return total_reward
    
    def _calculate_enhanced_reward(self, action):
        """
        è®¡ç®—å¢å¼ºå¥–åŠ±ï¼ˆä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰
        """
        if not hasattr(self, 'enhanced_reward_calculator'):
            # æ‡’åŠ è½½å¢å¼ºå¥–åŠ±è®¡ç®—å™¨
            try:
                from enhanced_reward_system import EnhancedRewardCalculator
                self.enhanced_reward_calculator = EnhancedRewardCalculator(self.config)
                self.reward_components_history = []
            except ImportError:
                print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥å¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼Œå›é€€åˆ°ç®€å•å¥–åŠ±")
                return self._calculate_simple_reward()
        
        # ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿè®¡ç®—å¥–åŠ±
        total_reward, reward_components = self.enhanced_reward_calculator.calculate_reward(
            self.current_obs, 
            self.prev_observation, 
            action or [0, 0, 0],  # é»˜è®¤åŠ¨ä½œ
            self.episode_step
        )
        
        # è®°å½•å¥–åŠ±ç»„ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.reward_components_history.append({
            'step': self.episode_step,
            'total_reward': total_reward,
            'components': reward_components
        })
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.reward_components_history) > 50:
            self.reward_components_history.pop(0)
        
        # æ›´æ–°ä¸Šä¸€å¸§åˆ†æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        if self.current_obs and self.current_obs.player_states:
            ps = list(self.current_obs.player_states.values())[0]
            self.last_score = ps.score
        
        return total_reward
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆå¯é€‰å®ç°ï¼‰"""
        if mode == 'human':
            print(f"Frame: {self.current_obs.global_state.total_frame if self.current_obs else 0}")
            if self.current_obs and self.current_obs.player_states:
                ps = list(self.current_obs.player_states.values())[0]
                score_delta = ps.score - self.last_score
                print(f"Score: {ps.score:.2f} (Î”{score_delta:+.2f}), "
                      f"Can eject: {ps.can_eject}, Can split: {ps.can_split}")
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ”š ç¯å¢ƒå…³é—­")
        pass

class MultiAgentGoBiggerEnv(gym.Env if GYMNASIUM_AVAILABLE else gym.Env):
    """
    å¤šæ™ºèƒ½ä½“GoBiggerç¯å¢ƒï¼ˆRL Agent vs ä¼ ç»ŸAI ROBOTï¼‰
    
    è¿™ä¸ªç¯å¢ƒæ•´åˆäº†ä½ åœ¨src_newä¸­å®ç°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ”¯æŒï¼š
    1. ä¸€ä¸ªRLæ™ºèƒ½ä½“é€šè¿‡Gymnasiumæ¥å£è®­ç»ƒ
    2. å¤šä¸ªä¼ ç»ŸAIç­–ç•¥æœºå™¨äººä½œä¸ºç¨³å®šå¯¹æ‰‹
    3. å›¢é˜Ÿæ’åæœºåˆ¶ä½œä¸ºå¥–åŠ±ä¿¡å·
    4. ä¸å•æ™ºèƒ½ä½“ç¯å¢ƒå…¼å®¹çš„æ¥å£
    """
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç¯å¢ƒ"""
        super().__init__()
        
        if not MULTI_AGENT_AVAILABLE:
            raise ImportError("å¤šæ™ºèƒ½ä½“åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿ gobigger_multi_env æ¨¡å—å·²ç¼–è¯‘")
        
        self.config = config or {}
        
        # åˆ›å»ºå¤šæ™ºèƒ½ä½“å¼•æ“é…ç½®
        engine_config = gobigger_multi_env.MultiAgentConfig()
        engine_config.maxFoodCount = self.config.get('max_food_count', 3000)
        engine_config.initFoodCount = self.config.get('init_food_count', 2500)
        engine_config.maxThornsCount = self.config.get('max_thorns_count', 12)
        engine_config.initThornsCount = self.config.get('init_thorns_count', 9)
        engine_config.maxFrames = self.config.get('max_frames', 3000)
        engine_config.aiOpponentCount = self.config.get('ai_opponent_count', 3)
        engine_config.gameUpdateInterval = self.config.get('update_interval', 16)
        
        # åˆ›å»ºC++å¤šæ™ºèƒ½ä½“å¼•æ“
        self.engine = gobigger_multi_env.MultiAgentGameEngine(engine_config)
        
        # å®šä¹‰åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´ï¼ˆä¸å•æ™ºèƒ½ä½“ä¿æŒå…¼å®¹ï¼‰
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0, 0], dtype=np.float32),
            high=np.array([1.0, 1.0, 2], dtype=np.float32),
            dtype=np.float32
        )
        
        # å¢å¼ºçš„è§‚å¯Ÿç©ºé—´ï¼ŒåŒ…å«å›¢é˜Ÿæ’åä¿¡æ¯
        self.observation_space_shape = (450,)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=self.observation_space_shape,
            dtype=np.float32
        )
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_obs = None
        self.episode_step = 0
        self.max_episode_steps = self.config.get('max_episode_steps', 3000)
        
        # å¥–åŠ±è¿½è¸ª
        self.last_score = 0.0
        self.initial_score = 0.0
        self.last_team_rank = 1
        self.episode_reward = 0.0
        
        # ä¼ ç»ŸAIç­–ç•¥ä¿¡æ¯
        self.ai_strategies = self.config.get('ai_strategies', [
            'FOOD_HUNTER', 'AGGRESSIVE', 'RANDOM'
        ])
        
        print(f"ğŸ¤– å¤šæ™ºèƒ½ä½“ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   RLæ™ºèƒ½ä½“ vs {engine_config.aiOpponentCount} ä¸ªä¼ ç»ŸAI")
        print(f"   AIç­–ç•¥: {self.ai_strategies}")
        
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        if seed is not None:
            np.random.seed(seed)
        
        # é‡ç½®å¤šæ™ºèƒ½ä½“å¼•æ“
        self.current_obs = self.engine.reset()
        self.episode_step = 0
        self.episode_reward = 0.0
        
        # åˆå§‹åŒ–å¥–åŠ±è¿½è¸ª
        reward_info = self.engine.get_reward_info()
        self.last_score = reward_info.get('score', 0.0)
        self.initial_score = self.last_score
        self.last_team_rank = reward_info.get('team_rank', 1)
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_multi_agent_features(), {}
        else:
            return self._extract_multi_agent_features()
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        # æ ¼å¼åŒ–RLæ™ºèƒ½ä½“åŠ¨ä½œ
        if isinstance(action, (list, tuple, np.ndarray)) and len(action) >= 3:
            move_x = float(np.clip(action[0], -1.0, 1.0))
            move_y = float(np.clip(action[1], -1.0, 1.0))
            action_type = int(np.round(np.clip(action[2], 0, 2)))
            rl_action = [move_x, move_y, action_type]
        else:
            rl_action = [0.0, 0.0, 0]
        
        # æ„é€ å¤šæ™ºèƒ½ä½“åŠ¨ä½œå­—å…¸
        actions_dict = {"rl_agent": rl_action}
        
        # æ‰§è¡ŒåŠ¨ä½œï¼ˆä¼ ç»ŸAIä¼šè‡ªåŠ¨æ‰§è¡Œï¼‰
        self.current_obs = self.engine.step(actions_dict)
        self.episode_step += 1
        
        # è®¡ç®—å¤šæ™ºèƒ½ä½“å¥–åŠ±
        reward = self._calculate_multi_agent_reward()
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        terminated = self.engine.is_done()
        truncated = self.episode_step >= self.max_episode_steps
        
        # ç´¯ç§¯å¥–åŠ±
        self.episode_reward += reward
        
        # å‡†å¤‡infoå­—å…¸
        info = {}
        if terminated or truncated:
            reward_info = self.engine.get_reward_info()
            final_score = reward_info.get('score', 0.0)
            final_rank = reward_info.get('team_rank', 1)
            total_teams = reward_info.get('total_teams', 1)
            
            # Monitoræ ‡å‡†æ ¼å¼
            info['episode'] = {
                'r': self.episode_reward,
                'l': self.episode_step,
                't': time.time()
            }
            
            # å¤šæ™ºèƒ½ä½“ç‰¹å®šä¿¡æ¯
            info['final_score'] = final_score
            info['score_delta'] = final_score - self.initial_score
            info['final_rank'] = final_rank
            info['total_teams'] = total_teams
            info['rank_progress'] = (total_teams - final_rank + 1) / total_teams
            
            # AIå¯¹æ‰‹çŠ¶æ€
            ai_states = self.current_obs.get('ai_states', {})
            info['ai_opponents_alive'] = len([s for s in ai_states.values() 
                                            if s.get('alive_balls_count', 0) > 0])
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_multi_agent_features(), reward, terminated, truncated, info
        else:
            return self._extract_multi_agent_features(), reward, terminated or truncated, info
    
    def _extract_multi_agent_features(self):
        """æå–å¤šæ™ºèƒ½ä½“ç‰¹å¾å‘é‡"""
        if not self.current_obs:
            return np.zeros(self.observation_space_shape, dtype=np.float32)
        
        features = []
        
        # 1. å…¨å±€çŠ¶æ€ç‰¹å¾ (10ç»´)
        global_state = self.current_obs.get('global_state', {})
        features.extend([
            global_state.get('frame', 0) / self.max_episode_steps,
            global_state.get('total_players', 0) / 10.0,
            global_state.get('food_count', 0) / 3000.0,
            global_state.get('thorns_count', 0) / 15.0,
        ])
        features.extend([0.0] * 6)  # é¢„ç•™ç‰¹å¾
        
        # 2. RLæ™ºèƒ½ä½“çŠ¶æ€ç‰¹å¾ (20ç»´)
        rl_obs = self.current_obs.get('rl_agent', {})
        if rl_obs:
            pos = rl_obs.get('position', (0, 0))
            vel = rl_obs.get('velocity', (0, 0))
            features.extend([
                pos[0] / 2000.0, pos[1] / 2000.0,
                rl_obs.get('radius', 0) / 100.0,
                rl_obs.get('score', 0) / 10000.0,
                vel[0] / 1000.0, vel[1] / 1000.0,
                float(rl_obs.get('can_split', False)),
                float(rl_obs.get('can_eject', False)),
            ])
            features.extend([0.0] * 12)  # é¢„ç•™
        else:
            features.extend([0.0] * 20)  # æ­»äº¡æ—¶å¡«0
        
        # 3. ğŸ”¥ å›¢é˜Ÿæ’åç‰¹å¾ (20ç»´) - æ ¸å¿ƒå¤šæ™ºèƒ½ä½“ç‰¹å¾
        team_ranking = global_state.get('team_ranking', [])
        ranking_features = [0.0] * 20
        
        for i, team_info in enumerate(team_ranking[:5]):
            base_idx = i * 4
            if base_idx + 3 < len(ranking_features):
                ranking_features[base_idx] = team_info.get('team_id', 0) / 10.0
                ranking_features[base_idx + 1] = team_info.get('score', 0) / 10000.0
                ranking_features[base_idx + 2] = team_info.get('rank', 1) / 5.0
                ranking_features[base_idx + 3] = 1.0 if team_info.get('team_id', -1) == 0 else 0.0
        
        features.extend(ranking_features)
        
        # 4. è§†é‡å†…é£Ÿç‰©ç‰¹å¾ (200ç»´: 50ä¸ªé£Ÿç‰© Ã— 4ç»´)
        nearby_food = rl_obs.get('nearby_food', [])
        food_features = []
        for i in range(50):
            if i < len(nearby_food):
                food = nearby_food[i]
                food_features.extend([
                    food[0] / 2000.0, food[1] / 2000.0,
                    food[2] / 10.0, food[3] / 100.0
                ])
            else:
                food_features.extend([0.0, 0.0, 0.0, 0.0])
        features.extend(food_features)
        
        # 5. è§†é‡å†…å…¶ä»–ç©å®¶ç‰¹å¾ (200ç»´: 20ä¸ªç©å®¶ Ã— 10ç»´)
        nearby_players = rl_obs.get('nearby_players', [])
        player_features = []
        for i in range(20):
            if i < len(nearby_players):
                player = nearby_players[i]
                player_features.extend([
                    player[0] / 2000.0, player[1] / 2000.0,
                    player[2] / 100.0, player[3] / 10000.0,
                    player[4] / 10.0, player[5] / 10.0
                ])
                player_features.extend([0.0] * 4)  # é¢„ç•™
            else:
                player_features.extend([0.0] * 10)
        features.extend(player_features)
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        target_length = self.observation_space_shape[0]
        if len(features) > target_length:
            features = features[:target_length]
        elif len(features) < target_length:
            features.extend([0.0] * (target_length - len(features)))
        
        return np.array(features, dtype=np.float32)
    
    def _calculate_multi_agent_reward(self):
        """
        è®¡ç®—å¤šæ™ºèƒ½ä½“å¥–åŠ±ï¼ˆåŒ…å«å›¢é˜Ÿæ’åå¥–åŠ±ï¼‰
        ğŸ”¥ è¿™æ˜¯å¤šæ™ºèƒ½ä½“ç¯å¢ƒçš„æ ¸å¿ƒåˆ›æ–°ï¼šå°†å›¢é˜Ÿæ’åä½œä¸ºå¥–åŠ±ä¿¡å·
        """
        reward_info = self.engine.get_reward_info()
        
        # 1. åŸºç¡€åˆ†æ•°å¥–åŠ±
        current_score = reward_info.get('score', 0.0)
        score_delta = current_score - self.last_score
        score_reward = score_delta / 100.0
        
        # 2. ğŸ”¥ å›¢é˜Ÿæ’åå¥–åŠ±ï¼ˆæ ¸å¿ƒå¤šæ™ºèƒ½ä½“ç‰¹å¾ï¼‰
        current_rank = reward_info.get('team_rank', 1)
        total_teams = reward_info.get('total_teams', 1)
        
        # å®‰å…¨æ£€æŸ¥ï¼šé¿å…é™¤é›¶é”™è¯¯
        if total_teams <= 0:
            total_teams = 1
            current_rank = 1
        
        # æ’åå¥–åŠ±ï¼šæ’åè¶Šé«˜å¥–åŠ±è¶Šå¤§
        rank_reward = (total_teams - current_rank + 1) / total_teams * 1.0
        
        # æ’åå˜åŒ–å¥–åŠ±ï¼šæ’åæå‡ç»™äºˆé¢å¤–å¥–åŠ±
        rank_change_reward = 0.0
        if current_rank < self.last_team_rank:
            rank_change_reward = 3.0  # æ’åæå‡å¥–åŠ±
        elif current_rank > self.last_team_rank:
            rank_change_reward = -2.0  # æ’åä¸‹é™æƒ©ç½š
        
        # 3. ç”Ÿå­˜å¥–åŠ±
        is_alive = reward_info.get('alive', False)
        survival_reward = 0.01 if is_alive else -10.0
        
        # 4. æ—¶é—´æƒ©ç½š
        time_penalty = -0.001
        
        # 5. ğŸ”¥ ä¼ ç»ŸAIäº¤äº’å¥–åŠ±ï¼ˆé¼“åŠ±ä¸AIå¯¹æ‰‹çš„æœ‰æ•ˆäº¤äº’ï¼‰
        ai_interaction_reward = 0.0
        ai_states = self.current_obs.get('ai_states', {})
        alive_ai_count = len([s for s in ai_states.values() 
                             if s.get('alive_balls_count', 0) > 0])
        
        # å¦‚æœå‡»è´¥äº†AIå¯¹æ‰‹ï¼Œç»™äºˆå¥–åŠ±
        if hasattr(self, '_last_alive_ai_count'):
            if alive_ai_count < self._last_alive_ai_count:
                ai_interaction_reward = 5.0  # å‡»è´¥AIå¯¹æ‰‹å¥–åŠ±
        self._last_alive_ai_count = alive_ai_count
        
        # æ€»å¥–åŠ±
        total_reward = (score_reward + rank_reward + rank_change_reward + 
                       survival_reward + time_penalty + ai_interaction_reward)
        
        # æ›´æ–°çŠ¶æ€
        self.last_score = current_score
        self.last_team_rank = current_rank
        
        return total_reward
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆå¢å¼ºç‰ˆæœ¬ï¼Œæ˜¾ç¤ºå¤šæ™ºèƒ½ä½“ä¿¡æ¯ï¼‰"""
        if mode == 'human' and self.current_obs:
            global_state = self.current_obs.get('global_state', {})
            rl_obs = self.current_obs.get('rl_agent', {})
            
            print(f"ğŸ® Frame: {global_state.get('frame', 0)}")
            
            # RLæ™ºèƒ½ä½“çŠ¶æ€
            if rl_obs:
                pos = rl_obs.get('position', (0, 0))
                print(f"ğŸ¯ RL Agent - Score: {rl_obs.get('score', 0):.1f}, "
                      f"Pos: ({pos[0]:.0f}, {pos[1]:.0f})")
            
            # å›¢é˜Ÿæ’å
            team_ranking = global_state.get('team_ranking', [])
            print("ğŸ† Team Ranking:")
            for i, team in enumerate(team_ranking):
                marker = "ğŸ‘‘ RL" if team.get('team_id') == 0 else "ğŸ¤– AI"
                print(f"  {i+1}. {marker} Team {team.get('team_id', 0)}: {team.get('score', 0):.1f}")
            
            # AIå¯¹æ‰‹çŠ¶æ€
            ai_states = self.current_obs.get('ai_states', {})
            alive_ai = len([s for s in ai_states.values() 
                           if s.get('alive_balls_count', 0) > 0])
            print(f"ğŸ¤– å­˜æ´»AIå¯¹æ‰‹: {alive_ai}/{len(ai_states)}")
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ”š å¤šæ™ºèƒ½ä½“ç¯å¢ƒå…³é—­")

def demo_gymnasium_usage():
    """æ¼”ç¤ºæ ‡å‡†Gymnasiumä½¿ç”¨æ–¹å¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    print("\n" + "="*50)
    print("ğŸ® GoBigger Gymnasium ç¯å¢ƒæ¼”ç¤º (ä¼˜åŒ–ç‰ˆ)")
    print("="*50 + "\n")
    
    # åˆ›å»ºç¯å¢ƒ
    env = GoBiggerEnv({'max_episode_steps': 100})
    
    # æ£€æŸ¥ç¯å¢ƒï¼ˆå¦‚æœå®‰è£…äº†stable-baselines3ï¼‰
    try:
        from stable_baselines3.common.env_checker import check_env
        print("ğŸ” ä½¿ç”¨ stable-baselines3 æ£€æŸ¥ç¯å¢ƒ...")
        check_env(env)
        print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\n")
    except ImportError:
        print("âš ï¸ æœªå®‰è£… stable-baselines3ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥\n")
    
    # é‡ç½®ç¯å¢ƒ
    if GYMNASIUM_AVAILABLE:
        obs, info = env.reset()
    else:
        obs = env.reset()
        info = {}
    
    print(f"ğŸ ç¯å¢ƒé‡ç½®å®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"   è§‚å¯Ÿç»´åº¦: {obs.shape}")
    print(f"   è§‚å¯Ÿå‰10ä¸ªå€¼: {obs[:10]}")
    print()
    
    # è¿è¡Œå‡ æ­¥
    total_reward = 0
    for step in range(10):
        # éšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        
        if GYMNASIUM_AVAILABLE:
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        else:
            obs, reward, done, info = env.step(action)
            terminated, truncated = done, False
        
        total_reward += reward
        
        print(f"æ­¥éª¤ {step+1:02d}: "
              f"åŠ¨ä½œ=[{action[0]:.2f}, {action[1]:.2f}, {int(np.round(action[2]))}], "
              f"å¥–åŠ±={reward:.4f}, "
              f"ç´¯è®¡å¥–åŠ±={total_reward:.4f}, "
              f"ç»“æŸ={done}")
        
        if done:
            print(f"\nğŸ å›åˆç»“æŸ (ç¬¬{step+1}æ­¥)")
            break
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print(f"   æœ€ç»ˆç´¯è®¡å¥–åŠ±: {total_reward:.4f}")
    print(f"   å¹³å‡æ­¥éª¤å¥–åŠ±: {total_reward/(step+1):.4f}")
    
    env.close()

def demo_multi_agent_usage():
    """æ¼”ç¤ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼ˆRL vs ä¼ ç»ŸAI ROBOTï¼‰"""
    print("\n" + "="*70)
    print("ğŸ¤– å¤šæ™ºèƒ½ä½“ GoBigger ç¯å¢ƒæ¼”ç¤º (RL Agent vs ä¼ ç»ŸAI ROBOT)")
    print("="*70 + "\n")
    
    if not MULTI_AGENT_AVAILABLE:
        print("âŒ å¤šæ™ºèƒ½ä½“åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿ç¼–è¯‘äº†å¤šæ™ºèƒ½ä½“æ¨¡å—")
        return
    
    # åˆ›å»ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒ
    config = {
        'max_episode_steps': 150,
        'ai_opponent_count': 3,        # 3ä¸ªä¼ ç»ŸAIå¯¹æ‰‹
        'max_frames': 800,
        'ai_strategies': ['FOOD_HUNTER', 'AGGRESSIVE', 'RANDOM']
    }
    
    env = MultiAgentGoBiggerEnv(config)
    
    # æ£€æŸ¥ç¯å¢ƒ
    try:
        from stable_baselines3.common.env_checker import check_env
        print("ğŸ” ä½¿ç”¨ stable-baselines3 æ£€æŸ¥å¤šæ™ºèƒ½ä½“ç¯å¢ƒ...")
        check_env(env)
        print("âœ… å¤šæ™ºèƒ½ä½“ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\n")
    except ImportError:
        print("âš ï¸ æœªå®‰è£… stable-baselines3ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥\n")
    
    # é‡ç½®ç¯å¢ƒ
    if GYMNASIUM_AVAILABLE:
        obs, info = env.reset()
    else:
        obs = env.reset()
        info = {}
    
    print(f"ğŸ å¤šæ™ºèƒ½ä½“ç¯å¢ƒé‡ç½®å®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"   è§‚å¯Ÿç»´åº¦: {obs.shape}")
    print(f"   AIå¯¹æ‰‹ç­–ç•¥: {config['ai_strategies']}")
    print()
    
    # è¿è¡Œå‡ æ­¥ï¼Œå±•ç¤ºRL vs ä¼ ç»ŸAIçš„äº¤äº’
    total_reward = 0
    best_rank = float('inf')
    
    for step in range(15):
        # æ™ºèƒ½åŠ¨ä½œç­–ç•¥ï¼šå‰å‡ æ­¥æ¢ç´¢ï¼Œåé¢æ›´æ¿€è¿›
        if step < 5:
            # æ¢ç´¢é˜¶æ®µï¼šéšæœºç§»åŠ¨ + å¶å°”åˆ†è£‚
            action = env.action_space.sample()
            action[2] = 1 if step == 3 and np.random.random() > 0.5 else 0  # å¶å°”åˆ†è£‚
        else:
            # æ¿€è¿›é˜¶æ®µï¼šæ›´å¤šé«˜çº§åŠ¨ä½œ
            angle = step * 0.5  # èºæ—‹ç§»åŠ¨
            action = np.array([
                np.cos(angle) * 0.8,  # xæ–¹å‘
                np.sin(angle) * 0.8,  # yæ–¹å‘
                1 if step % 7 == 0 else (2 if step % 5 == 0 else 0)  # åˆ†è£‚æˆ–å–·å°„
            ], dtype=np.float32)
        
        if GYMNASIUM_AVAILABLE:
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        else:
            obs, reward, done, info = env.step(action)
            terminated, truncated = done, False
        
        total_reward += reward
        
        # è·å–å½“å‰æ’å
        current_rank = info.get('team_rank', 1) if 'team_rank' in env.engine.get_reward_info() else 1
        if current_rank < best_rank:
            best_rank = current_rank
        
        print(f"æ­¥éª¤ {step+1:02d}: "
              f"åŠ¨ä½œ=[{action[0]:.2f}, {action[1]:.2f}, {['ç§»åŠ¨','åˆ†è£‚','å–·å°„'][int(np.round(action[2]))]}], "
              f"å¥–åŠ±={reward:.4f}, "
              f"ç´¯è®¡å¥–åŠ±={total_reward:.4f}")
        
        # æ¯5æ­¥æ¸²æŸ“ä¸€æ¬¡ï¼Œæ˜¾ç¤ºå¤šæ™ºèƒ½ä½“çŠ¶æ€
        if step % 5 == 0:
            print("\n" + "-" * 50)
            env.render()
            print("-" * 50 + "\n")
        
        if done:
            print(f"\nğŸ å¤šæ™ºèƒ½ä½“å›åˆç»“æŸ (ç¬¬{step+1}æ­¥)")
            if 'final_rank' in info:
                print(f"   ğŸ† æœ€ç»ˆæ’å: {info['final_rank']}/{info.get('total_teams', 1)}")
                print(f"   ğŸ“Š åˆ†æ•°å˜åŒ–: {info.get('score_delta', 0):+.1f}")
                print(f"   ğŸ¤– å­˜æ´»AIå¯¹æ‰‹: {info.get('ai_opponents_alive', 0)}")
                print(f"   ğŸ“ˆ æ’åè¿›åº¦: {info.get('rank_progress', 0)*100:.1f}%")
            break
    
    print(f"\nâœ… å¤šæ™ºèƒ½ä½“æ¼”ç¤ºå®Œæˆï¼")
    print(f"   ğŸ¯ æœ€ç»ˆç´¯è®¡å¥–åŠ±: {total_reward:.4f}")
    print(f"   ğŸ“Š å¹³å‡æ­¥éª¤å¥–åŠ±: {total_reward/(step+1):.4f}")
    print(f"   ğŸ† æœ€ä½³æ’å: {best_rank}")
    print(f"   ğŸ¤– AIå¯¹æ‰‹ç±»å‹: FOOD_HUNTER(é£Ÿç‰©çŒæ‰‹), AGGRESSIVE(æ”»å‡»å‹), RANDOM(éšæœº)")
    
    env.close()

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--multi-agent":
        print("ğŸš€ å¯åŠ¨å¤šæ™ºèƒ½ä½“æ¨¡å¼æ¼”ç¤º...")
        demo_multi_agent_usage()
    else:
        print("ğŸš€ å¯åŠ¨å•æ™ºèƒ½ä½“æ¨¡å¼æ¼”ç¤º...")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ --multi-agent å‚æ•°å¯åŠ¨å¤šæ™ºèƒ½ä½“æ¨¡å¼")
        demo_gymnasium_usage()
    demo_multi_agent_usage()