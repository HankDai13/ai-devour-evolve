#!/usr/bin/env python3
"""
è°ƒè¯•å¥–åŠ±è®¡ç®—çš„è¯¦ç»†è„šæœ¬
åˆ†ææ¯ä¸€æ­¥çš„å¥–åŠ±è®¡ç®—è¿‡ç¨‹ï¼Œæ‰¾å‡ºä¸ºä»€ä¹ˆå¥–åŠ±ä¸€ç›´ä¸º0
"""
import sys
import os
from pathlib import Path
import numpy as np

# æ·»åŠ è·¯å¾„
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
if build_dir.exists():
    sys.path.insert(0, str(build_dir))
    os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

from gobigger_gym_env import GoBiggerEnv

def debug_reward_step_by_step():
    """é€æ­¥è°ƒè¯•å¥–åŠ±è®¡ç®—"""
    print("ğŸ” å¼€å§‹é€æ­¥è°ƒè¯•å¥–åŠ±è®¡ç®—...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = GoBiggerEnv({'max_episode_steps': 50})
    obs, info = env.reset()
    
    print(f"åˆå§‹è§‚å¯Ÿç»´åº¦: {obs.shape}")
    print(f"åˆå§‹ä¿¡æ¯: {info}")
    
    total_reward = 0
    rewards_detail = []
    
    for step in range(20):  # è¿è¡Œ20æ­¥è¿›è¡Œè¯¦ç»†åˆ†æ
        # é€‰æ‹©éšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        
        # è®°å½•æ­¥éª¤å‰çš„çŠ¶æ€
        if env.current_obs and env.current_obs.player_states:
            ps_before = list(env.current_obs.player_states.values())[0]
            score_before = ps_before.score
            cells_before = len(ps_before.clone) if hasattr(ps_before, 'clone') and isinstance(ps_before.clone, list) else 1
            can_eject_before = ps_before.can_eject
            can_split_before = ps_before.can_split
        else:
            score_before = 0
            cells_before = 0
            can_eject_before = False
            can_split_before = False
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        
        # è®°å½•æ­¥éª¤åçš„çŠ¶æ€
        if env.current_obs and env.current_obs.player_states:
            ps_after = list(env.current_obs.player_states.values())[0]
            score_after = ps_after.score
            cells_after = len(ps_after.clone) if hasattr(ps_after, 'clone') and isinstance(ps_after.clone, list) else 1
            can_eject_after = ps_after.can_eject
            can_split_after = ps_after.can_split
        else:
            score_after = 0
            cells_after = 0
            can_eject_after = False
            can_split_after = False
        
        # æ‰‹åŠ¨è®¡ç®—å¥–åŠ±ç»„ä»¶ï¼ˆæ¨¡æ‹Ÿå¥–åŠ±å‡½æ•°é€»è¾‘ï¼‰
        score_delta = score_after - score_before
        score_reward = score_delta / 100.0
        time_penalty = -0.001
        death_penalty = -10.0 if env.engine.is_done() else 0.0
        survival_reward = 0.01 if not env.engine.is_done() else 0.0
        cell_delta = cells_after - cells_before
        size_reward = cell_delta * 0.1
        
        manual_reward = score_reward + time_penalty + death_penalty + survival_reward + size_reward
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯
        step_detail = {
            'step': step + 1,
            'action': action.tolist() if hasattr(action, 'tolist') else action,
            'score_before': score_before,
            'score_after': score_after,
            'score_delta': score_delta,
            'cells_before': cells_before,
            'cells_after': cells_after,
            'cell_delta': cell_delta,
            'can_eject_before': can_eject_before,
            'can_eject_after': can_eject_after,
            'can_split_before': can_split_before,
            'can_split_after': can_split_after,
            'score_reward': score_reward,
            'time_penalty': time_penalty,
            'death_penalty': death_penalty,
            'survival_reward': survival_reward,
            'size_reward': size_reward,
            'manual_reward': manual_reward,
            'env_reward': reward,
            'reward_match': abs(manual_reward - reward) < 1e-6,
            'terminated': terminated,
            'truncated': truncated,
            'info': info
        }
        
        rewards_detail.append(step_detail)
        total_reward += reward
        
        # æ‰“å°æ­¥éª¤è¯¦æƒ…
        print(f"\n--- æ­¥éª¤ {step + 1} ---")
        print(f"åŠ¨ä½œ: {step_detail['action']}")
        print(f"åˆ†æ•°å˜åŒ–: {score_before:.2f} â†’ {score_after:.2f} (Î”{score_delta:+.2f})")
        print(f"ç»†èƒæ•°å˜åŒ–: {cells_before} â†’ {cells_after} (Î”{cell_delta:+d})")
        print(f"èƒ½åŠ›çŠ¶æ€: åçƒ({can_eject_before}â†’{can_eject_after}), åˆ†è£‚({can_split_before}â†’{can_split_after})")
        print(f"å¥–åŠ±ç»„ä»¶:")
        print(f"  - åˆ†æ•°å¥–åŠ±: {score_reward:+.4f}")
        print(f"  - æ—¶é—´æƒ©ç½š: {time_penalty:+.4f}")
        print(f"  - æ­»äº¡æƒ©ç½š: {death_penalty:+.4f}")
        print(f"  - ç”Ÿå­˜å¥–åŠ±: {survival_reward:+.4f}")
        print(f"  - å°ºå¯¸å¥–åŠ±: {size_reward:+.4f}")
        print(f"æ‰‹åŠ¨è®¡ç®—å¥–åŠ±: {manual_reward:+.4f}")
        print(f"ç¯å¢ƒè¿”å›å¥–åŠ±: {reward:+.4f}")
        print(f"å¥–åŠ±åŒ¹é…: {'âœ…' if step_detail['reward_match'] else 'âŒ'}")
        print(f"æ¸¸æˆçŠ¶æ€: ç»ˆæ­¢={terminated}, æˆªæ–­={truncated}")
        
        if terminated or truncated:
            print(f"ğŸ Episode ç»“æŸåœ¨æ­¥éª¤ {step + 1}")
            break
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š å¥–åŠ±åˆ†ææ€»ç»“")
    print("="*60)
    
    non_zero_rewards = [r for r in rewards_detail if abs(r['env_reward']) > 1e-6]
    positive_rewards = [r for r in rewards_detail if r['env_reward'] > 0]
    negative_rewards = [r for r in rewards_detail if r['env_reward'] < 0]
    
    print(f"æ€»æ­¥æ•°: {len(rewards_detail)}")
    print(f"éé›¶å¥–åŠ±æ­¥æ•°: {len(non_zero_rewards)}")
    print(f"æ­£å¥–åŠ±æ­¥æ•°: {len(positive_rewards)}")
    print(f"è´Ÿå¥–åŠ±æ­¥æ•°: {len(negative_rewards)}")
    print(f"æ€»å¥–åŠ±: {total_reward:.4f}")
    print(f"å¹³å‡å¥–åŠ±: {total_reward / len(rewards_detail):.4f}")
    
    # åˆ†æå¥–åŠ±ç»„ä»¶
    total_score_reward = sum(r['score_reward'] for r in rewards_detail)
    total_time_penalty = sum(r['time_penalty'] for r in rewards_detail)
    total_death_penalty = sum(r['death_penalty'] for r in rewards_detail)
    total_survival_reward = sum(r['survival_reward'] for r in rewards_detail)
    total_size_reward = sum(r['size_reward'] for r in rewards_detail)
    
    print(f"\nå¥–åŠ±ç»„ä»¶æ€»å’Œ:")
    print(f"  - åˆ†æ•°å¥–åŠ±æ€»å’Œ: {total_score_reward:+.4f}")
    print(f"  - æ—¶é—´æƒ©ç½šæ€»å’Œ: {total_time_penalty:+.4f}")
    print(f"  - æ­»äº¡æƒ©ç½šæ€»å’Œ: {total_death_penalty:+.4f}")
    print(f"  - ç”Ÿå­˜å¥–åŠ±æ€»å’Œ: {total_survival_reward:+.4f}")
    print(f"  - å°ºå¯¸å¥–åŠ±æ€»å’Œ: {total_size_reward:+.4f}")
    
    # æ‰¾å‡ºåˆ†æ•°æœ‰å˜åŒ–çš„æ­¥éª¤
    score_change_steps = [r for r in rewards_detail if abs(r['score_delta']) > 1e-6]
    if score_change_steps:
        print(f"\nğŸ“ˆ åˆ†æ•°æœ‰å˜åŒ–çš„æ­¥éª¤ ({len(score_change_steps)} ä¸ª):")
        for r in score_change_steps:
            print(f"  æ­¥éª¤ {r['step']}: åˆ†æ•° {r['score_before']:.2f} â†’ {r['score_after']:.2f} "
                  f"(Î”{r['score_delta']:+.2f}), å¥–åŠ±: {r['env_reward']:+.4f}")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰æ­¥éª¤çš„åˆ†æ•°å‘ç”Ÿå˜åŒ–ï¼")
    
    # æ£€æŸ¥å¥–åŠ±è®¡ç®—ä¸€è‡´æ€§
    mismatched_rewards = [r for r in rewards_detail if not r['reward_match']]
    if mismatched_rewards:
        print(f"\nâŒ å¥–åŠ±è®¡ç®—ä¸ä¸€è‡´çš„æ­¥éª¤ ({len(mismatched_rewards)} ä¸ª):")
        for r in mismatched_rewards:
            print(f"  æ­¥éª¤ {r['step']}: æ‰‹åŠ¨={r['manual_reward']:+.4f}, "
                  f"ç¯å¢ƒ={r['env_reward']:+.4f}, å·®å€¼={r['manual_reward']-r['env_reward']:+.4f}")
    else:
        print(f"\nâœ… æ‰€æœ‰æ­¥éª¤çš„å¥–åŠ±è®¡ç®—éƒ½ä¸€è‡´")
    
    return rewards_detail

def check_reward_function_implementation():
    """æ£€æŸ¥å¥–åŠ±å‡½æ•°çš„å…·ä½“å®ç°"""
    print("\nğŸ” æ£€æŸ¥å¥–åŠ±å‡½æ•°å®ç°...")
    
    env = GoBiggerEnv({'max_episode_steps': 10})
    obs, info = env.reset()
    
    # æ£€æŸ¥åˆå§‹çŠ¶æ€
    if env.current_obs and env.current_obs.player_states:
        ps = list(env.current_obs.player_states.values())[0]
        print(f"åˆå§‹åˆ†æ•°: {ps.score}")
        print(f"last_score: {env.last_score}")
        print(f"initial_score: {env.initial_score}")
        
        # æ£€æŸ¥cloneå±æ€§
        if hasattr(ps, 'clone'):
            print(f"clone å±æ€§ç±»å‹: {type(ps.clone)}")
            print(f"clone å†…å®¹: {ps.clone}")
            if isinstance(ps.clone, list):
                print(f"clone åˆ—è¡¨é•¿åº¦: {len(ps.clone)}")
            else:
                print(f"clone ä¸æ˜¯åˆ—è¡¨")
        else:
            print(f"âŒ player_state æ²¡æœ‰ clone å±æ€§")
        
        # æµ‹è¯•æ‰‹åŠ¨å¥–åŠ±è®¡ç®—
        print(f"\næµ‹è¯•æ‰‹åŠ¨å¥–åŠ±è®¡ç®—:")
        manual_reward = env._calculate_reward()
        print(f"æ‰‹åŠ¨è®¡ç®—çš„å¥–åŠ±: {manual_reward}")
    
    # æ‰§è¡Œä¸€æ­¥
    action = env.action_space.sample()
    obs, reward, terminated, truncated, info = env.step(action)
    
    print(f"\næ‰§è¡Œä¸€æ­¥å:")
    print(f"åŠ¨ä½œ: {action}")
    print(f"å¥–åŠ±: {reward}")
    print(f"terminated: {terminated}")
    print(f"truncated: {truncated}")
    
    if env.current_obs and env.current_obs.player_states:
        ps = list(env.current_obs.player_states.values())[0]
        print(f"æ–°åˆ†æ•°: {ps.score}")
        print(f"last_score: {env.last_score}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹å¥–åŠ±è®¡ç®—è°ƒè¯•")
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ£€æŸ¥å¥–åŠ±å‡½æ•°å®ç°
    check_reward_function_implementation()
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šé€æ­¥è°ƒè¯•
    debug_reward_step_by_step()
    
    print("\nğŸ¯ è°ƒè¯•å®Œæˆï¼")
