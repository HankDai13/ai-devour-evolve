#!/usr/bin/env python3
"""
å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - ä¼˜åŒ–GoBiggerå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±æœºåˆ¶
===================================================

æ­¤è„šæœ¬æä¾›äº†ä¸€ä¸ªæ›´åŠ ç»†è‡´çš„å¥–åŠ±ç³»ç»Ÿï¼Œä¸“é—¨è®¾è®¡æ¥ï¼š
1. é¼“åŠ±æ›´é«˜æ•ˆçš„é£Ÿç‰©æ¶ˆè´¹
2. å¥–åŠ±é€‚æ—¶çš„åˆ†è£‚ç­–ç•¥
3. å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
4. æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024å¹´
"""

import numpy as np
import math

class EnhancedRewardCalculator:
    """å¢å¼ºå¥–åŠ±è®¡ç®—å™¨"""
    
    def __init__(self, config=None):
        self.config = config or {}
        
        # å¥–åŠ±æƒé‡é…ç½®ï¼ˆå¯è°ƒå‚æ•°ï¼‰
        self.weights = {
            'score_growth': 2.0,        # åˆ†æ•°å¢é•¿å¥–åŠ±æƒé‡
            'efficiency': 1.0,          # æ•ˆç‡å¥–åŠ±æƒé‡
            'exploration': 0.5,         # æ¢ç´¢å¥–åŠ±æƒé‡
            'strategic_split': 1.5,     # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±æƒé‡
            'food_density': 0.8,        # é£Ÿç‰©å¯†åº¦å¥–åŠ±æƒé‡
            'survival': 0.02,           # ç”Ÿå­˜å¥–åŠ±æƒé‡
            'time_penalty': -0.001,     # æ—¶é—´æƒ©ç½šæƒé‡
            'death_penalty': -15.0,     # æ­»äº¡æƒ©ç½šæƒé‡
        }
        
        # å†å²çŠ¶æ€è¿½è¸ª
        self.history = []
        self.moving_avg_score = 0.0
        self.moving_avg_window = 10
        
        # æˆ˜ç•¥è¡Œä¸ºè¿½è¸ª
        self.consecutive_growth_steps = 0
        self.last_split_step = -1
        self.food_eaten_count = 0
        
        # åŒºåŸŸæ¢ç´¢è¿½è¸ª
        self.visited_regions = set()
        self.region_size = 200  # åŒºåŸŸå¤§å°ï¼ˆç”¨äºæ¢ç´¢å¥–åŠ±ï¼‰
        
    def reset(self):
        """é‡ç½®å¥–åŠ±è®¡ç®—å™¨çŠ¶æ€"""
        self.history.clear()
        self.moving_avg_score = 0.0
        self.consecutive_growth_steps = 0
        self.last_split_step = -1
        self.food_eaten_count = 0
        self.visited_regions.clear()
        
    def calculate_reward(self, current_state, previous_state, action, step_count):
        """
        è®¡ç®—å¢å¼ºå¥–åŠ±
        
        Args:
            current_state: å½“å‰æ¸¸æˆçŠ¶æ€
            previous_state: ä¸Šä¸€æ­¥æ¸¸æˆçŠ¶æ€  
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            step_count: å½“å‰æ­¥æ•°
            
        Returns:
            float: è®¡ç®—å¾—å‡ºçš„å¥–åŠ±å€¼
        """
        if not current_state or not current_state.player_states:
            return self.weights['death_penalty']
        
        ps_current = list(current_state.player_states.values())[0]
        
        # è·å–åŸºç¡€ä¿¡æ¯
        if previous_state and previous_state.player_states:
            ps_previous = list(previous_state.player_states.values())[0]
        else:
            ps_previous = None
            
        reward_components = {}
        
        # 1. åˆ†æ•°å¢é•¿å¥–åŠ±ï¼ˆéçº¿æ€§ï¼Œé¼“åŠ±æŒç»­å¢é•¿ï¼‰
        reward_components['score_growth'] = self._calculate_score_growth_reward(
            ps_current, ps_previous)
            
        # 2. æ•ˆç‡å¥–åŠ±ï¼ˆåˆ†æ•°å¢é•¿ä¸ç§»åŠ¨è·ç¦»çš„æ¯”ç‡ï¼‰
        reward_components['efficiency'] = self._calculate_efficiency_reward(
            ps_current, ps_previous, action)
            
        # 3. æ¢ç´¢å¥–åŠ±ï¼ˆé¼“åŠ±è®¿é—®æ–°åŒºåŸŸï¼‰
        reward_components['exploration'] = self._calculate_exploration_reward(
            ps_current)
            
        # 4. æˆ˜ç•¥åˆ†è£‚å¥–åŠ±ï¼ˆåœ¨åˆé€‚æ—¶æœºåˆ†è£‚ï¼‰
        reward_components['strategic_split'] = self._calculate_strategic_split_reward(
            ps_current, ps_previous, action, step_count)
            
        # 5. é£Ÿç‰©å¯†åº¦å¥–åŠ±ï¼ˆåœ¨é£Ÿç‰©å¯†é›†åŒºåŸŸè·å¾—å¥–åŠ±ï¼‰
        reward_components['food_density'] = self._calculate_food_density_reward(
            ps_current)
            
        # 6. åŸºç¡€å¥–åŠ±ï¼ˆç”Ÿå­˜ã€æ—¶é—´æƒ©ç½šã€æ­»äº¡æƒ©ç½šï¼‰
        reward_components.update(self._calculate_basic_rewards(current_state))
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = 0.0
        for component, value in reward_components.items():
            total_reward += self.weights.get(component, 1.0) * value
            
        # æ›´æ–°å†å²çŠ¶æ€
        self._update_history(ps_current, reward_components, step_count)
        
        return total_reward, reward_components
    
    def _calculate_score_growth_reward(self, ps_current, ps_previous):
        """è®¡ç®—åˆ†æ•°å¢é•¿å¥–åŠ±ï¼ˆéçº¿æ€§ï¼‰"""
        if not ps_previous:
            return 0.0
            
        score_delta = ps_current.score - ps_previous.score
        
        if score_delta > 0:
            # éçº¿æ€§å¥–åŠ±ï¼šé¼“åŠ±æ›´å¤§çš„åˆ†æ•°å¢é•¿
            reward = math.sqrt(score_delta) / 10.0
            self.consecutive_growth_steps += 1
            
            # è¿ç»­å¢é•¿å¥–åŠ±åŠ æˆ
            if self.consecutive_growth_steps >= 3:
                reward *= 1.2  # 20%åŠ æˆ
            if self.consecutive_growth_steps >= 5:
                reward *= 1.5  # é¢å¤–50%åŠ æˆ
                
            self.food_eaten_count += 1
            return reward
        else:
            self.consecutive_growth_steps = 0
            return score_delta / 100.0  # å°çš„è´Ÿå¥–åŠ±
    
    def _calculate_efficiency_reward(self, ps_current, ps_previous, action):
        """è®¡ç®—æ•ˆç‡å¥–åŠ±ï¼ˆåˆ†æ•°å¢é•¿ä¸è¡ŒåŠ¨çš„æ¯”ç‡ï¼‰"""
        if not ps_previous:
            return 0.0
            
        score_delta = ps_current.score - ps_previous.score
        
        # è®¡ç®—ç§»åŠ¨è·ç¦»ï¼ˆå¦‚æœactionåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰
        if len(action) >= 2:
            move_distance = math.sqrt(action[0]**2 + action[1]**2)
        else:
            move_distance = 1.0
        
        if score_delta > 0 and move_distance > 0:
            # æ•ˆç‡ = åˆ†æ•°å¢é•¿ / ç§»åŠ¨æˆæœ¬
            efficiency = score_delta / (move_distance + 0.1)  # é¿å…é™¤é›¶
            return min(efficiency / 50.0, 1.0)  # é™åˆ¶æœ€å¤§å¥–åŠ±
        
        return 0.0
    
    def _calculate_exploration_reward(self, ps_current):
        """è®¡ç®—æ¢ç´¢å¥–åŠ±ï¼ˆé¼“åŠ±è®¿é—®æ–°åŒºåŸŸï¼‰"""
        if not hasattr(ps_current, 'rectangle') or len(ps_current.rectangle) < 4:
            return 0.0
        
        # è·å–å½“å‰ä½ç½®çš„åŒºåŸŸID
        center_x = (ps_current.rectangle[0] + ps_current.rectangle[2]) / 2
        center_y = (ps_current.rectangle[1] + ps_current.rectangle[3]) / 2
        
        region_x = int(center_x // self.region_size)
        region_y = int(center_y // self.region_size)
        region_id = (region_x, region_y)
        
        # å¦‚æœæ˜¯æ–°åŒºåŸŸï¼Œç»™äºˆå¥–åŠ±
        if region_id not in self.visited_regions:
            self.visited_regions.add(region_id)
            return 0.5  # æ¢ç´¢æ–°åŒºåŸŸå¥–åŠ±
        
        return 0.0
    
    def _calculate_strategic_split_reward(self, ps_current, ps_previous, action, step_count):
        """è®¡ç®—æˆ˜ç•¥åˆ†è£‚å¥–åŠ±"""
        if not ps_previous:
            return 0.0
        
        # æ£€æµ‹æ˜¯å¦è¿›è¡Œäº†åˆ†è£‚ï¼ˆå‡è®¾action[2]æ˜¯åˆ†è£‚ä¿¡å·ï¼‰
        split_action = len(action) >= 3 and action[2] > 1.5
        
        if split_action:
            # æ£€æŸ¥åˆ†è£‚æ˜¯å¦åœ¨åˆé€‚çš„æ—¶æœº
            current_cells = len(ps_current.clone) if hasattr(ps_current, 'clone') and isinstance(ps_current.clone, list) else 1
            food_nearby = len(ps_current.food) if hasattr(ps_current, 'food') else 0
            
            # å¦‚æœé™„è¿‘æœ‰è¶³å¤Ÿé£Ÿç‰©ä¸”ä¸Šæ¬¡åˆ†è£‚é—´éš”è¶³å¤Ÿé•¿
            steps_since_split = step_count - self.last_split_step
            if food_nearby >= 3 and steps_since_split >= 20:
                self.last_split_step = step_count
                return 1.0  # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±
            else:
                return -0.2  # ä¸å½“åˆ†è£‚æƒ©ç½š
        
        return 0.0
    
    def _calculate_food_density_reward(self, ps_current):
        """è®¡ç®—é£Ÿç‰©å¯†åº¦å¥–åŠ±ï¼ˆåœ¨é£Ÿç‰©å¯†é›†åŒºåŸŸæ´»åŠ¨ï¼‰"""
        if not hasattr(ps_current, 'food'):
            return 0.0
        
        food_count = len(ps_current.food)
        
        # æ ¹æ®è§†é‡å†…é£Ÿç‰©æ•°é‡ç»™äºˆå¥–åŠ±
        if food_count >= 10:
            return 0.3
        elif food_count >= 5:
            return 0.1
        else:
            return 0.0
    
    def _calculate_basic_rewards(self, current_state):
        """è®¡ç®—åŸºç¡€å¥–åŠ±ï¼ˆç”Ÿå­˜ã€æ—¶é—´ã€æ­»äº¡ï¼‰"""
        rewards = {}
        
        # ç”Ÿå­˜å¥–åŠ±
        if not current_state or not hasattr(current_state, 'player_states') or not current_state.player_states:
            rewards['survival'] = 0.0
            rewards['death_penalty'] = 1.0  # æ ‡è®°æ­»äº¡
        else:
            rewards['survival'] = 1.0
            rewards['death_penalty'] = 0.0
        
        # æ—¶é—´æƒ©ç½šï¼ˆé¼“åŠ±å¿«é€Ÿå†³ç­–ï¼‰
        rewards['time_penalty'] = 1.0
        
        return rewards
    
    def _update_history(self, ps_current, reward_components, step_count):
        """æ›´æ–°å†å²çŠ¶æ€è¿½è¸ª"""
        state_info = {
            'step': step_count,
            'score': ps_current.score,
            'reward_components': reward_components.copy(),
            'cells': len(ps_current.clone) if hasattr(ps_current, 'clone') and isinstance(ps_current.clone, list) else 1,
            'food_count': len(ps_current.food) if hasattr(ps_current, 'food') else 0
        }
        
        self.history.append(state_info)
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.history) > 100:
            self.history.pop(0)
        
        # æ›´æ–°ç§»åŠ¨å¹³å‡åˆ†æ•°
        if len(self.history) >= self.moving_avg_window:
            recent_scores = [h['score'] for h in self.history[-self.moving_avg_window:]]
            self.moving_avg_score = sum(recent_scores) / len(recent_scores)
    
    def get_reward_analysis(self):
        """è·å–å¥–åŠ±åˆ†ææŠ¥å‘Š"""
        if not self.history:
            return "æš‚æ— å†å²æ•°æ®"
        
        latest = self.history[-1]
        
        analysis = f"""
å¥–åŠ±åˆ†ææŠ¥å‘Š
===========
å½“å‰æ­¥æ•°: {latest['step']}
å½“å‰åˆ†æ•°: {latest['score']:.2f}
ç§»åŠ¨å¹³å‡åˆ†æ•°: {self.moving_avg_score:.2f}
è¿ç»­å¢é•¿æ­¥æ•°: {self.consecutive_growth_steps}
é£Ÿç‰©æ¶ˆè´¹æ¬¡æ•°: {self.food_eaten_count}
æ¢ç´¢åŒºåŸŸæ•°: {len(self.visited_regions)}

æœ€æ–°å¥–åŠ±ç»„ä»¶:
"""
        
        for component, value in latest['reward_components'].items():
            weighted_value = self.weights.get(component, 1.0) * value
            analysis += f"  {component}: {value:.4f} (æƒé‡å: {weighted_value:.4f})\n"
        
        return analysis

# ä½¿ç”¨ç¤ºä¾‹
def demo_enhanced_reward():
    """æ¼”ç¤ºå¢å¼ºå¥–åŠ±ç³»ç»Ÿ"""
    print("ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    calculator = EnhancedRewardCalculator()
    
    # æ¨¡æ‹Ÿä¸€äº›æ¸¸æˆçŠ¶æ€...
    # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æµ‹è¯•ä»£ç 
    
    print("å¢å¼ºå¥–åŠ±ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
    print("\nç‰¹æ€§:")
    print("âœ… éçº¿æ€§åˆ†æ•°å¢é•¿å¥–åŠ±")
    print("âœ… æ•ˆç‡å¥–åŠ±ï¼ˆåˆ†æ•°/ç§»åŠ¨æ¯”ç‡ï¼‰")
    print("âœ… æ¢ç´¢æ–°åŒºåŸŸå¥–åŠ±")
    print("âœ… æˆ˜ç•¥åˆ†è£‚å¥–åŠ±")
    print("âœ… é£Ÿç‰©å¯†åº¦å¥–åŠ±")
    print("âœ… è¿ç»­å¢é•¿åŠ æˆ")

if __name__ == "__main__":
    demo_enhanced_reward()
