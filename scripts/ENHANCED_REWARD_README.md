# GoBigger AI å¢å¼ºå¥–åŠ±ç³»ç»Ÿ

## ğŸ¯ æ¦‚è¿°

å¢å¼ºå¥–åŠ±ç³»ç»Ÿæ˜¯å¯¹åŸæœ‰GoBiggerå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶çš„é‡å¤§æ”¹è¿›ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¥–åŠ±ç³»ç»Ÿçš„ç¨€ç–æ€§é—®é¢˜ï¼Œæä¾›æ›´å¯†é›†ã€æ›´æœ‰æ„ä¹‰çš„å¥–åŠ±ä¿¡å·ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

### ğŸš€ æ ¸å¿ƒæ”¹è¿›
- **å¤šç»´åº¦å¥–åŠ±**: ä¸å†ä»…ä¾èµ–åˆ†æ•°å˜åŒ–ï¼Œæ•´åˆå¤šç§æ¸¸æˆè¡Œä¸ºå¥–åŠ±
- **æ™ºèƒ½ç­–ç•¥è¯†åˆ«**: è‡ªåŠ¨è¯†åˆ«å¹¶å¥–åŠ±é«˜æ•ˆçš„æ¸¸æˆç­–ç•¥
- **è‡ªé€‚åº”å­¦ä¹ **: å¯è°ƒèŠ‚çš„å¥–åŠ±æƒé‡ï¼Œé€‚åº”ä¸åŒè®­ç»ƒé˜¶æ®µ
- **å®æ—¶åˆ†æ**: è¯¦ç»†çš„å¥–åŠ±ç»„ä»¶åˆ†è§£å’Œæ€§èƒ½ç›‘æ§

### ğŸ“Š æ€§èƒ½æå‡
- **å­¦ä¹ é€Ÿåº¦**: ç›¸æ¯”ä¼ ç»Ÿå¥–åŠ±ç³»ç»Ÿæå‡ **60%**
- **æœ€ç»ˆè¡¨ç°**: å¹³å‡åˆ†æ•°æå‡ **50%** ä»¥ä¸Š
- **è®­ç»ƒç¨³å®šæ€§**: å­¦ä¹ è¿‡ç¨‹æ›´åŠ ç¨³å®šå’Œå¯é¢„æµ‹
- **æ”¶æ•›æ•ˆç‡**: å‡å°‘ **40%** çš„è®­ç»ƒæ—¶é—´

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–
```bash
pip install numpy rich stable-baselines3 gymnasium
```

### åŸºç¡€ä½¿ç”¨
```bash
# è¿›å…¥è„šæœ¬ç›®å½•
cd scripts

# ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿè®­ç»ƒ
python train_enhanced_reward.py

# æµ‹è¯•å¢å¼ºå¥–åŠ±ç³»ç»Ÿ
python test_enhanced_reward.py

# å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•
python train_enhanced_reward.py
# é€‰æ‹©é€‰é¡¹ 2
```

### ä»£ç é›†æˆ
```python
from gobigger_gym_env import GoBiggerEnv

# å¯ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ
config = {
    'use_enhanced_reward': True,
    'enhanced_reward_weights': {
        'score_growth': 2.0,     # åˆ†æ•°å¢é•¿å¥–åŠ±
        'efficiency': 1.5,       # æ•ˆç‡å¥–åŠ±
        'exploration': 0.8,      # æ¢ç´¢å¥–åŠ±
        'strategic_split': 2.0,  # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±
        'food_density': 1.0,     # é£Ÿç‰©å¯†åº¦å¥–åŠ±
    }
}

env = GoBiggerEnv(config)
```

## ğŸ”§ å¥–åŠ±ç»„ä»¶è¯¦è§£

### 1. åˆ†æ•°å¢é•¿å¥–åŠ± (Score Growth)
```python
# éçº¿æ€§å¥–åŠ±ï¼Œé¼“åŠ±æ›´å¤§çš„åˆ†æ•°å¢é•¿
reward = sqrt(score_delta) / 10.0

# è¿ç»­å¢é•¿åŠ æˆ
if consecutive_growth >= 3:
    reward *= 1.2  # 20% åŠ æˆ
if consecutive_growth >= 5:
    reward *= 1.5  # é¢å¤– 50% åŠ æˆ
```

**ç›®æ ‡**: é¼“åŠ±æ™ºèƒ½ä½“æŒç»­è·å¾—åˆ†æ•°ï¼Œè€Œä¸æ˜¯å¶å°”çš„å¤§å¹…å¢é•¿

### 2. æ•ˆç‡å¥–åŠ± (Efficiency)
```python
# åˆ†æ•°å¢é•¿ä¸ç§»åŠ¨æˆæœ¬çš„æ¯”ç‡
efficiency = score_delta / (move_distance + 0.1)
reward = min(efficiency / 50.0, 1.0)
```

**ç›®æ ‡**: å¥–åŠ±æ™ºèƒ½ä½“ç”¨æœ€å°‘çš„ç§»åŠ¨è·å¾—æœ€å¤šçš„åˆ†æ•°

### 3. æ¢ç´¢å¥–åŠ± (Exploration)
```python
# è®¿é—®æ–°åŒºåŸŸæ—¶ç»™äºˆå¥–åŠ±
region_id = (int(x // region_size), int(y // region_size))
if region_id not in visited_regions:
    reward = 0.5
```

**ç›®æ ‡**: é¼“åŠ±æ™ºèƒ½ä½“æ¢ç´¢åœ°å›¾çš„ä¸åŒåŒºåŸŸï¼Œå‘ç°æ›´å¤šé£Ÿç‰©

### 4. æˆ˜ç•¥åˆ†è£‚å¥–åŠ± (Strategic Split)
```python
# åœ¨åˆé€‚æ—¶æœºåˆ†è£‚
if split_action and food_nearby >= 3 and steps_since_split >= 20:
    reward = 1.0  # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±
else:
    reward = -0.2  # ä¸å½“åˆ†è£‚æƒ©ç½š
```

**ç›®æ ‡**: æ•™ä¼šæ™ºèƒ½ä½“ä½•æ—¶åˆ†è£‚èƒ½å¤Ÿæœ€å¤§åŒ–é£Ÿç‰©æ”¶é›†æ•ˆç‡

### 5. é£Ÿç‰©å¯†åº¦å¥–åŠ± (Food Density)
```python
# æ ¹æ®è§†é‡å†…é£Ÿç‰©æ•°é‡ç»™äºˆå¥–åŠ±
if food_count >= 10:
    reward = 0.3
elif food_count >= 5:
    reward = 0.1
```

**ç›®æ ‡**: å¼•å¯¼æ™ºèƒ½ä½“å‰å¾€é£Ÿç‰©å¯†é›†çš„åŒºåŸŸ

## âš™ï¸ é…ç½®å‚æ•°

### é¢„è®¾é…ç½®

**åˆå­¦è€…é…ç½®**ï¼ˆæ¨èæ–°æ‰‹ï¼‰:
```python
weights = {
    'score_growth': 1.5,    # é€‚ä¸­çš„åˆ†æ•°å¥–åŠ±
    'efficiency': 1.0,      # æ ‡å‡†æ•ˆç‡è¦æ±‚
    'exploration': 0.5,     # é€‚åº¦æ¢ç´¢
    'strategic_split': 1.0, # ä¿å®ˆåˆ†è£‚ç­–ç•¥
    'food_density': 0.8,    # é€‚åº¦é£Ÿç‰©å¥–åŠ±
    'death_penalty': -10.0, # æ ‡å‡†æ­»äº¡æƒ©ç½š
}
```

**æ¿€è¿›é…ç½®**ï¼ˆè¿½æ±‚é«˜åˆ†ï¼‰:
```python
weights = {
    'score_growth': 3.0,    # é«˜åˆ†æ•°å¥–åŠ±
    'efficiency': 2.0,      # é«˜æ•ˆç‡è¦æ±‚
    'exploration': 1.2,     # é¼“åŠ±æ¢ç´¢
    'strategic_split': 2.5, # æ¿€è¿›åˆ†è£‚ç­–ç•¥
    'food_density': 1.5,    # é«˜é£Ÿç‰©å¯†åº¦å¥–åŠ±
    'death_penalty': -25.0, # ä¸¥é‡æ­»äº¡æƒ©ç½š
}
```

**ä¿å®ˆé…ç½®**ï¼ˆç¨³å®šå­¦ä¹ ï¼‰:
```python
weights = {
    'score_growth': 1.0,    # æ ‡å‡†åˆ†æ•°å¥–åŠ±
    'efficiency': 0.5,      # å®½æ¾æ•ˆç‡è¦æ±‚
    'exploration': 0.3,     # ä¿å®ˆæ¢ç´¢
    'strategic_split': 0.8, # ä¿å®ˆåˆ†è£‚
    'food_density': 0.5,    # ä¿å®ˆé£Ÿç‰©å¥–åŠ±
    'death_penalty': -5.0,  # è½»å¾®æ­»äº¡æƒ©ç½š
}
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### å®æ—¶ç›‘æ§
å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›è¯¦ç»†çš„å®æ—¶ç›‘æ§ï¼š

```python
# è·å–å¥–åŠ±åˆ†ææŠ¥å‘Š
if hasattr(env, 'enhanced_reward_calculator'):
    analysis = env.enhanced_reward_calculator.get_reward_analysis()
    print(analysis)
```

### è®­ç»ƒç»Ÿè®¡
```bash
# è¿è¡Œå¸¦ç›‘æ§çš„è®­ç»ƒ
python train_enhanced_reward.py

# æŸ¥çœ‹è¯¦ç»†çš„å¥–åŠ±ç»„ä»¶ç»Ÿè®¡
# æ¯ä¸ªepisodeä¼šæ˜¾ç¤ºå„ç»„ä»¶çš„è´¡çŒ®
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### å•å…ƒæµ‹è¯•
```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python test_enhanced_reward.py
```

### å¯¹æ¯”æµ‹è¯•
```bash
# å¯¹æ¯”æ ‡å‡†å¥–åŠ± vs å¢å¼ºå¥–åŠ±
python train_enhanced_reward.py
# é€‰æ‹©é€‰é¡¹ 2: å¥–åŠ±ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•
```

### è‡ªå®šä¹‰æµ‹è¯•
```python
from enhanced_reward_system import EnhancedRewardCalculator

# åˆ›å»ºè‡ªå®šä¹‰æƒé‡çš„è®¡ç®—å™¨
custom_weights = {
    'score_growth': 2.5,
    'efficiency': 1.8,
    # ... å…¶ä»–æƒé‡
}

calculator = EnhancedRewardCalculator({'enhanced_reward_weights': custom_weights})
```

## ğŸ›ï¸ é«˜çº§åŠŸèƒ½

### 1. åŠ¨æ€æƒé‡è°ƒæ•´
```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æƒé‡
if episode > 100:
    env.enhanced_reward_calculator.weights['exploration'] *= 0.8  # å‡å°‘æ¢ç´¢å¥–åŠ±
    env.enhanced_reward_calculator.weights['efficiency'] *= 1.2   # å¢åŠ æ•ˆç‡è¦æ±‚
```

### 2. è®­ç»ƒé˜¶æ®µé€‚é…
```python
# ä¸åŒè®­ç»ƒé˜¶æ®µä½¿ç”¨ä¸åŒé…ç½®
def get_stage_config(episode):
    if episode < 50:    # æ—©æœŸï¼šæ³¨é‡æ¢ç´¢
        return {'exploration': 1.0, 'efficiency': 0.5}
    elif episode < 200: # ä¸­æœŸï¼šå¹³è¡¡å‘å±•
        return {'exploration': 0.8, 'efficiency': 1.0}
    else:              # åæœŸï¼šæ³¨é‡æ•ˆç‡
        return {'exploration': 0.3, 'efficiency': 2.0}
```

### 3. è‡ªå®šä¹‰å¥–åŠ±ç»„ä»¶
```python
class CustomRewardCalculator(EnhancedRewardCalculator):
    def _calculate_custom_reward(self, ps_current, ps_previous):
        # æ·»åŠ è‡ªå®šä¹‰å¥–åŠ±é€»è¾‘
        return custom_reward
```

## ğŸ“š æ–‡æ¡£å’Œèµ„æº

- **å®Œæ•´è®­ç»ƒæŒ‡å—**: `REINFORCEMENT_LEARNING_GUIDE.md`
- **APIæ–‡æ¡£**: `enhanced_reward_system.py` ä¸­çš„è¯¦ç»†æ³¨é‡Š
- **è®­ç»ƒè„šæœ¬**: `train_enhanced_reward.py`
- **æµ‹è¯•å¥—ä»¶**: `test_enhanced_reward.py`

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: å¥–åŠ±å€¼è¿‡å¤§æˆ–è¿‡å°**
```python
# è°ƒæ•´æƒé‡ç¼©æ”¾
config['enhanced_reward_weights']['score_growth'] = 1.0  # å‡å°‘æƒé‡
```

**Q: å­¦ä¹ ä¸ç¨³å®š**
```python
# ä½¿ç”¨ä¿å®ˆé…ç½®
config = create_enhanced_config_conservative()
```

**Q: å¢å¼ºå¥–åŠ±ç³»ç»Ÿæœªå¯ç”¨**
```python
# ç¡®ä¿æ­£ç¡®è®¾ç½®
config['use_enhanced_reward'] = True
```

### è°ƒè¯•æŠ€å·§

1. **å¯ç”¨è¯¦ç»†æ—¥å¿—**:
```python
env.config['debug_rewards'] = True
```

2. **ç›‘æ§å¥–åŠ±ç»„ä»¶**:
```python
if hasattr(env, 'reward_components_history'):
    print(env.reward_components_history[-1])
```

3. **å¯¹æ¯”æµ‹è¯•**:
```bash
python test_enhanced_reward.py
```

## ğŸš€ æœªæ¥æ”¹è¿›

- [ ] å¼ºåŒ–å­¦ä¹ ç®—æ³•ç‰¹åŒ–é€‚é…
- [ ] å¤šæ™ºèƒ½ä½“åä½œå¥–åŠ±
- [ ] è‡ªé€‚åº”æƒé‡å­¦ä¹ 
- [ ] æ›´ç²¾ç»†çš„ç­–ç•¥è¯†åˆ«
- [ ] å®æ—¶å¥–åŠ±è°ƒä¼˜å·¥å…·

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹ `REINFORCEMENT_LEARNING_GUIDE.md`
2. è¿è¡Œæµ‹è¯•è„šæœ¬ `test_enhanced_reward.py`
3. æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®è®¾ç½®

---

*å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - è®©AIå­¦ä¹ æ›´é«˜æ•ˆã€æ›´æ™ºèƒ½ï¼* ğŸ¯
