#!/usr/bin/env python3
"""
ä½¿ç”¨GoBiggeræ ¸å¿ƒå¼•æ“è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
æ”¯æŒå¤šç§RLç®—æ³•ï¼šPPOã€DQNã€A2Cç­‰
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import json
from collections import deque
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥richåº“ç”¨äºç¾åŒ–ç•Œé¢
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.layout import Layout
    from rich.panel import Panel
    from rich.live import Live
    from rich.text import Text
    from rich.align import Align
    from rich.columns import Columns
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")

# è·¯å¾„è®¾ç½®ï¼šå®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from gobigger_gym_env import GoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.monitor import Monitor
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±æ˜¾ç¤ºï¼‰"""
    
    def __init__(self, eval_freq=1000, save_freq=5000, verbose=1, total_timesteps=50000):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.best_mean_reward = -np.inf
        self.episode_rewards = deque(maxlen=100)
        self.episode_scores = deque(maxlen=100)
        self.total_timesteps = total_timesteps
        self.use_enhanced_reward = False  # é»˜è®¤å€¼ï¼Œä¼šåœ¨å¤–éƒ¨è®¾ç½®
        
        # Richç•Œé¢ç»„ä»¶
        if RICH_AVAILABLE:
            self.console = Console()
            self.training_table = Table(title="ğŸ¤– GoBigger RL Training Status")
            self.training_table.add_column("Metric", style="cyan", no_wrap=True)
            self.training_table.add_column("Value", style="magenta")
            
            # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
            self.training_stats = {
                "ep_len_mean": 0,
                "ep_rew_mean": 0,
                "ep_score_mean": 0,
                "fps": 0,
                "iterations": 0,
                "time_elapsed": 0,
                "total_timesteps": 0,
                "approx_kl": 0,
                "clip_fraction": 0,
                "entropy_loss": 0,
                "learning_rate": 0,
                "loss": 0,
                "policy_gradient_loss": 0,
                "value_loss": 0,
                "episodes_completed": 0
            }
            
            self.start_time = time.time()
            self.last_table_update = 0
            self.table_update_interval = 5  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡è¡¨æ ¼
            
            # åˆ›å»ºè¿›åº¦æ¡
            self.progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=self.console,
                expand=True
            )
            self.progress_task = None
        
    def _update_training_table(self):
        """æ›´æ–°è®­ç»ƒçŠ¶æ€è¡¨æ ¼"""
        if not RICH_AVAILABLE:
            return
            
        # æ¸…é™¤æ—§è¡¨æ ¼å†…å®¹
        reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
        self.training_table = Table(title=f"ğŸ¤– GoBigger RL Training Status ({reward_type})")
        self.training_table.add_column("Category", style="cyan", no_wrap=True)
        self.training_table.add_column("Metric", style="blue", no_wrap=True)
        self.training_table.add_column("Value", style="magenta")
        
        # Rollout metrics
        self.training_table.add_row("rollout/", "ep_len_mean", f"{self.training_stats['ep_len_mean']:.0f}")
        self.training_table.add_row("", "ep_rew_mean", f"{self.training_stats['ep_rew_mean']:.2f}")
        self.training_table.add_row("", "ep_score_mean", f"{self.training_stats['ep_score_mean']:.0f}")
        
        # Time metrics
        self.training_table.add_row("time/", "fps", f"{self.training_stats['fps']:.0f}")
        self.training_table.add_row("", "iterations", f"{self.training_stats['iterations']}")
        self.training_table.add_row("", "time_elapsed", f"{self.training_stats['time_elapsed']:.0f}")
        self.training_table.add_row("", "total_timesteps", f"{self.training_stats['total_timesteps']}")
        
        # Training metrics
        if self.training_stats['approx_kl'] > 0:
            self.training_table.add_row("train/", "approx_kl", f"{self.training_stats['approx_kl']:.6f}")
            self.training_table.add_row("", "clip_fraction", f"{self.training_stats['clip_fraction']:.4f}")
            self.training_table.add_row("", "entropy_loss", f"{self.training_stats['entropy_loss']:.2f}")
            self.training_table.add_row("", "learning_rate", f"{self.training_stats['learning_rate']:.6f}")
            self.training_table.add_row("", "loss", f"{self.training_stats['loss']:.6f}")
            self.training_table.add_row("", "policy_gradient_loss", f"{self.training_stats['policy_gradient_loss']:.6f}")
            self.training_table.add_row("", "value_loss", f"{self.training_stats['value_loss']:.6f}")
        
        # Episode info
        self.training_table.add_row("episodes/", "completed", f"{self.training_stats['episodes_completed']}")
        
        # å¢å¼ºå¥–åŠ±ç³»ç»Ÿç‰¹æœ‰ä¿¡æ¯
        if self.use_enhanced_reward:
            self.training_table.add_row("", "", "")  # åˆ†éš”çº¿
            self.training_table.add_row("enhanced/", "system", "ğŸ¯ å¯†é›†å¥–åŠ±ä¿¡å·")
            self.training_table.add_row("", "components", "å¤šç»´åº¦å¥–åŠ±")
        
    def _on_step(self) -> bool:
        # æ”¶é›†å¥–åŠ±å’Œåˆ†æ•°ç»Ÿè®¡
        for info in self.locals['infos']:
            if 'final_score' in info:
                final_score = info['final_score']
                score_delta = info.get('score_delta', 0)
                episode_length = info.get('episode_length', 0)
                
                self.episode_scores.append(final_score)
                self.training_stats['episodes_completed'] += 1
                
                if not RICH_AVAILABLE and self.verbose > 0:
                    print(f"ğŸ¯ Episode ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                          f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ­¥æ•°: {episode_length}")
            
            # æ‰‹åŠ¨æ”¶é›†episode rewardä¿¡æ¯
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_len = info['episode']['l']
                self.episode_rewards.append(episode_reward)
                
                # æ‰‹åŠ¨æ›´æ–°episodeç»Ÿè®¡
                if len(self.episode_rewards) > 0:
                    self.training_stats['ep_rew_mean'] = np.mean(self.episode_rewards)
                if len(self.episode_rewards) > 0:
                    # ä½¿ç”¨episodeé•¿åº¦ä¿¡æ¯
                    ep_lengths = [info['episode']['l'] for info in self.locals.get('infos', []) if 'episode' in info]
                    if ep_lengths:
                        self.training_stats['ep_len_mean'] = np.mean(ep_lengths)
                    else:
                        self.training_stats['ep_len_mean'] = episode_len
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        if RICH_AVAILABLE:
            current_time = time.time()
            self.training_stats['time_elapsed'] = current_time - self.start_time
            self.training_stats['total_timesteps'] = self.num_timesteps
            
            # ä»loggerè·å–è®­ç»ƒæŒ‡æ ‡
            if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
                logger_data = self.model.logger.name_to_value
                
                # Rollout metrics
                if 'rollout/ep_len_mean' in logger_data:
                    self.training_stats['ep_len_mean'] = logger_data['rollout/ep_len_mean']
                if 'rollout/ep_rew_mean' in logger_data:
                    self.training_stats['ep_rew_mean'] = logger_data['rollout/ep_rew_mean']
                
                # Time metrics
                if 'time/fps' in logger_data:
                    self.training_stats['fps'] = logger_data['time/fps']
                if 'time/iterations' in logger_data:
                    self.training_stats['iterations'] = logger_data['time/iterations']
                
                # Training metrics
                if 'train/approx_kl' in logger_data:
                    self.training_stats['approx_kl'] = logger_data['train/approx_kl']
                if 'train/clip_fraction' in logger_data:
                    self.training_stats['clip_fraction'] = logger_data['train/clip_fraction']
                if 'train/entropy_loss' in logger_data:
                    self.training_stats['entropy_loss'] = logger_data['train/entropy_loss']
                if 'train/learning_rate' in logger_data:
                    self.training_stats['learning_rate'] = logger_data['train/learning_rate']
                if 'train/loss' in logger_data:
                    self.training_stats['loss'] = logger_data['train/loss']
                if 'train/policy_gradient_loss' in logger_data:
                    self.training_stats['policy_gradient_loss'] = logger_data['train/policy_gradient_loss']
                if 'train/value_loss' in logger_data:
                    self.training_stats['value_loss'] = logger_data['train/value_loss']
            
            # æ›´æ–°å¹³å‡åˆ†æ•°
            if len(self.episode_scores) > 0:
                self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
        
        # å®šæœŸè¯„ä¼°å’Œä¿å­˜ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¹²æ‰°ç•Œé¢ï¼‰
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards)
                mean_score = np.mean(self.episode_scores) if len(self.episode_scores) > 0 else 0
                
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    if not RICH_AVAILABLE and self.verbose > 0:
                        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.save_freq == 0:
            model_path = f"checkpoints/model_{self.num_timesteps}_steps.zip"
            self.model.save(model_path)
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
        
        return True

def create_env(config=None):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorï¼‰"""
    default_config = {
        'max_episode_steps': 2000,  # æ¯å±€æœ€å¤§æ­¥æ•°
        'use_enhanced_reward': False,  # é»˜è®¤ä½¿ç”¨ç®€å•å¥–åŠ±
    }
    if config:
        default_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(default_config)
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

def create_enhanced_env(config=None):
    """åˆ›å»ºå¢å¼ºå¥–åŠ±è®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorï¼‰"""
    enhanced_config = {
        'max_episode_steps': 2000,
        'use_enhanced_reward': True,  # å¯ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ
        'enhanced_reward_weights': {
            'score_growth': 2.0,        # åˆ†æ•°å¢é•¿å¥–åŠ±æƒé‡
            'efficiency': 1.5,          # æ•ˆç‡å¥–åŠ±æƒé‡
            'exploration': 0.8,         # æ¢ç´¢å¥–åŠ±æƒé‡
            'strategic_split': 2.0,     # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±æƒé‡
            'food_density': 1.0,        # é£Ÿç‰©å¯†åº¦å¥–åŠ±æƒé‡
            'survival': 0.02,           # ç”Ÿå­˜å¥–åŠ±æƒé‡
            'time_penalty': -0.001,     # æ—¶é—´æƒ©ç½šæƒé‡
            'death_penalty': -20.0,     # æ­»äº¡æƒ©ç½šæƒé‡
        }
    }
    if config:
        enhanced_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(enhanced_config)
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

def train_with_stable_baselines3(algorithm='PPO', total_timesteps=100000, config=None, use_enhanced_reward=False):
    """ä½¿ç”¨stable-baselines3è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {algorithm} ç®—æ³•è®­ç»ƒ ({reward_type})...")
    
    # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨vectorized environmentä»¥æ­£ç¡®æ”¯æŒepisodeç»Ÿè®¡
    def make_env():
        if use_enhanced_reward:
            return create_enhanced_env(config)
        else:
            return create_env(config)
    
    if use_enhanced_reward:
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")
    else:
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ")
    
    # ä½¿ç”¨make_vec_envæ¥æ­£ç¡®é›†æˆMonitorå’Œepisodeç»Ÿè®¡
    env = make_vec_env(make_env, n_envs=1, vec_env_cls=DummyVecEnv)
    
    # åˆ›å»ºæ¨¡å‹
    if algorithm == 'PPO':
        model = PPO(
            "MlpPolicy", 
            env,
            learning_rate=3e-4,
            n_steps=512,  # é™ä½æ­¥æ•°ä»¥è·å¾—æ›´é¢‘ç¹çš„ç»Ÿè®¡æ›´æ–°
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=0 if RICH_AVAILABLE else 1,  # Richç•Œé¢æ—¶é™é»˜æ¨¡å¼
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'DQN':
        model = DQN(
            "MlpPolicy",
            env,
            learning_rate=1e-4,
            buffer_size=50000,
            learning_starts=1000,
            batch_size=32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            gradient_steps=1,
            target_update_interval=1000,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'A2C':
        model = A2C(
            "MlpPolicy",
            env,
            learning_rate=7e-4,
            n_steps=5,
            gamma=0.99,
            gae_lambda=1.0,
            ent_coef=0.01,
            vf_coef=0.25,
            max_grad_norm=0.5,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # åˆ›å»ºå›è°ƒ
    os.makedirs("checkpoints", exist_ok=True)
    
    if RICH_AVAILABLE:
        # åˆ›å»ºRichç•Œé¢ç‰ˆæœ¬çš„å›è°ƒ
        class RichTrainingCallback(TrainingCallback):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.use_enhanced_reward = use_enhanced_reward  # åœ¨çˆ¶ç±»åˆå§‹åŒ–åè®¾ç½®
                # print(f"ğŸ” å›è°ƒåˆå§‹åŒ–è°ƒè¯•: use_enhanced_reward = {self.use_enhanced_reward}")
                
                # åˆå§‹åŒ–è¿›åº¦æ¡ä»»åŠ¡
                if hasattr(self, 'progress'):
                    reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                    self.progress_task = self.progress.add_task(
                        f"[green]ğŸš€ {algorithm} è®­ç»ƒ ({reward_type})", 
                        total=total_timesteps
                    )
                
            def _on_step(self):
                # æ›´æ–°è®­ç»ƒç»Ÿè®¡
                current_time = time.time()
                self.training_stats['time_elapsed'] = current_time - self.start_time
                self.training_stats['total_timesteps'] = self.num_timesteps
                
                # ä»æ¨¡å‹è·å–è®­ç»ƒæŒ‡æ ‡
                if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                    logger_data = self.model.logger.name_to_value
                    
                    # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰loggeræ•°æ® (å¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨è°ƒè¯•)
                    # if self.num_timesteps % 1000 == 0:  # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                    #     print(f"\nğŸ” Loggerå®Œæ•´æ•°æ®è°ƒè¯• (æ­¥æ•°: {self.num_timesteps}):")
                    #     if logger_data:
                    #         for key, value in logger_data.items():
                    #             print(f"  {key}: {value}")
                    #     else:
                    #         print("  Loggeræ•°æ®ä¸ºç©º!")
                    #     
                    #     print(f"ğŸ” Training Statsè°ƒè¯•:")
                    #     for key, value in self.training_stats.items():
                    #         if value != 0:  # åªæ˜¾ç¤ºéé›¶å€¼
                    #             print(f"  {key}: {value}")
                    #     print()
                    
                    # æ›´æ–°å„é¡¹æŒ‡æ ‡
                    for key, value in logger_data.items():
                        if key in ['rollout/ep_len_mean', 'rollout/ep_rew_mean', 'time/fps', 
                                  'time/iterations', 'train/approx_kl', 'train/clip_fraction',
                                  'train/entropy_loss', 'train/learning_rate', 'train/loss',
                                  'train/policy_gradient_loss', 'train/value_loss']:
                            stat_name = key.split('/')[-1]
                            if stat_name in self.training_stats:
                                self.training_stats[stat_name] = value
                
                # æ›´æ–°å¹³å‡åˆ†æ•°
                if len(self.episode_scores) > 0:
                    self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
                
                # å®šæœŸæ›´æ–°æ˜¾ç¤º
                if current_time - self.last_table_update > self.table_update_interval:
                    self._update_training_table()
                    self.last_table_update = current_time
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if hasattr(self, 'progress') and self.progress_task is not None:
                        self.progress.update(self.progress_task, completed=self.num_timesteps)
                    
                    # åˆ›å»ºå¸ƒå±€ç»„ä»¶
                    reward_panel, tensorboard_panel, status_panel = self._create_training_layout()
                    
                    # æ¸…å±å¹¶æ˜¾ç¤º
                    self.console.clear()
                    
                    # æ˜¾ç¤ºä¸Šæ’é¢æ¿
                    top_row = Columns([reward_panel, tensorboard_panel], equal=True)
                    self.console.print(top_row)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡è¡¨æ ¼
                    stats_panel = Panel(
                        self.training_table,
                        title="è®­ç»ƒç»Ÿè®¡",
                        border_style="blue"
                    )
                    self.console.print(stats_panel)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                    self.console.print(status_panel)
                
                return super()._on_step()
            
            def _create_training_layout(self):
                """åˆ›å»ºè®­ç»ƒç•Œé¢å¸ƒå±€"""
                # å¥–åŠ±ç³»ç»Ÿä¿¡æ¯é¢æ¿
                reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                reward_icon = "ğŸ¯" if self.use_enhanced_reward else "ğŸ“Š"
                reward_desc = "å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå¤šç»´åº¦åé¦ˆ" if self.use_enhanced_reward else "ç»å…¸å¼ºåŒ–å­¦ä¹ å¥–åŠ±"
                
                reward_panel = Panel(
                    f"[bold green]{reward_icon} {reward_type}ç³»ç»Ÿ[/bold green]\n[dim]{reward_desc}[/dim]",
                    title="å¥–åŠ±ç³»ç»Ÿ",
                    border_style="green"
                )
                
                # TensorBoardæç¤º
                tensorboard_panel = Panel(
                    "[bold cyan]ğŸ“Š TensorBoardç›‘æ§[/bold cyan]\n" +
                    "[dim]åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/dim]\n" +
                    "[yellow]tensorboard --logdir ./tensorboard_logs[/yellow]\n" +
                    "[dim]ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:6006[/dim]",
                    title="ç›‘æ§å·¥å…·",
                    border_style="cyan"
                )
                
                # çŠ¶æ€ä¿¡æ¯
                progress_percent = (self.num_timesteps / self.total_timesteps) * 100
                status_lines = []
                status_lines.append(f"[bold blue]è®­ç»ƒè¿›åº¦[/bold blue]: {progress_percent:.1f}% ({self.num_timesteps:,}/{self.total_timesteps:,} steps)")
                
                if len(self.episode_scores) > 0:
                    latest_score = self.episode_scores[-1]
                    avg_score = np.mean(self.episode_scores)
                    status_lines.append(f"[bold yellow]Episodeä¿¡æ¯[/bold yellow]: æœ€æ–°åˆ†æ•°={latest_score:.0f}, å¹³å‡åˆ†æ•°={avg_score:.0f}")
                
                if self.use_enhanced_reward:
                    status_lines.append("[dim]ğŸ’¡ å¢å¼ºå¥–åŠ±æä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·[/dim]")
                
                status_panel = Panel(
                    "\n".join(status_lines),
                    title="çŠ¶æ€ä¿¡æ¯",
                    border_style="magenta"
                )
                
                # åˆ›å»ºå®Œæ•´å¸ƒå±€å¹¶è¿”å›æ‰€æœ‰ç»„ä»¶
                return reward_panel, tensorboard_panel, status_panel
        
        callback = RichTrainingCallback(eval_freq=2000, save_freq=10000, total_timesteps=total_timesteps)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸ“ˆ å¼€å§‹ä½¿ç”¨ {algorithm} è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
        print(f"ğŸ¯ å¥–åŠ±ç³»ç»Ÿ: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}")
        print("ï¿½ TensorBoardæ—¥å¿—: ./tensorboard_logs/")
        print("ï¿½ğŸ’¡ è®­ç»ƒç•Œé¢å°†æ¯5ç§’æ›´æ–°ä¸€æ¬¡...")
        print("\nğŸš€ å¯åŠ¨ TensorBoard ç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   ç„¶åè®¿é—®: http://localhost:6006")
        print("\n" + "="*60)
        start_time = time.time()
        
        # å¯åŠ¨è¿›åº¦æ¡
        with callback.progress:
            model.learn(
                total_timesteps=total_timesteps,
                callback=callback,
                tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
            )
        
        train_time = time.time() - start_time
        
        # è®­ç»ƒå®Œæˆä¿¡æ¯
        callback.console.clear()
        completion_panel = Panel(
            f"[bold green]âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼[/bold green]\n\n" +
            f"[yellow]è®­ç»ƒæ—¶é—´[/yellow]: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)\n" +
            f"[yellow]æ€»æ­¥æ•°[/yellow]: {total_timesteps:,}\n" +
            f"[yellow]å¥–åŠ±ç³»ç»Ÿ[/yellow]: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}\n" +
            f"[yellow]ç®—æ³•[/yellow]: {algorithm}\n\n" +
            f"[cyan]ğŸ“Š æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/cyan]\n" +
            f"[dim]tensorboard --logdir ./tensorboard_logs[/dim]\n\n" +
            f"[cyan]ğŸ“ æ¨¡å‹æ–‡ä»¶:[/cyan]\n" +
            f"[dim]./models/ å’Œ ./checkpoints/[/dim]",
            title="ğŸ‰ è®­ç»ƒå®Œæˆ",
            border_style="green"
        )
        callback.console.print(completion_panel)
        
    else:
        # ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢è®­ç»ƒ
        callback = TrainingCallback(eval_freq=2000, save_freq=10000, total_timesteps=total_timesteps)
        callback.use_enhanced_reward = use_enhanced_reward
        
        reward_info = "å¢å¼ºå¥–åŠ±ç³»ç»Ÿ" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ"
        print(f"ğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps} ({reward_info})")
        print("ğŸ“Š TensorBoardç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   è®¿é—®: http://localhost:6006")
        print("="*50)
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
        )
        
        train_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")
        print("ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    reward_suffix = "_enhanced" if use_enhanced_reward else "_standard"
    final_model_path = f"models/{algorithm}_gobigger{reward_suffix}_final.zip"
    os.makedirs("models", exist_ok=True)
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def simple_random_training(episodes=100, use_enhanced_reward=False):
    """ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤ºï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸ® è¿è¡Œéšæœºç­–ç•¥æ¼”ç¤ºè®­ç»ƒ ({reward_type})...")
    
    if use_enhanced_reward:
        env = create_enhanced_env({'max_episode_steps': 500})
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    else:
        env = create_env({'max_episode_steps': 500})
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # éšæœºåŠ¨ä½œ
            action = env.action_space.sample()
            action[2] = int(action[2])  # åŠ¨ä½œç±»å‹ä¸ºæ•´æ•°
            
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    if (episode + 1) % 10 == 0:  # æ¯10ä¸ªepisodeæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                        print(f"  Episode {episode + 1} ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                              f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ€»å¥–åŠ±: {total_reward:.3f}")
                        
                        # å¦‚æœä½¿ç”¨å¢å¼ºå¥–åŠ±ï¼Œæ˜¾ç¤ºå¥–åŠ±ç»„ä»¶ä¿¡æ¯
                        if use_enhanced_reward and hasattr(env, 'reward_components_history') and env.reward_components_history:
                            latest_components = env.reward_components_history[-1]['components']
                            if latest_components:
                                print(f"    å¥–åŠ±ç»„ä»¶: ", end="")
                                for comp, val in latest_components.items():
                                    if abs(val) > 0.001:
                                        print(f"{comp}={val:.3f} ", end="")
                                print()
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards)
    title = f'Episode Rewards ({reward_type})'
    plt.title(title)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(episode_lengths)
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    
    plt.tight_layout()
    filename = f'random_training_results_{reward_type.lower().replace(" ", "_")}.png'
    plt.savefig(filename)
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filename}")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-20:]):.3f}")
    
    if use_enhanced_reward:
        print("ğŸ’¡ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›äº†æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")

def evaluate_model(model_path, episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½å’Œè¯„ä¼°æ¨¡å‹")
        return
    
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    if 'PPO' in model_path:
        model = PPO.load(model_path)
    elif 'DQN' in model_path:
        model = DQN.load(model_path)
    elif 'A2C' in model_path:
        model = A2C.load(model_path)
    else:
        print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env({'max_episode_steps': 1000})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}, "
                          f"æœ€ç»ˆåˆ†æ•°={final_score:.2f}, åˆ†æ•°å˜åŒ–={score_delta:+.2f}")
                else:
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}")
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¤– GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ (æ”¯æŒå¢å¼ºå¥–åŠ±)")
    print("=" * 60)
    
    if not RICH_AVAILABLE:
        print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")
        print("   å½“å‰ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢")
        print()
    else:
        console = Console()
        welcome_panel = Panel(
            "[bold green]ğŸ‰ æ¬¢è¿ä½¿ç”¨ GoBigger RL è®­ç»ƒå™¨[/bold green]\n\n" +
            "[yellow]âœ¨ Richç•Œé¢æ”¯æŒ[/yellow]: ç¾åŒ–ç•Œé¢ã€è¿›åº¦æ¡ã€å®æ—¶ç»Ÿè®¡\n" +
            "[yellow]ğŸ“Š TensorBoardé›†æˆ[/yellow]: è¯¦ç»†è®­ç»ƒæ›²çº¿ç›‘æ§\n" +
            "[yellow]ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿ[/yellow]: å¤šç»´åº¦å¯†é›†å¥–åŠ±ä¿¡å·\n" +
            "[yellow]ğŸš€ å¤šç®—æ³•æ”¯æŒ[/yellow]: PPOã€DQNã€A2C",
            title="åŠŸèƒ½ç‰¹è‰²",
            border_style="green"
        )
        console.print(welcome_panel)
        print()
    
    # è®­ç»ƒé…ç½®
    config = {
        'max_episode_steps': 1500,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    
    if STABLE_BASELINES_AVAILABLE:
        print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
        print("1. PPO + æ ‡å‡†å¥–åŠ± - ç»å…¸å¼ºåŒ–å­¦ä¹ ")
        print("2. PPO + å¢å¼ºå¥–åŠ± - å¯†é›†å¥–åŠ±ä¿¡å· (æ¨è)")
        print("3. DQN + æ ‡å‡†å¥–åŠ±")
        print("4. DQN + å¢å¼ºå¥–åŠ±")
        print("5. A2C + æ ‡å‡†å¥–åŠ±")
        print("6. A2C + å¢å¼ºå¥–åŠ±")
        print("7. è¯„ä¼°ç°æœ‰æ¨¡å‹")
        print("8. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("9. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        
        choice = input("\nè¯·é€‰æ‹© (1-9): ").strip()
        
        if choice == '1':
            model = train_with_stable_baselines3('PPO', total_timesteps=200000, config=config, use_enhanced_reward=False)
        elif choice == '2':
            model = train_with_stable_baselines3('PPO', total_timesteps=200000, config=config, use_enhanced_reward=True)
        elif choice == '3':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '4':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '5':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '6':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '7':
            model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
            if os.path.exists(model_path):
                evaluate_model(model_path)
            else:
                print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        elif choice == '8':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '9':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    else:
        print("ğŸ¯ é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("2. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        
        choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
        
        if choice == '1':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '2':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼š")
    print("  - ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    print("  - ğŸŒ TensorBoardè®¿é—®: http://localhost:6006")
    print("  - ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: ./models/")
    print("  - ğŸ’¾ æ£€æŸ¥ç‚¹ä½ç½®: ./checkpoints/")
    print("  - ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œæ¨èç”¨äºæ–°è®­ç»ƒ")
    if not RICH_AVAILABLE:
        print("  - âœ¨ å»ºè®®å®‰è£… rich åº“äº«å—æ›´å¥½çš„è®­ç»ƒç•Œé¢ä½“éªŒ: pip install rich")

if __name__ == "__main__":
    main()
