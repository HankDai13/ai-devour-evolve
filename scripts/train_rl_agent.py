#!/usr/bin/env python3
"""
ä½¿ç”¨GoBiggeræ ¸å¿ƒå¼•æ“è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
ğŸ”¥ ä¼˜åŒ–ç‰ˆï¼šè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜ï¼Œæ™ºèƒ½åˆ†è£‚ç­–ç•¥ï¼Œæé«˜æ¢ç´¢èƒ½åŠ›

æ ¸å¿ƒä¼˜åŒ–ï¼š
1. æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆNovelty Bonusï¼‰- é¼“åŠ±å¤šæ–¹å‘æ¢ç´¢
2. æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ± - å‡å°‘ç›²ç›®åˆ†è£‚ï¼Œé¼“åŠ±é€šè¿‡åˆ†è£‚è·å¾—æ›´å¤šé£Ÿç‰©  
3. ä¼˜åŒ–è®­ç»ƒè¶…å‚æ•° - æé«˜ç†µç³»æ•°ï¼Œå¢å¼ºæ¢ç´¢æ€§

æ”¯æŒå¤šç§RLç®—æ³•ï¼šPPOã€DQNã€A2Cç­‰
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import json
import math
from collections import deque, defaultdict
import matplotlib.pyplot as plt
from typing import Dict, Tuple, Set, Optional, List, Any
from dataclasses import dataclass

# å°è¯•å¯¼å…¥richåº“ç”¨äºç¾åŒ–ç•Œé¢
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.layout import Layout
    from rich.panel import Panel
    from rich.live import Live
    from rich.text import Text
    from rich.align import Align
    from rich.columns import Columns
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºåŸç”ŸPythonç±»å‹ï¼Œè§£å†³JSONåºåˆ—åŒ–é—®é¢˜"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

# è·¯å¾„è®¾ç½®ï¼šå®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from gobigger_gym_env import GoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.monitor import Monitor
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

@dataclass
class ExplorationState:
    """æ¢ç´¢çŠ¶æ€è¿½è¸ª"""
    visited_cells: Set[Tuple[int, int]]
    cell_visit_count: Dict[Tuple[int, int], int]
    cell_size: int = 100  # æ¯ä¸ªæ ¼å­çš„åƒç´ å¤§å°
    max_visits_bonus: int = 3  # æœ€å¤§è®¿é—®æ¬¡æ•°å¥–åŠ±

class OptimizedRewardCalculator:
    """
    ğŸ”¥ ä¼˜åŒ–çš„å¥–åŠ±è®¡ç®—å™¨ - è§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
    
    å®ç°ä¸‰å¤§æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆNovelty Bonusï¼‰
    2. æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ±
    3. ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        
        # === ğŸ”¥ æ ¸å¿ƒå¥–åŠ±æƒé‡é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰===
        self.weights = {
            # åŸºç¡€å¥–åŠ±
            'score_growth': 1.0,           # åˆ†æ•°å¢é•¿åŸºç¡€å¥–åŠ±
            'survival': 0.01,              # ç”Ÿå­˜å¥–åŠ±
            'death_penalty': -50.0,        # æ­»äº¡é‡æƒ©ç½š
            
            # === ğŸ”¥ æ¢ç´¢å¥–åŠ±ç³»ç»Ÿï¼ˆè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"æ ¸å¿ƒï¼‰===
            'exploration_bonus': 2.0,      # ğŸ”¥ æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆå…³é”®ï¼ï¼‰
            'diversity_bonus': 1.0,        # ğŸ”¥ ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±
            'backtrack_penalty': -0.5,     # ğŸ”¥ åå¤å›åˆ°åŒä¸€åŒºåŸŸæƒ©ç½š
            
            # === ğŸ”¥ æ™ºèƒ½åˆ†è£‚å¥–åŠ±ç³»ç»Ÿ ===
            'smart_split_bonus': 3.0,      # ğŸ”¥ æ™ºèƒ½åˆ†è£‚å¥–åŠ±
            'waste_split_penalty': -2.0,   # ğŸ”¥ æµªè´¹åˆ†è£‚æƒ©ç½š
            'split_efficiency': 2.0,       # ğŸ”¥ åˆ†è£‚åé£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±
            
            # === å…¶ä»–ç­–ç•¥å¥–åŠ± ===
            'food_efficiency': 1.5,        # é£Ÿç‰©è·å–æ•ˆç‡
            'size_management': 1.0,         # å¤§å°ç®¡ç†å¥–åŠ±
        }
        
        # === æ¢ç´¢çŠ¶æ€è¿½è¸ª ===
        self.exploration = ExplorationState(
            visited_cells=set(),
            cell_visit_count=defaultdict(int),
            cell_size=self.config.get('exploration_cell_size', 100)
        )
        
        # === ç§»åŠ¨æ–¹å‘è¿½è¸ªï¼ˆè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"ï¼‰ ===
        self.movement_history = deque(maxlen=20)  # è®°å½•æœ€è¿‘20æ­¥çš„ç§»åŠ¨æ–¹å‘
        self.direction_diversity_window = self.config.get('direction_diversity_window', 15)  # æ–¹å‘å¤šæ ·æ€§è®¡ç®—çª—å£
        
        # === åˆ†è£‚ç­–ç•¥è¿½è¸ª ===
        self.split_history = []
        self.last_split_step = -1
        self.split_efficiency_window = self.config.get('split_efficiency_window', 60)  # åˆ†è£‚æ•ˆç‡è¯„ä¼°çª—å£
        
        # === å†å²çŠ¶æ€ç¼“å­˜ ===
        self.history = deque(maxlen=100)
        self.step_count = 0
        
        # === ç»Ÿè®¡ä¿¡æ¯ ===
        self.stats = {
            'total_exploration_bonus': 0.0,
            'total_smart_split_bonus': 0.0,
            'unique_cells_visited': 0,
            'direction_changes': 0,
            'efficient_splits': 0,
            'wasted_splits': 0,
        }
    
    def calculate_optimized_reward(self, current_state, previous_state, action, base_reward: float) -> Tuple[float, Dict[str, float]]:
        """
        ğŸ”¥ è®¡ç®—ä¼˜åŒ–çš„å¥–åŠ± - åœ¨åŸºç¡€å¥–åŠ±åŸºç¡€ä¸Šå¢åŠ æ¢ç´¢å’Œç­–ç•¥å¥–åŠ±
        
        Args:
            current_state: å½“å‰æ¸¸æˆçŠ¶æ€
            previous_state: å‰ä¸€æ¸¸æˆçŠ¶æ€  
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            base_reward: åŸºç¡€å¥–åŠ±
            
        Returns:
            (ä¼˜åŒ–åæ€»å¥–åŠ±, å¥–åŠ±åˆ†è§£å­—å…¸)
        """
        self.step_count += 1
        reward_components = {'base_reward': base_reward}
        
        # 1. ğŸ”¥ æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±ï¼ˆè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"çš„æ ¸å¿ƒï¼‰
        reward_components.update(self._calculate_exploration_rewards(current_state))
        
        # 2. ğŸ”¥ ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±
        reward_components.update(self._calculate_movement_diversity_rewards(action))
        
        # 3. ğŸ”¥ æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ±
        reward_components.update(self._calculate_smart_split_rewards(current_state, previous_state, action))
        
        # 4. é£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±
        reward_components.update(self._calculate_food_efficiency_rewards(current_state, previous_state))
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = base_reward + sum(
            self.weights.get(component, 1.0) * value 
            for component, value in reward_components.items()
            if component != 'base_reward'
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(reward_components)
        
        return total_reward, reward_components
    
    def _calculate_exploration_rewards(self, current_state) -> Dict[str, float]:
        """
        ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼šè®¡ç®—æ¢ç´¢å¥–åŠ±ï¼Œè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
        
        å®ç°æ€è·¯ï¼š
        1. å°†åœ°å›¾åˆ’åˆ†ä¸ºç½‘æ ¼
        2. è¿½è¸ªè®¿é—®è¿‡çš„æ ¼å­
        3. ç»™è®¿é—®æ–°æ ¼å­å¤§é¢å¥–åŠ±
        4. å¯¹é‡å¤è®¿é—®ç»™äºˆé€’å‡å¥–åŠ±æˆ–æƒ©ç½š
        """
        rewards = {}
        
        # è·å–å½“å‰ä½ç½®
        if hasattr(current_state, 'rectangle') and len(current_state.rectangle) >= 4:
            center_x = (current_state.rectangle[0] + current_state.rectangle[2]) / 2
            center_y = (current_state.rectangle[1] + current_state.rectangle[3]) / 2
        elif hasattr(current_state, 'position'):
            center_x, center_y = current_state.position[0], current_state.position[1]
        else:
            # æ— æ³•è·å–ä½ç½®ï¼Œè·³è¿‡æ¢ç´¢å¥–åŠ±
            rewards['exploration_bonus'] = 0.0
            rewards['backtrack_penalty'] = 0.0
            return rewards
        
        # è®¡ç®—å½“å‰æ ¼å­åæ ‡
        cell_x = int(center_x // self.exploration.cell_size)
        cell_y = int(center_y // self.exploration.cell_size)
        cell_coord = (cell_x, cell_y)
        
        # æ›´æ–°è®¿é—®è®¡æ•°
        self.exploration.cell_visit_count[cell_coord] += 1
        visit_count = self.exploration.cell_visit_count[cell_coord]
        
        # æ–°åŒºåŸŸæ¢ç´¢å¥–åŠ±
        if cell_coord not in self.exploration.visited_cells:
            # ğŸ”¥ é¦–æ¬¡è®¿é—®æ–°åŒºåŸŸï¼šå¤§é¢å¥–åŠ±
            rewards['exploration_bonus'] = 1.0  # åŸºç¡€æ¢ç´¢å¥–åŠ±
            self.exploration.visited_cells.add(cell_coord)
            self.stats['unique_cells_visited'] += 1
            
            # é¢å¤–å¥–åŠ±ï¼šå¦‚æœè¿™ä¸ªæ–°åŒºåŸŸè·ç¦»ä¹‹å‰è®¿é—®çš„åŒºåŸŸè¾ƒè¿œ
            if len(self.exploration.visited_cells) > 1:
                min_distance = min(
                    abs(cell_x - other_x) + abs(cell_y - other_y)
                    for other_x, other_y in self.exploration.visited_cells
                    if (other_x, other_y) != cell_coord
                )
                if min_distance >= 3:  # è·ç¦»è¶³å¤Ÿè¿œ
                    rewards['exploration_bonus'] += 0.5  # è¿œè·ç¦»æ¢ç´¢é¢å¤–å¥–åŠ±
        else:
            # å·²è®¿é—®åŒºåŸŸï¼šæ ¹æ®è®¿é—®æ¬¡æ•°ç»™äºˆé€’å‡å¥–åŠ±æˆ–æƒ©ç½š
            if visit_count <= self.exploration.max_visits_bonus:
                # å°‘é‡è®¿é—®ä»æœ‰å°å¥–åŠ±
                rewards['exploration_bonus'] = 0.1 / visit_count
            else:
                # è¿‡åº¦è®¿é—®ç»™äºˆæƒ©ç½š
                rewards['backtrack_penalty'] = min(visit_count - self.exploration.max_visits_bonus, 5) * 0.1
                rewards['exploration_bonus'] = 0.0
        
        if 'backtrack_penalty' not in rewards:
            rewards['backtrack_penalty'] = 0.0
        
        return rewards
    
    def _calculate_movement_diversity_rewards(self, action) -> Dict[str, float]:
        """
        ğŸ”¥ è®¡ç®—ç§»åŠ¨æ–¹å‘å¤šæ ·æ€§å¥–åŠ±ï¼Œè¿›ä¸€æ­¥è§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"é—®é¢˜
        """
        rewards = {'diversity_bonus': 0.0}
        
        # è®°å½•å½“å‰ç§»åŠ¨æ–¹å‘
        if len(action) >= 2:
            current_direction = np.array([action[0], action[1]])
            direction_magnitude = np.linalg.norm(current_direction)
            
            if direction_magnitude > 0.1:  # æœ‰æ˜æ˜¾ç§»åŠ¨
                normalized_direction = current_direction / direction_magnitude
                self.movement_history.append(normalized_direction)
                
                # è®¡ç®—æ–¹å‘å¤šæ ·æ€§ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„å†å²è®°å½•ï¼‰
                if len(self.movement_history) >= self.direction_diversity_window:
                    recent_directions = list(self.movement_history)[-self.direction_diversity_window:]
                    
                    # è®¡ç®—æ–¹å‘å˜åŒ–çš„æ–¹å·®ä½œä¸ºå¤šæ ·æ€§æŒ‡æ ‡
                    direction_changes = []
                    for i in range(1, len(recent_directions)):
                        # è®¡ç®—è¿ç»­æ–¹å‘é—´çš„è§’åº¦å˜åŒ–
                        dot_product = np.clip(np.dot(recent_directions[i-1], recent_directions[i]), -1, 1)
                        angle_change = np.arccos(dot_product)
                        direction_changes.append(angle_change)
                    
                    if direction_changes:
                        # æ–¹å‘å˜åŒ–è¶Šå¤§ï¼Œå¤šæ ·æ€§å¥–åŠ±è¶Šé«˜
                        avg_angle_change = np.mean(direction_changes)
                        diversity_score = avg_angle_change / np.pi  # æ ‡å‡†åŒ–åˆ°[0,1]
                        
                        if diversity_score > 0.3:  # è¶³å¤Ÿçš„æ–¹å‘å˜åŒ–
                            rewards['diversity_bonus'] = diversity_score * 0.5
                            self.stats['direction_changes'] += 1
        
        return rewards
    
    def _calculate_smart_split_rewards(self, current_state, previous_state, action) -> Dict[str, float]:
        """
        ğŸ”¥ è®¡ç®—æ™ºèƒ½åˆ†è£‚ç­–ç•¥å¥–åŠ±
        
        å‡å°‘ç›²ç›®åˆ†è£‚ï¼Œé¼“åŠ±åœ¨æœ‰è¶³å¤Ÿé£Ÿç‰©æ—¶åˆ†è£‚
        """
        rewards = {
            'smart_split_bonus': 0.0,
            'waste_split_penalty': 0.0,
            'split_efficiency': 0.0
        }
        
        # æ£€æµ‹æ˜¯å¦è¿›è¡Œäº†åˆ†è£‚ï¼ˆå‡è®¾ç¬¬ä¸‰ä¸ªåŠ¨ä½œæ˜¯åˆ†è£‚ä¿¡å·ï¼‰
        split_action = False
        if len(action) >= 3:
            split_action = action[2] > 1.5
        
        if split_action:
            current_cells = 1  # é»˜è®¤å€¼
            food_nearby = 0
            current_score = 0
            
            # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
            if hasattr(current_state, 'clone') and isinstance(current_state.clone, list):
                current_cells = len(current_state.clone)
            if hasattr(current_state, 'food') and isinstance(current_state.food, list):
                food_nearby = len(current_state.food)
            if hasattr(current_state, 'score'):
                current_score = current_state.score
            
            # è¯„ä¼°åˆ†è£‚æ™ºèƒ½ç¨‹åº¦
            steps_since_last_split = self.step_count - self.last_split_step
            
            # ğŸ”¥ æ™ºèƒ½åˆ†è£‚æ¡ä»¶ï¼š
            # 1. é™„è¿‘æœ‰è¶³å¤Ÿé£Ÿç‰©ï¼ˆè‡³å°‘3ä¸ªï¼‰
            # 2. è·ç¦»ä¸Šæ¬¡åˆ†è£‚è¶³å¤Ÿä¹…ï¼ˆè‡³å°‘30æ­¥ï¼‰
            # 3. å½“å‰åˆ†æ•°è¶³å¤Ÿé«˜ï¼ˆèƒ½å¤Ÿæ‰¿å—åˆ†è£‚ï¼‰
            # 4. å½“å‰ç»†èƒæ•°ä¸è¿‡å¤šï¼ˆé¿å…è¿‡åº¦åˆ†è£‚ï¼‰
            
            is_smart_split = (
                food_nearby >= 3 and  # æ¡ä»¶1ï¼šé™„è¿‘æœ‰è¶³å¤Ÿé£Ÿç‰©
                steps_since_last_split >= 30 and  # æ¡ä»¶2ï¼šåˆ†è£‚é—´éš”è¶³å¤Ÿ
                current_score >= 2000 and  # æ¡ä»¶3ï¼šåˆ†æ•°è¶³å¤Ÿ
                current_cells <= 8  # æ¡ä»¶4ï¼šé¿å…è¿‡åº¦åˆ†è£‚
            )
            
            if is_smart_split:
                # ğŸ”¥ æ™ºèƒ½åˆ†è£‚å¥–åŠ±
                food_density_bonus = min(food_nearby / 10.0, 1.0)  # é£Ÿç‰©å¯†åº¦å¥–åŠ±
                timing_bonus = min(steps_since_last_split / 100.0, 0.5)  # æ—¶æœºå¥–åŠ±
                rewards['smart_split_bonus'] = 1.0 + food_density_bonus + timing_bonus
                
                self.stats['efficient_splits'] += 1
            else:
                # ğŸ”¥ æµªè´¹åˆ†è£‚æƒ©ç½š
                penalty_factors = []
                if food_nearby < 3:
                    penalty_factors.append(0.5)  # é£Ÿç‰©ä¸è¶³æƒ©ç½š
                if steps_since_last_split < 30:
                    penalty_factors.append(0.3)  # åˆ†è£‚è¿‡é¢‘æƒ©ç½š
                if current_cells > 8:
                    penalty_factors.append(0.4)  # è¿‡åº¦åˆ†è£‚æƒ©ç½š
                
                rewards['waste_split_penalty'] = sum(penalty_factors)
                self.stats['wasted_splits'] += 1
            
            # è®°å½•åˆ†è£‚äº‹ä»¶
            self.last_split_step = self.step_count
            self.split_history.append({
                'step': self.step_count,
                'food_nearby': food_nearby,
                'cells_before': current_cells,
                'smart_split': is_smart_split
            })
        
        # åˆ†è£‚åæ•ˆç‡å¥–åŠ±ï¼šè¯„ä¼°åˆ†è£‚åæ˜¯å¦è·å¾—äº†æ›´å¤šé£Ÿç‰©
        if previous_state and hasattr(current_state, 'score') and hasattr(previous_state, 'score'):
            score_growth = current_state.score - previous_state.score
            
            # å¦‚æœæœ€è¿‘è¿›è¡Œäº†åˆ†è£‚ä¸”åˆ†æ•°æœ‰æ˜¾è‘—å¢é•¿
            recent_splits = [s for s in self.split_history if self.step_count - s['step'] <= self.split_efficiency_window]
            if recent_splits and score_growth > 0:
                # æ ¹æ®åˆ†è£‚åçš„åˆ†æ•°å¢é•¿ç»™äºˆæ•ˆç‡å¥–åŠ±
                efficiency_ratio = score_growth / len(recent_splits)
                rewards['split_efficiency'] = min(efficiency_ratio / 500.0, 1.0)
        
        return rewards
    
    def _calculate_food_efficiency_rewards(self, current_state, previous_state) -> Dict[str, float]:
        """è®¡ç®—é£Ÿç‰©è·å–æ•ˆç‡å¥–åŠ±"""
        rewards = {'food_efficiency': 0.0}
        
        if previous_state is None:
            return rewards
        
        # è®¡ç®—åˆ†æ•°å¢é•¿ï¼ˆä¸»è¦æ¥è‡ªé£Ÿç‰©ï¼‰
        score_growth = getattr(current_state, 'score', 0) - getattr(previous_state, 'score', 0)
        
        if score_growth > 0:
            # åŸºäºåˆ†æ•°å¢é•¿çš„æ•ˆç‡å¥–åŠ±
            efficiency = min(score_growth / 100.0, 2.0)  # é™åˆ¶æœ€å¤§å¥–åŠ±
            rewards['food_efficiency'] = efficiency
        
        return rewards
    
    def _update_stats(self, reward_components):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if 'exploration_bonus' in reward_components and reward_components['exploration_bonus'] > 0:
            self.stats['total_exploration_bonus'] += reward_components['exploration_bonus']
        
        if 'smart_split_bonus' in reward_components and reward_components['smart_split_bonus'] > 0:
            self.stats['total_smart_split_bonus'] += reward_components['smart_split_bonus']
    
    def get_exploration_stats(self) -> Dict[str, Any]:
        """è·å–æ¢ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'unique_cells_visited': len(self.exploration.visited_cells),
            'total_cell_visits': sum(self.exploration.cell_visit_count.values()),
            'exploration_coverage': len(self.exploration.visited_cells),
            'avg_visits_per_cell': (
                sum(self.exploration.cell_visit_count.values()) / len(self.exploration.visited_cells)
                if self.exploration.visited_cells else 0
            ),
            'direction_diversity_score': self.stats['direction_changes'] / max(self.step_count, 1),
        }
    
    def get_split_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†è£‚ç­–ç•¥ç»Ÿè®¡ä¿¡æ¯"""
        total_splits = len(self.split_history)
        return {
            'total_splits': total_splits,
            'efficient_splits': self.stats['efficient_splits'],
            'wasted_splits': self.stats['wasted_splits'],
            'split_efficiency_ratio': (
                self.stats['efficient_splits'] / total_splits
                if total_splits > 0 else 0
            ),
            'avg_split_interval': (
                self.step_count / total_splits if total_splits > 0 else float('inf')
            ),
        }
    
    def reset(self):
        """é‡ç½®å¥–åŠ±è®¡ç®—å™¨ï¼ˆæ–°episodeæ—¶è°ƒç”¨ï¼‰"""
        self.exploration.visited_cells.clear()
        self.exploration.cell_visit_count.clear()
        self.movement_history.clear()
        self.split_history.clear()
        self.history.clear()
        self.step_count = 0
        self.last_split_step = -1
        
        # é‡ç½®ç»Ÿè®¡
        for key in self.stats:
            self.stats[key] = 0
        self.stats['unique_cells_visited'] = 0
        self.stats['direction_changes'] = 0
        self.stats['efficient_splits'] = 0
        self.stats['wasted_splits'] = 0

# å…¨å±€å¥–åŠ±è®¡ç®—å™¨å®ä¾‹
_global_reward_calculator = OptimizedRewardCalculator()

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±æ˜¾ç¤ºå’Œé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–ï¼‰"""
    
    def __init__(self, eval_freq=1000, save_freq=5000, verbose=1, total_timesteps=50000, 
                 auto_save_freq=10000, checkpoint_freq=25000, enable_lr_decay=False):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.auto_save_freq = auto_save_freq  # è‡ªåŠ¨ä¿å­˜é¢‘ç‡
        self.checkpoint_freq = checkpoint_freq  # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
        self.enable_lr_decay = enable_lr_decay  # å­¦ä¹ ç‡è¡°å‡
        self.best_mean_reward = -np.inf
        self.episode_rewards = deque(maxlen=100)
        self.episode_scores = deque(maxlen=100)
        self.total_timesteps = total_timesteps
        self.use_enhanced_reward = False  # é»˜è®¤å€¼ï¼Œä¼šåœ¨å¤–éƒ¨è®¾ç½®
        
        # é•¿æ—¶é—´è®­ç»ƒç›¸å…³
        self.initial_lr = None  # åˆå§‹å­¦ä¹ ç‡ï¼Œåœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®
        self.last_auto_save = 0
        self.last_checkpoint = 0
        
        # Richç•Œé¢ç»„ä»¶
        if RICH_AVAILABLE:
            self.console = Console()
            self.training_table = Table(title="ğŸ¤– GoBigger RL Training Status")
            self.training_table.add_column("Metric", style="cyan", no_wrap=True)
            self.training_table.add_column("Value", style="magenta")
            
            # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
            self.training_stats = {
                "ep_len_mean": 0,
                "ep_rew_mean": 0,
                "ep_score_mean": 0,
                "fps": 0,
                "iterations": 0,
                "time_elapsed": 0,
                "total_timesteps": 0,
                "approx_kl": 0,
                "clip_fraction": 0,
                "entropy_loss": 0,
                "learning_rate": 0,
                "loss": 0,
                "policy_gradient_loss": 0,
                "value_loss": 0,
                "episodes_completed": 0
            }
            
            self.start_time = time.time()
            self.last_table_update = 0
            self.table_update_interval = 5  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡è¡¨æ ¼
            
            # åˆ›å»ºè¿›åº¦æ¡
            self.progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=self.console,
                expand=True
            )
            self.progress_task = None
        
    def _update_training_table(self):
        """æ›´æ–°è®­ç»ƒçŠ¶æ€è¡¨æ ¼"""
        if not RICH_AVAILABLE:
            return
            
        # æ¸…é™¤æ—§è¡¨æ ¼å†…å®¹
        reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
        self.training_table = Table(title=f"ğŸ¤– GoBigger RL Training Status ({reward_type})")
        self.training_table.add_column("Category", style="cyan", no_wrap=True)
        self.training_table.add_column("Metric", style="blue", no_wrap=True)
        self.training_table.add_column("Value", style="magenta")
        
        # Rollout metrics
        self.training_table.add_row("rollout/", "ep_len_mean", f"{self.training_stats['ep_len_mean']:.0f}")
        self.training_table.add_row("", "ep_rew_mean", f"{self.training_stats['ep_rew_mean']:.2f}")
        self.training_table.add_row("", "ep_score_mean", f"{self.training_stats['ep_score_mean']:.0f}")
        
        # Time metrics
        self.training_table.add_row("time/", "fps", f"{self.training_stats['fps']:.0f}")
        self.training_table.add_row("", "iterations", f"{self.training_stats['iterations']}")
        self.training_table.add_row("", "time_elapsed", f"{self.training_stats['time_elapsed']:.0f}")
        self.training_table.add_row("", "total_timesteps", f"{self.training_stats['total_timesteps']}")
        
        # Training metrics
        if self.training_stats['approx_kl'] > 0:
            self.training_table.add_row("train/", "approx_kl", f"{self.training_stats['approx_kl']:.6f}")
            self.training_table.add_row("", "clip_fraction", f"{self.training_stats['clip_fraction']:.4f}")
            self.training_table.add_row("", "entropy_loss", f"{self.training_stats['entropy_loss']:.2f}")
            self.training_table.add_row("", "learning_rate", f"{self.training_stats['learning_rate']:.6f}")
            self.training_table.add_row("", "loss", f"{self.training_stats['loss']:.6f}")
            self.training_table.add_row("", "policy_gradient_loss", f"{self.training_stats['policy_gradient_loss']:.6f}")
            self.training_table.add_row("", "value_loss", f"{self.training_stats['value_loss']:.6f}")
        
        # Episode info
        self.training_table.add_row("episodes/", "completed", f"{self.training_stats['episodes_completed']}")
        
        # å¢å¼ºå¥–åŠ±ç³»ç»Ÿç‰¹æœ‰ä¿¡æ¯
        if self.use_enhanced_reward:
            self.training_table.add_row("", "", "")  # åˆ†éš”çº¿
            self.training_table.add_row("enhanced/", "system", "ğŸ¯ å¯†é›†å¥–åŠ±ä¿¡å·")
            self.training_table.add_row("", "components", "å¤šç»´åº¦å¥–åŠ±")
        
    def _on_step(self) -> bool:
        # æ”¶é›†å¥–åŠ±å’Œåˆ†æ•°ç»Ÿè®¡
        for info in self.locals['infos']:
            if 'final_score' in info:
                final_score = info['final_score']
                score_delta = info.get('score_delta', 0)
                episode_length = info.get('episode_length', 0)
                
                self.episode_scores.append(final_score)
                self.training_stats['episodes_completed'] += 1
                
                if not RICH_AVAILABLE and self.verbose > 0:
                    print(f"ğŸ¯ Episode ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                          f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ­¥æ•°: {episode_length}")
            
            # æ‰‹åŠ¨æ”¶é›†episode rewardä¿¡æ¯
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_len = info['episode']['l']
                self.episode_rewards.append(episode_reward)
                
                # æ‰‹åŠ¨æ›´æ–°episodeç»Ÿè®¡
                if len(self.episode_rewards) > 0:
                    self.training_stats['ep_rew_mean'] = np.mean(self.episode_rewards)
                if len(self.episode_rewards) > 0:
                    # ä½¿ç”¨episodeé•¿åº¦ä¿¡æ¯
                    ep_lengths = [info['episode']['l'] for info in self.locals.get('infos', []) if 'episode' in info]
                    if ep_lengths:
                        self.training_stats['ep_len_mean'] = np.mean(ep_lengths)
                    else:
                        self.training_stats['ep_len_mean'] = episode_len
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        if RICH_AVAILABLE:
            current_time = time.time()
            self.training_stats['time_elapsed'] = current_time - self.start_time
            self.training_stats['total_timesteps'] = self.num_timesteps
            
            # ä»loggerè·å–è®­ç»ƒæŒ‡æ ‡
            if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
                logger_data = self.model.logger.name_to_value
                
                # Rollout metrics
                if 'rollout/ep_len_mean' in logger_data:
                    self.training_stats['ep_len_mean'] = logger_data['rollout/ep_len_mean']
                if 'rollout/ep_rew_mean' in logger_data:
                    self.training_stats['ep_rew_mean'] = logger_data['rollout/ep_rew_mean']
                
                # Time metrics
                if 'time/fps' in logger_data:
                    self.training_stats['fps'] = logger_data['time/fps']
                if 'time/iterations' in logger_data:
                    self.training_stats['iterations'] = logger_data['time/iterations']
                
                # Training metrics
                if 'train/approx_kl' in logger_data:
                    self.training_stats['approx_kl'] = logger_data['train/approx_kl']
                if 'train/clip_fraction' in logger_data:
                    self.training_stats['clip_fraction'] = logger_data['train/clip_fraction']
                if 'train/entropy_loss' in logger_data:
                    self.training_stats['entropy_loss'] = logger_data['train/entropy_loss']
                if 'train/learning_rate' in logger_data:
                    self.training_stats['learning_rate'] = logger_data['train/learning_rate']
                if 'train/loss' in logger_data:
                    self.training_stats['loss'] = logger_data['train/loss']
                if 'train/policy_gradient_loss' in logger_data:
                    self.training_stats['policy_gradient_loss'] = logger_data['train/policy_gradient_loss']
                if 'train/value_loss' in logger_data:
                    self.training_stats['value_loss'] = logger_data['train/value_loss']
            
            # æ›´æ–°å¹³å‡åˆ†æ•°
            if len(self.episode_scores) > 0:
                self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
        
        # å®šæœŸè¯„ä¼°å’Œä¿å­˜ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¹²æ‰°ç•Œé¢ï¼‰
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards)
                mean_score = np.mean(self.episode_scores) if len(self.episode_scores) > 0 else 0
                
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    if not RICH_AVAILABLE and self.verbose > 0:
                        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.save_freq == 0:
            model_path = f"checkpoints/model_{self.num_timesteps}_steps.zip"
            self.model.save(model_path)
        
        # ğŸš€ é•¿æ—¶é—´è®­ç»ƒå¢å¼ºåŠŸèƒ½
        # è‡ªåŠ¨ä¿å­˜ï¼ˆæ›´é¢‘ç¹ï¼Œé˜²æ­¢æ„å¤–ä¸¢å¤±ï¼‰
        if self.num_timesteps - self.last_auto_save >= self.auto_save_freq:
            auto_save_path = f"models/auto_save_{self.num_timesteps}.zip"
            self.model.save(auto_save_path)
            self.last_auto_save = self.num_timesteps
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜: {auto_save_path}")
        
        # æ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆç”¨äºé•¿æ—¶é—´è®­ç»ƒæ¢å¤ï¼‰
        if self.num_timesteps - self.last_checkpoint >= self.checkpoint_freq:
            checkpoint_path = f"checkpoints/checkpoint_{self.num_timesteps}.zip"
            self.model.save(checkpoint_path)
            self.last_checkpoint = self.num_timesteps
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡ï¼ˆè½¬æ¢numpyç±»å‹ä»¥é¿å…JSONåºåˆ—åŒ–é”™è¯¯ï¼‰
            stats_path = f"checkpoints/stats_{self.num_timesteps}.json"
            stats_data = {
                'timesteps': self.num_timesteps,
                'best_reward': self.best_mean_reward,
                'episode_rewards': list(self.episode_rewards),
                'episode_scores': list(self.episode_scores),
                'training_stats': self.training_stats,
                'elapsed_hours': (time.time() - self.start_time) / 3600
            }
            # è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºåŸç”ŸPythonç±»å‹
            stats_data = convert_numpy_types(stats_data)
            
            with open(stats_path, 'w') as f:
                json.dump(stats_data, f, indent=2)
            
            if not RICH_AVAILABLE and self.verbose > 0:
                elapsed_hours = (time.time() - self.start_time) / 3600
                print(f"ğŸ“‹ æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_path} (è®­ç»ƒ {elapsed_hours:.1f} å°æ—¶)")
        
        # å­¦ä¹ ç‡è¡°å‡ï¼ˆé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–ï¼‰
        if self.enable_lr_decay and self.initial_lr is not None:
            progress = self.num_timesteps / self.total_timesteps
            # çº¿æ€§è¡°å‡åˆ°åˆå§‹å­¦ä¹ ç‡çš„10%
            new_lr = self.initial_lr * (1.0 - 0.9 * progress)
            if hasattr(self.model, 'lr_schedule'):
                # æ›´æ–°å­¦ä¹ ç‡
                if hasattr(self.model.policy.optimizer, 'param_groups'):
                    for param_group in self.model.policy.optimizer.param_groups:
                        param_group['lr'] = new_lr
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
        
        return True

def create_env(config=None):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorå’Œä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿï¼‰"""
    default_config = {
        'max_episode_steps': 3000,      # ğŸ”¥ æ›´é•¿çš„episodeä»¥ä¾¿å……åˆ†æ¢ç´¢
        'use_enhanced_reward': False,   # é»˜è®¤ä½¿ç”¨ç®€å•å¥–åŠ±
        'debug_rewards': False,         # å¥–åŠ±è°ƒè¯•é€‰é¡¹
        'use_optimized_reward': True,   # ğŸ”¥ å¯ç”¨ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿï¼ˆæ¢ç´¢+æ™ºèƒ½åˆ†è£‚ï¼‰
        'exploration_cell_size': 80,    # ğŸ”¥ æ¢ç´¢ç½‘æ ¼å¤§å°ï¼ˆé€‚ä¸­çš„æ ¼å­å¤§å°ï¼‰
        'direction_diversity_window': 15,  # ğŸ”¥ æ–¹å‘å¤šæ ·æ€§è¯„ä¼°çª—å£
        'split_efficiency_window': 60,     # ğŸ”¥ åˆ†è£‚æ•ˆç‡è¯„ä¼°çª—å£
        'reward_debug': False,          # å¥–åŠ±ç»„ä»¶è°ƒè¯•
    }
    if config:
        default_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(default_config)
    
    # ğŸ”¥ å¦‚æœå¯ç”¨ä¼˜åŒ–å¥–åŠ±ï¼ŒåŒ…è£…ç¯å¢ƒ
    if default_config.get('use_optimized_reward', True):
        env = OptimizedRewardWrapper(env, default_config)
        print("ğŸ¯ å¯ç”¨ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿ - æ¢ç´¢æ¿€åŠ±+æ™ºèƒ½åˆ†è£‚ç­–ç•¥")
    
    # ğŸ”¥ å¯ç”¨å¥–åŠ±è°ƒè¯•ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if default_config.get('debug_rewards', False):
        env.debug_rewards = True
        print("ğŸ” å¯ç”¨å¥–åŠ±è°ƒè¯•æ¨¡å¼ - å°†æ˜¾ç¤ºSplit/EjectåŠ¨ä½œå¥–åŠ±")
    
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

class OptimizedRewardWrapper:
    """
    ğŸ”¥ ä¼˜åŒ–å¥–åŠ±åŒ…è£…å™¨ - åœ¨åŸæœ‰å¥–åŠ±åŸºç¡€ä¸Šæ·»åŠ æ¢ç´¢å’Œç­–ç•¥å¥–åŠ±
    """
    
    def __init__(self, env, config):
        self.env = env
        self.config = config
        self.reward_calculator = OptimizedRewardCalculator(config)
        self.previous_state = None
        self.episode_optimized_rewards = []
        self.episode_exploration_rewards = []
        self.episode_split_rewards = []
        
        # å°†ç¯å¢ƒæ–¹æ³•å§”æ‰˜ç»™åŸç¯å¢ƒ
        for attr in ['observation_space', 'action_space', 'spec']:
            if hasattr(env, attr):
                setattr(self, attr, getattr(env, attr))
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥å¹¶è®¡ç®—ä¼˜åŒ–å¥–åŠ±"""
        obs, base_reward, done, info = self.env.step(action)
        
        # ğŸ”¥ è®¡ç®—ä¼˜åŒ–å¥–åŠ±
        current_state = obs  # å‡è®¾obsåŒ…å«çŠ¶æ€ä¿¡æ¯
        optimized_reward, reward_components = self.reward_calculator.calculate_optimized_reward(
            current_state, self.previous_state, action, base_reward
        )
        
        # æ›´æ–°çŠ¶æ€
        self.previous_state = current_state
        
        # ç»Ÿè®¡å¥–åŠ±ç»„ä»¶
        exploration_reward = reward_components.get('exploration_bonus', 0) + reward_components.get('diversity_bonus', 0)
        split_reward = reward_components.get('smart_split_bonus', 0) + reward_components.get('split_efficiency', 0)
        
        self.episode_optimized_rewards.append(optimized_reward - base_reward)
        self.episode_exploration_rewards.append(exploration_reward)
        self.episode_split_rewards.append(split_reward)
        
        # ğŸ”¥ åœ¨infoä¸­æ·»åŠ å¥–åŠ±è°ƒè¯•ä¿¡æ¯
        if self.config.get('reward_debug', False):
            info['reward_components'] = reward_components
            info['optimized_reward_bonus'] = optimized_reward - base_reward
        
        # episodeç»“æŸæ—¶æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if done:
            info['exploration_stats'] = self.reward_calculator.get_exploration_stats()
            info['split_stats'] = self.reward_calculator.get_split_stats()
            info['total_exploration_reward'] = sum(self.episode_exploration_rewards)
            info['total_split_reward'] = sum(self.episode_split_rewards)
            info['total_optimized_bonus'] = sum(self.episode_optimized_rewards)
            
            # é‡ç½®episodeç»Ÿè®¡
            self.episode_optimized_rewards = []
            self.episode_exploration_rewards = []
            self.episode_split_rewards = []
            self.reward_calculator.reset()
            self.previous_state = None
        
        return obs, optimized_reward, done, info
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.env.reset()
        self.previous_state = None
        self.reward_calculator.reset()
        self.episode_optimized_rewards = []
        self.episode_exploration_rewards = []
        self.episode_split_rewards = []
        return obs
    
    def render(self, mode='human'):
        """æ¸²æŸ“ç¯å¢ƒ"""
        return self.env.render(mode)
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.env.close()
    
    def __getattr__(self, name):
        """å§”æ‰˜å…¶ä»–å±æ€§ç»™åŸç¯å¢ƒ"""
        return getattr(self.env, name)

def create_enhanced_env(config=None):
    """åˆ›å»ºå¢å¼ºå¥–åŠ±è®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorï¼‰"""
    enhanced_config = {
        'max_episode_steps': 2000,
        'use_enhanced_reward': True,  # å¯ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ
        'enhanced_reward_weights': {
            'score_growth': 2.0,        # åˆ†æ•°å¢é•¿å¥–åŠ±æƒé‡
            'efficiency': 1.5,          # æ•ˆç‡å¥–åŠ±æƒé‡
            'exploration': 0.8,         # æ¢ç´¢å¥–åŠ±æƒé‡
            'strategic_split': 2.0,     # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±æƒé‡
            'food_density': 1.0,        # é£Ÿç‰©å¯†åº¦å¥–åŠ±æƒé‡
            'survival': 0.02,           # ç”Ÿå­˜å¥–åŠ±æƒé‡
            'time_penalty': -0.001,     # æ—¶é—´æƒ©ç½šæƒé‡
            'death_penalty': -20.0,     # æ­»äº¡æƒ©ç½šæƒé‡
        }
    }
    if config:
        enhanced_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(enhanced_config)
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

def train_with_stable_baselines3(algorithm='PPO', total_timesteps=100000, config=None, use_enhanced_reward=False):
    """ä½¿ç”¨stable-baselines3è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {algorithm} ç®—æ³•è®­ç»ƒ ({reward_type})...")
    
    # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨vectorized environmentä»¥æ­£ç¡®æ”¯æŒepisodeç»Ÿè®¡
    def make_env():
        if use_enhanced_reward:
            return create_enhanced_env(config)
        else:
            return create_env(config)
    
    if use_enhanced_reward:
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")
    else:
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ")
    
    # ä½¿ç”¨make_vec_envæ¥æ­£ç¡®é›†æˆMonitorå’Œepisodeç»Ÿè®¡
    env = make_vec_env(make_env, n_envs=1, vec_env_cls=DummyVecEnv)
    
    # ğŸš€ é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„ç½‘ç»œé…ç½®
    def get_optimized_policy_kwargs(total_timesteps):
        """æ ¹æ®è®­ç»ƒè§„æ¨¡ä¼˜åŒ–ç½‘ç»œç»“æ„"""
        if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
            return dict(
                net_arch=[512, 512, 256, 128],  # æ›´æ·±æ›´å®½çš„ç½‘ç»œ
                activation_fn=torch.nn.ReLU,
                share_features_extractor=False
            )
        elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[256, 256, 128],  # æ›´æ·±çš„ç½‘ç»œ
                activation_fn=torch.nn.ReLU,
                share_features_extractor=False
            )
        elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[128, 128],
                activation_fn=torch.nn.ReLU
            )
        else:  # çŸ­æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[64, 64],
                activation_fn=torch.nn.ReLU
            )
    
    try:
        import torch
        policy_kwargs = get_optimized_policy_kwargs(total_timesteps)
        print(f"ğŸ§  ç½‘ç»œç»“æ„: {policy_kwargs['net_arch']} (é€‚é… {total_timesteps:,} æ­¥è®­ç»ƒ)")
    except ImportError:
        policy_kwargs = None
        print("âš ï¸  æœªæ£€æµ‹åˆ°PyTorchï¼Œä½¿ç”¨é»˜è®¤ç½‘ç»œç»“æ„")
    
    # ğŸ¯ é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„è¶…å‚æ•°ï¼ˆğŸ”¥ é‡ç‚¹æé«˜ç†µç³»æ•°å¢å¼ºæ¢ç´¢ï¼‰
    def get_optimized_hyperparams(algorithm, total_timesteps):
        """æ ¹æ®ç®—æ³•å’Œè®­ç»ƒè§„æ¨¡ä¼˜åŒ–è¶…å‚æ•° - é‡ç‚¹æé«˜ç†µç³»æ•°è§£å†³æ¢ç´¢é—®é¢˜"""
        if algorithm == 'PPO':
            if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
                return {
                    "learning_rate": 2e-4,  # æ›´ä½çš„å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
                    "n_steps": 2048,  # æ›´å¤§çš„rolloutï¼Œæ›´å¥½çš„é‡‡æ ·æ•ˆç‡
                    "batch_size": 256,  # æ›´å¤§çš„æ‰¹é‡ï¼Œæ›´ç¨³å®šçš„æ¢¯åº¦
                    "n_epochs": 6,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.998,  # æ›´é«˜çš„æŠ˜æ‰£å› å­ï¼Œè€ƒè™‘é•¿æœŸå›æŠ¥
                    "gae_lambda": 0.99,  # æ›´é«˜çš„GAEå‚æ•°
                    "clip_range": 0.15,  # ç¨å¤§çš„è£å‰ªèŒƒå›´ï¼Œå¢åŠ å­¦ä¹ çµæ´»æ€§
                    "ent_coef": 0.1,   # ğŸ”¥ğŸ”¥ğŸ”¥ è¶…é«˜ç†µç³»æ•°ï¼æœ€å¼ºæ¢ç´¢è®¾ç½®ï¼Œè§£å†³"ä¸€æ¡è·¯èµ°åˆ°é»‘"
                    "vf_coef": 0.25,  # ä»·å€¼å‡½æ•°ç³»æ•°
                    "max_grad_norm": 0.5  # ç¨å¾®æ”¾æ¾æ¢¯åº¦è£å‰ª
                }
            elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
                return {
                    "learning_rate": 2.5e-4,  # ç¨ä½çš„å­¦ä¹ ç‡
                    "n_steps": 1024,  # æ›´å¤§çš„rollout
                    "batch_size": 128,  # æ›´å¤§çš„æ‰¹é‡
                    "n_epochs": 8,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.995,  # æ›´é«˜çš„æŠ˜æ‰£å› å­
                    "gae_lambda": 0.98,  # GAEå‚æ•°
                    "clip_range": 0.18,  # ç¨å¤§çš„è£å‰ªèŒƒå›´
                    "ent_coef": 0.08,  # ğŸ”¥ğŸ”¥ğŸ”¥ è¶…é«˜ç†µç³»æ•°ï¼å¼ºåŠ›æ¢ç´¢è®¾ç½®
                    "vf_coef": 0.5,  # ä»·å€¼å‡½æ•°ç³»æ•°
                    "max_grad_norm": 0.5  # æ¢¯åº¦è£å‰ª
                }
            elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
                return {
                    "learning_rate": 3e-4,
                    "n_steps": 512,
                    "batch_size": 64,
                    "n_epochs": 6,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.99,
                    "gae_lambda": 0.95,
                    "clip_range": 0.2,
                    "ent_coef": 0.06,  # ğŸ”¥ğŸ”¥ğŸ”¥ é«˜ç†µç³»æ•°ï¼Œå¼ºåŒ–æ¢ç´¢å¤šæ ·æ€§
                }
            else:  # æ ‡å‡†è®­ç»ƒ
                return {
                    "learning_rate": 3e-4,
                    "n_steps": 256,
                    "batch_size": 32,
                    "n_epochs": 5,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.99,
                    "gae_lambda": 0.95,
                    "clip_range": 0.2,
                    "ent_coef": 0.05,  # ğŸ”¥ğŸ”¥ğŸ”¥ æ˜¾è‘—æé«˜ç†µç³»æ•°ï¼Œå¼ºåŒ–æ¢ç´¢
                }
        return {}
    
    hyperparams = get_optimized_hyperparams(algorithm, total_timesteps)
    
    # åˆ›å»ºæ¨¡å‹
    if algorithm == 'PPO':
        model = PPO(
            "MlpPolicy", 
            env,
            policy_kwargs=policy_kwargs,
            verbose=0 if RICH_AVAILABLE else 1,  # Richç•Œé¢æ—¶é™é»˜æ¨¡å¼
            tensorboard_log="./tensorboard_logs/",
            **hyperparams
        )
    elif algorithm == 'DQN':
        model = DQN(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=1e-4,
            buffer_size=min(100000, max(50000, total_timesteps // 10)),  # è‡ªé€‚åº”ç¼“å†²åŒº
            learning_starts=max(1000, total_timesteps // 100),
            batch_size=64 if total_timesteps >= 100000 else 32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            gradient_steps=1,
            target_update_interval=max(500, total_timesteps // 100),
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'A2C':
        model = A2C(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=7e-4,
            n_steps=8 if total_timesteps >= 100000 else 5,
            gamma=0.99,
            gae_lambda=1.0,
            ent_coef=0.01,
            vf_coef=0.25,
            max_grad_norm=0.5,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # ğŸš€ åˆ›å»ºé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„å›è°ƒ
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs("models", exist_ok=True)
    
    # åŠ¨æ€è°ƒæ•´å›è°ƒé¢‘ç‡
    if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
        eval_freq = 10000
        save_freq = 50000
        auto_save_freq = 25000
        checkpoint_freq = 100000
        enable_lr_decay = True
        print(f"ğŸŒŸ è¶…é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}, è‡ªåŠ¨ä¿å­˜={auto_save_freq}")
        print(f"   é¢„è®¡è®­ç»ƒæ—¶é—´: 20-30å°æ—¶ (é€‚åˆå‘¨æœ«é•¿æ—¶é—´è®­ç»ƒ)")
    elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
        eval_freq = 5000
        save_freq = 25000
        auto_save_freq = 10000
        checkpoint_freq = 50000
        enable_lr_decay = True
        print(f"ğŸ¯ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}, è‡ªåŠ¨ä¿å­˜={auto_save_freq}")
    elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
        eval_freq = 2500
        save_freq = 10000
        auto_save_freq = 5000
        checkpoint_freq = 25000
        enable_lr_decay = True
        print(f"ğŸ¯ ä¸­é•¿è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}")
    else:  # æ ‡å‡†è®­ç»ƒ
        eval_freq = 1000
        save_freq = 5000
        auto_save_freq = 2500
        checkpoint_freq = 10000
        enable_lr_decay = False
    
    if RICH_AVAILABLE:
        # åˆ›å»ºRichç•Œé¢ç‰ˆæœ¬çš„å›è°ƒ
        class RichTrainingCallback(TrainingCallback):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.use_enhanced_reward = use_enhanced_reward  # åœ¨çˆ¶ç±»åˆå§‹åŒ–åè®¾ç½®
                
                # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼ˆç”¨äºå­¦ä¹ ç‡è¡°å‡ï¼‰
                if hasattr(model, 'learning_rate'):
                    if callable(model.learning_rate):
                        self.initial_lr = model.learning_rate(1.0)  # è·å–åˆå§‹å­¦ä¹ ç‡
                    else:
                        self.initial_lr = model.learning_rate
                    print(f"ğŸ“Š åˆå§‹å­¦ä¹ ç‡: {self.initial_lr}")
                
                # print(f"ğŸ” å›è°ƒåˆå§‹åŒ–è°ƒè¯•: use_enhanced_reward = {self.use_enhanced_reward}")
                
                # åˆå§‹åŒ–è¿›åº¦æ¡ä»»åŠ¡
                if hasattr(self, 'progress'):
                    reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                    if total_timesteps >= 3000000:
                        training_mode = "è¶…é•¿æ—¶é—´"
                    elif total_timesteps >= 1000000:
                        training_mode = "é•¿æ—¶é—´"
                    elif total_timesteps >= 500000:
                        training_mode = "ä¸­é•¿æ—¶é—´"
                    else:
                        training_mode = "æ ‡å‡†"
                    self.progress_task = self.progress.add_task(
                        f"[green]ğŸš€ {algorithm} {training_mode}è®­ç»ƒ ({reward_type})", 
                        total=total_timesteps
                    )
                
            def _on_step(self):
                # æ›´æ–°è®­ç»ƒç»Ÿè®¡
                current_time = time.time()
                self.training_stats['time_elapsed'] = current_time - self.start_time
                self.training_stats['total_timesteps'] = self.num_timesteps
                
                # ä»æ¨¡å‹è·å–è®­ç»ƒæŒ‡æ ‡
                if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                    logger_data = self.model.logger.name_to_value
                    
                    # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰loggeræ•°æ® (å¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨è°ƒè¯•)
                    # if self.num_timesteps % 1000 == 0:  # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                    #     print(f"\nğŸ” Loggerå®Œæ•´æ•°æ®è°ƒè¯• (æ­¥æ•°: {self.num_timesteps}):")
                    #     if logger_data:
                    #         for key, value in logger_data.items():
                    #             print(f"  {key}: {value}")
                    #     else:
                    #         print("  Loggeræ•°æ®ä¸ºç©º!")
                    #     
                    #     print(f"ğŸ” Training Statsè°ƒè¯•:")
                    #     for key, value in self.training_stats.items():
                    #         if value != 0:  # åªæ˜¾ç¤ºéé›¶å€¼
                    #             print(f"  {key}: {value}")
                    #     print()
                    
                    # æ›´æ–°å„é¡¹æŒ‡æ ‡
                    for key, value in logger_data.items():
                        if key in ['rollout/ep_len_mean', 'rollout/ep_rew_mean', 'time/fps', 
                                  'time/iterations', 'train/approx_kl', 'train/clip_fraction',
                                  'train/entropy_loss', 'train/learning_rate', 'train/loss',
                                  'train/policy_gradient_loss', 'train/value_loss']:
                            stat_name = key.split('/')[-1]
                            if stat_name in self.training_stats:
                                self.training_stats[stat_name] = value
                
                # æ›´æ–°å¹³å‡åˆ†æ•°
                if len(self.episode_scores) > 0:
                    self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
                
                # å®šæœŸæ›´æ–°æ˜¾ç¤º
                if current_time - self.last_table_update > self.table_update_interval:
                    self._update_training_table()
                    self.last_table_update = current_time
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if hasattr(self, 'progress') and self.progress_task is not None:
                        self.progress.update(self.progress_task, completed=self.num_timesteps)
                    
                    # åˆ›å»ºå¸ƒå±€ç»„ä»¶
                    reward_panel, tensorboard_panel, status_panel = self._create_training_layout()
                    
                    # æ¸…å±å¹¶æ˜¾ç¤º
                    self.console.clear()
                    
                    # æ˜¾ç¤ºä¸Šæ’é¢æ¿
                    top_row = Columns([reward_panel, tensorboard_panel], equal=True)
                    self.console.print(top_row)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡è¡¨æ ¼
                    stats_panel = Panel(
                        self.training_table,
                        title="è®­ç»ƒç»Ÿè®¡",
                        border_style="blue"
                    )
                    self.console.print(stats_panel)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                    self.console.print(status_panel)
                
                return super()._on_step()
            
            def _create_training_layout(self):
                """åˆ›å»ºè®­ç»ƒç•Œé¢å¸ƒå±€"""
                # å¥–åŠ±ç³»ç»Ÿä¿¡æ¯é¢æ¿
                reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                reward_icon = "ğŸ¯" if self.use_enhanced_reward else "ğŸ“Š"
                reward_desc = "å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå¤šç»´åº¦åé¦ˆ" if self.use_enhanced_reward else "ç»å…¸å¼ºåŒ–å­¦ä¹ å¥–åŠ±"
                
                reward_panel = Panel(
                    f"[bold green]{reward_icon} {reward_type}ç³»ç»Ÿ[/bold green]\n[dim]{reward_desc}[/dim]",
                    title="å¥–åŠ±ç³»ç»Ÿ",
                    border_style="green"
                )
                
                # TensorBoardæç¤º
                tensorboard_panel = Panel(
                    "[bold cyan]ğŸ“Š TensorBoardç›‘æ§[/bold cyan]\n" +
                    "[dim]åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/dim]\n" +
                    "[yellow]tensorboard --logdir ./tensorboard_logs[/yellow]\n" +
                    "[dim]ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:6006[/dim]",
                    title="ç›‘æ§å·¥å…·",
                    border_style="cyan"
                )
                
                # çŠ¶æ€ä¿¡æ¯
                progress_percent = (self.num_timesteps / self.total_timesteps) * 100
                status_lines = []
                status_lines.append(f"[bold blue]è®­ç»ƒè¿›åº¦[/bold blue]: {progress_percent:.1f}% ({self.num_timesteps:,}/{self.total_timesteps:,} steps)")
                
                if len(self.episode_scores) > 0:
                    latest_score = self.episode_scores[-1]
                    avg_score = np.mean(self.episode_scores)
                    status_lines.append(f"[bold yellow]Episodeä¿¡æ¯[/bold yellow]: æœ€æ–°åˆ†æ•°={latest_score:.0f}, å¹³å‡åˆ†æ•°={avg_score:.0f}")
                
                if self.use_enhanced_reward:
                    status_lines.append("[dim]ğŸ’¡ å¢å¼ºå¥–åŠ±æä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·[/dim]")
                
                status_panel = Panel(
                    "\n".join(status_lines),
                    title="çŠ¶æ€ä¿¡æ¯",
                    border_style="magenta"
                )
                
                # åˆ›å»ºå®Œæ•´å¸ƒå±€å¹¶è¿”å›æ‰€æœ‰ç»„ä»¶
                return reward_panel, tensorboard_panel, status_panel
        
        callback = RichTrainingCallback(
            eval_freq=eval_freq, 
            save_freq=save_freq, 
            total_timesteps=total_timesteps,
            auto_save_freq=auto_save_freq,
            checkpoint_freq=checkpoint_freq,
            enable_lr_decay=enable_lr_decay
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸ“ˆ å¼€å§‹ä½¿ç”¨ {algorithm} è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
        print(f"ğŸ¯ å¥–åŠ±ç³»ç»Ÿ: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}")
        print("ï¿½ TensorBoardæ—¥å¿—: ./tensorboard_logs/")
        print("ï¿½ğŸ’¡ è®­ç»ƒç•Œé¢å°†æ¯5ç§’æ›´æ–°ä¸€æ¬¡...")
        print("\nğŸš€ å¯åŠ¨ TensorBoard ç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   ç„¶åè®¿é—®: http://localhost:6006")
        print("\n" + "="*60)
        start_time = time.time()
        
        # å¯åŠ¨è¿›åº¦æ¡
        with callback.progress:
            model.learn(
                total_timesteps=total_timesteps,
                callback=callback,
                tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
            )
        
        train_time = time.time() - start_time
        
        # è®­ç»ƒå®Œæˆä¿¡æ¯
        callback.console.clear()
        completion_panel = Panel(
            f"[bold green]âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼[/bold green]\n\n" +
            f"[yellow]è®­ç»ƒæ—¶é—´[/yellow]: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)\n" +
            f"[yellow]æ€»æ­¥æ•°[/yellow]: {total_timesteps:,}\n" +
            f"[yellow]å¥–åŠ±ç³»ç»Ÿ[/yellow]: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}\n" +
            f"[yellow]ç®—æ³•[/yellow]: {algorithm}\n\n" +
            f"[cyan]ğŸ“Š æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/cyan]\n" +
            f"[dim]tensorboard --logdir ./tensorboard_logs[/dim]\n\n" +
            f"[cyan]ğŸ“ æ¨¡å‹æ–‡ä»¶:[/cyan]\n" +
            f"[dim]./models/ å’Œ ./checkpoints/[/dim]",
            title="ğŸ‰ è®­ç»ƒå®Œæˆ",
            border_style="green"
        )
        callback.console.print(completion_panel)
        
    else:
        # ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢è®­ç»ƒ
        callback = TrainingCallback(
            eval_freq=eval_freq, 
            save_freq=save_freq, 
            total_timesteps=total_timesteps,
            auto_save_freq=auto_save_freq,
            checkpoint_freq=checkpoint_freq,
            enable_lr_decay=enable_lr_decay
        )
        callback.use_enhanced_reward = use_enhanced_reward
        
        # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼ˆç”¨äºå­¦ä¹ ç‡è¡°å‡ï¼‰
        if hasattr(model, 'learning_rate'):
            if callable(model.learning_rate):
                callback.initial_lr = model.learning_rate(1.0)
            else:
                callback.initial_lr = model.learning_rate
        
        reward_info = "å¢å¼ºå¥–åŠ±ç³»ç»Ÿ" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ"
        if total_timesteps >= 3000000:
            training_mode = "è¶…é•¿æ—¶é—´"
        elif total_timesteps >= 1000000:
            training_mode = "é•¿æ—¶é—´"
        elif total_timesteps >= 500000:
            training_mode = "ä¸­é•¿æ—¶é—´"
        else:
            training_mode = "æ ‡å‡†"
        print(f"ğŸ“ˆ å¼€å§‹{training_mode}è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps:,} ({reward_info})")
        if enable_lr_decay:
            print(f"ğŸ“Š å­¦ä¹ ç‡è¡°å‡: å¯ç”¨ (åˆå§‹LR: {callback.initial_lr})")
        print(f"ğŸ’¾ ä¿å­˜è®¾ç½®: æ£€æŸ¥ç‚¹æ¯{checkpoint_freq:,}æ­¥, è‡ªåŠ¨ä¿å­˜æ¯{auto_save_freq:,}æ­¥")
        if total_timesteps >= 3000000:
            print(f"â° é¢„è®¡è®­ç»ƒæ—¶é—´: 20-30å°æ—¶ (å»ºè®®å‘¨æœ«è¿›è¡Œ)")
        print("ğŸ“Š TensorBoardç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   è®¿é—®: http://localhost:6006")
        print("="*50)
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
        )
        
        train_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")
        print("ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    reward_suffix = "_enhanced" if use_enhanced_reward else "_standard"
    final_model_path = f"models/{algorithm}_gobigger{reward_suffix}_final.zip"
    os.makedirs("models", exist_ok=True)
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def simple_random_training(episodes=100, use_enhanced_reward=False):
    """ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤ºï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸ® è¿è¡Œéšæœºç­–ç•¥æ¼”ç¤ºè®­ç»ƒ ({reward_type})...")
    
    if use_enhanced_reward:
        env = create_enhanced_env({'max_episode_steps': 500})
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    else:
        env = create_env({'max_episode_steps': 500})
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # éšæœºåŠ¨ä½œ
            action = env.action_space.sample()
            action[2] = int(action[2])  # åŠ¨ä½œç±»å‹ä¸ºæ•´æ•°
            
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    if (episode + 1) % 10 == 0:  # æ¯10ä¸ªepisodeæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                        print(f"  Episode {episode + 1} ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                              f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ€»å¥–åŠ±: {total_reward:.3f}")
                        
                        # å¦‚æœä½¿ç”¨å¢å¼ºå¥–åŠ±ï¼Œæ˜¾ç¤ºå¥–åŠ±ç»„ä»¶ä¿¡æ¯
                        if use_enhanced_reward and hasattr(env, 'reward_components_history') and env.reward_components_history:
                            latest_components = env.reward_components_history[-1]['components']
                            if latest_components:
                                print(f"    å¥–åŠ±ç»„ä»¶: ", end="")
                                for comp, val in latest_components.items():
                                    if abs(val) > 0.001:
                                        print(f"{comp}={val:.3f} ", end="")
                                print()
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards)
    title = f'Episode Rewards ({reward_type})'
    plt.title(title)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(episode_lengths)
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    
    plt.tight_layout()
    filename = f'random_training_results_{reward_type.lower().replace(" ", "_")}.png'
    plt.savefig(filename)
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filename}")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-20:]):.3f}")
    
    if use_enhanced_reward:
        print("ğŸ’¡ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›äº†æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")

def evaluate_model(model_path, episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½å’Œè¯„ä¼°æ¨¡å‹")
        return
    
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    if 'PPO' in model_path:
        model = PPO.load(model_path)
    elif 'DQN' in model_path:
        model = DQN.load(model_path)
    elif 'A2C' in model_path:
        model = A2C.load(model_path)
    else:
        print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env({'max_episode_steps': 1000})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}, "
                          f"æœ€ç»ˆåˆ†æ•°={final_score:.2f}, åˆ†æ•°å˜åŒ–={score_delta:+.2f}")
                else:
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}")
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")

def test_optimized_reward_system():
    """
    ğŸ§ª æµ‹è¯•ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿçš„åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–å¥–åŠ±è®¡ç®—å™¨...")
    
    calculator = OptimizedRewardCalculator()
    
    # æ¨¡æ‹Ÿä¸€äº›çŠ¶æ€ç”¨äºæµ‹è¯•
    class MockState:
        def __init__(self, score, position, food_count=0):
            self.score = score
            self.rectangle = [position[0]-10, position[1]-10, position[0]+10, position[1]+10]
            self.food = [f"food_{i}" for i in range(food_count)]
            self.clone = ["main_cell"]
    
    # æµ‹è¯•æ¢ç´¢å¥–åŠ±
    print("\nğŸ“ æµ‹è¯•æ¢ç´¢å¥–åŠ±:")
    state1 = MockState(1000, (100, 100), 2)  # æ–°ä½ç½®
    state2 = MockState(1100, (200, 100), 3)  # å¦ä¸€ä¸ªæ–°ä½ç½®
    state3 = MockState(1200, (100, 100), 1)  # å›åˆ°ç¬¬ä¸€ä¸ªä½ç½®
    
    action = [0.5, 0.3, 0]  # å‘å³ä¸Šç§»åŠ¨ï¼Œä¸åˆ†è£‚
    
    reward1, components1 = calculator.calculate_optimized_reward(state1, None, action, 10.0)
    print(f"ç¬¬ä¸€ä¸ªä½ç½®: æ€»å¥–åŠ±={reward1:.3f}, æ¢ç´¢å¥–åŠ±={components1.get('exploration_bonus', 0):.3f}")
    
    reward2, components2 = calculator.calculate_optimized_reward(state2, state1, action, 15.0)
    print(f"æ–°ä½ç½®: æ€»å¥–åŠ±={reward2:.3f}, æ¢ç´¢å¥–åŠ±={components2.get('exploration_bonus', 0):.3f}")
    
    reward3, components3 = calculator.calculate_optimized_reward(state3, state2, action, 12.0)
    print(f"å›åˆ°è€ä½ç½®: æ€»å¥–åŠ±={reward3:.3f}, æ¢ç´¢å¥–åŠ±={components3.get('exploration_bonus', 0):.3f}, å›é€€æƒ©ç½š={components3.get('backtrack_penalty', 0):.3f}")
    
    # æµ‹è¯•æ™ºèƒ½åˆ†è£‚
    print("\nğŸ”€ æµ‹è¯•æ™ºèƒ½åˆ†è£‚:")
    split_action = [0.1, 0.1, 2.0]  # åˆ†è£‚åŠ¨ä½œ
    state_with_food = MockState(3000, (300, 300), 5)  # é«˜åˆ†+å¤šé£Ÿç‰©
    reward_split, components_split = calculator.calculate_optimized_reward(state_with_food, state3, split_action, 8.0)
    print(f"æ™ºèƒ½åˆ†è£‚: æ€»å¥–åŠ±={reward_split:.3f}, åˆ†è£‚å¥–åŠ±={components_split.get('smart_split_bonus', 0):.3f}")
    
    # æµ‹è¯•æµªè´¹åˆ†è£‚
    bad_split_state = MockState(1500, (400, 400), 1)  # ä½åˆ†+å°‘é£Ÿç‰©
    reward_bad, components_bad = calculator.calculate_optimized_reward(bad_split_state, state_with_food, split_action, 5.0)
    print(f"æµªè´¹åˆ†è£‚: æ€»å¥–åŠ±={reward_bad:.3f}, åˆ†è£‚æƒ©ç½š={components_bad.get('waste_split_penalty', 0):.3f}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“Š æ¢ç´¢ç»Ÿè®¡:", calculator.get_exploration_stats())
    print("ğŸ“Š åˆ†è£‚ç»Ÿè®¡:", calculator.get_split_stats())
    
    print("\nâœ… ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¤– GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ (æ”¯æŒå¢å¼ºå¥–åŠ±)")
    print("ğŸ”¥ Geminiä¼˜åŒ–ç‰ˆæœ¬ - è§£å†³ç­–ç•¥å´©å¡Œé—®é¢˜")
    print("=" * 60)
    
    # æ˜¾ç¤ºGeminiä¼˜åŒ–ä¿¡æ¯
    if not RICH_AVAILABLE:
        print("ğŸ¯ Geminiåˆ†æä¼˜åŒ–äº®ç‚¹:")
        print("  âœ… é™ä½n_epochs (20â†’6-8) - é˜²æ­¢è¿‡æ‹Ÿåˆå¯¼è‡´çš„ç­–ç•¥å´©å¡Œ")
        print("  âœ… æé«˜ent_coef (0.005â†’0.01+) - å¢åŠ ç­–ç•¥å¤šæ ·æ€§å’Œæ¢ç´¢")
        print("  âœ… äº‹ä»¶é©±åŠ¨å¥–åŠ± - å¤§å¹…å¥–åŠ±Split/Ejecté«˜çº§åŠ¨ä½œ")
        print("  âœ… å¹³è¡¡clip_range - æé«˜å­¦ä¹ çµæ´»æ€§")
        print("  ğŸ”¥ ç›®æ ‡ï¼šæ™ºèƒ½ä½“å­¦ä¼šå¤šæ ·åŒ–ç­–ç•¥ï¼Œä½¿ç”¨æ‰€æœ‰åŠ¨ä½œç±»å‹")
        print()
    else:
        console = Console()
        optimization_panel = Panel(
            "[bold green]ğŸ”¥ Gemini AI æ·±åº¦åˆ†æä¼˜åŒ–[/bold green]\n\n" +
            "[yellow]ğŸ¯ ç­–ç•¥å´©å¡Œé—®é¢˜è¯Šæ–­[/yellow]:\n" +
            "  â€¢ è¶…é«˜n_epochs(20)å¯¼è‡´ä¸¥é‡è¿‡æ‹Ÿåˆ\n" +
            "  â€¢ ä½ent_coef(0.005)æŠ‘åˆ¶æ¢ç´¢å¤šæ ·æ€§\n" +
            "  â€¢ æ™ºèƒ½ä½“åªä¼šå•ä¸€æ–¹å‘ç§»åŠ¨+ä»ä¸Split/Eject\n\n" +
            "[yellow]âœ¨ ä¼˜åŒ–æªæ–½[/yellow]:\n" +
            "  â€¢ [green]é™ä½n_epochs(20â†’6-8)[/green] - é˜²æ­¢è¿‡æ‹Ÿåˆ\n" +
            "  â€¢ [green]æé«˜ent_coef(0.005â†’0.01+)[/green] - å¢åŠ ç­–ç•¥å¤šæ ·æ€§\n" +
            "  â€¢ [green]äº‹ä»¶é©±åŠ¨å¥–åŠ±[/green] - é‡å¥–Split(+2.0)/Eject(+1.5)åŠ¨ä½œ\n" +
            "  â€¢ [green]å¹³è¡¡clip_range[/green] - æé«˜å­¦ä¹ çµæ´»æ€§\n\n" +
            "[bold cyan]ğŸ¯ é¢„æœŸæ•ˆæœ[/bold cyan]: æ™ºèƒ½ä½“å­¦ä¼šå¤šæ ·åŒ–ç­–ç•¥ï¼Œä¸»åŠ¨ä½¿ç”¨æ‰€æœ‰åŠ¨ä½œç±»å‹",
            title="Gemini AI ä¼˜åŒ–ç‰ˆæœ¬",
            border_style="red"
        )
        console.print(optimization_panel)
        print()
    
    if not RICH_AVAILABLE:
        print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")
        print("   å½“å‰ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢")
        print()
    else:
        console = Console()
        welcome_panel = Panel(
            "[bold green]ğŸ‰ æ¬¢è¿ä½¿ç”¨ GoBigger RL è®­ç»ƒå™¨[/bold green]\n\n" +
            "[yellow]âœ¨ Richç•Œé¢æ”¯æŒ[/yellow]: ç¾åŒ–ç•Œé¢ã€è¿›åº¦æ¡ã€å®æ—¶ç»Ÿè®¡\n" +
            "[yellow]ğŸ“Š TensorBoardé›†æˆ[/yellow]: è¯¦ç»†è®­ç»ƒæ›²çº¿ç›‘æ§\n" +
            "[yellow]ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿ[/yellow]: å¤šç»´åº¦å¯†é›†å¥–åŠ±ä¿¡å·\n" +
            "[yellow]ğŸš€ å¤šç®—æ³•æ”¯æŒ[/yellow]: PPOã€DQNã€A2C\n" +
            "[yellow]â° é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–[/yellow]: è‡ªåŠ¨ä¿å­˜ã€å­¦ä¹ ç‡è¡°å‡ã€ç½‘ç»œä¼˜åŒ–",
            title="åŠŸèƒ½ç‰¹è‰²",
            border_style="green"
        )
        console.print(welcome_panel)
        print()
    
    # è®­ç»ƒé…ç½®
    config = {
        'max_episode_steps': 1500,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    
    if STABLE_BASELINES_AVAILABLE:
        print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼ (Geminiä¼˜åŒ–ç‰ˆæœ¬):")
        print("1. PPO + æ ‡å‡†å¥–åŠ± - ç»å…¸å¼ºåŒ–å­¦ä¹  + äº‹ä»¶é©±åŠ¨å¥–åŠ±")
        print("2. PPO + å¢å¼ºå¥–åŠ± - å¯†é›†å¥–åŠ±ä¿¡å· + Split/Ejectæ¿€åŠ± (æ¨è)")
        print("3. ğŸŒ™ PPO + å¢å¼ºå¥–åŠ± - é•¿æ—¶é—´è®­ç»ƒ (2Mæ­¥, ä¸€æ•´æ™š)")
        print("4. ğŸŒŸ PPO + å¢å¼ºå¥–åŠ± - è¶…é•¿æ—¶é—´è®­ç»ƒ (4Mæ­¥, å‘¨æœ«è®­ç»ƒ)")
        print("5. DQN + æ ‡å‡†å¥–åŠ±")
        print("6. DQN + å¢å¼ºå¥–åŠ±")
        print("7. A2C + æ ‡å‡†å¥–åŠ±")
        print("8. A2C + å¢å¼ºå¥–åŠ±") 
        print("9. è¯„ä¼°ç°æœ‰æ¨¡å‹")
        print("10. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("11. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        print("")
        print("ğŸ’¡ Geminiä¼˜åŒ–è¯´æ˜:")
        print("   - æ‰€æœ‰PPOæ¨¡å¼å‡å·²ä¼˜åŒ–è¶…å‚æ•°ï¼Œé˜²æ­¢ç­–ç•¥å´©å¡Œ")
        print("   - å¢åŠ äº†Split(+2.0)å’ŒEject(+1.5)åŠ¨ä½œçš„äº‹ä»¶å¥–åŠ±")
        print("   - æé«˜æ¢ç´¢å¤šæ ·æ€§ï¼Œé¿å…å•ä¸€æ–¹å‘ç§»åŠ¨")
        
        choice = input("\nè¯·é€‰æ‹© (1-11): ").strip()
        
        if choice == '1':
            model = train_with_stable_baselines3('PPO', total_timesteps=1000000, config=config, use_enhanced_reward=False)
        elif choice == '2':
            model = train_with_stable_baselines3('PPO', total_timesteps=1000000, config=config, use_enhanced_reward=True)
        elif choice == '3':
            print("ğŸŒ™ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼ - é€‚åˆä¸€æ•´æ™šè®­ç»ƒ")
            print("   - è®­ç»ƒæ­¥æ•°: 2,000,000 (çº¦8-12å°æ—¶)")
            print("   - ç½‘ç»œç»“æ„: æ·±åº¦ä¼˜åŒ– [256, 256, 128]")
            print("   - å­¦ä¹ ç‡è¡°å‡: å¯ç”¨")
            print("   - è‡ªåŠ¨ä¿å­˜: æ¯10Kæ­¥") 
            print("   - æ£€æŸ¥ç‚¹: æ¯50Kæ­¥")
            confirm = input("ç¡®è®¤å¼€å§‹é•¿æ—¶é—´è®­ç»ƒ? (y/N): ").strip().lower()
            if confirm == 'y':
                model = train_with_stable_baselines3('PPO', total_timesteps=2000000, config=config, use_enhanced_reward=True)
            else:
                print("âŒ å–æ¶ˆè®­ç»ƒ")
                return
        elif choice == '4':
            print("ğŸŒŸ è¶…é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼ - é€‚åˆå‘¨æœ«è®­ç»ƒ")
            print("   - è®­ç»ƒæ­¥æ•°: 4,000,000 (çº¦20-30å°æ—¶)")
            print("   - ç½‘ç»œç»“æ„: æ·±åº¦ä¼˜åŒ– [512, 512, 256, 128]")
            print("   - å­¦ä¹ ç‡è¡°å‡: å¯ç”¨ (æ›´ä¿å®ˆçš„è¡°å‡)")
            print("   - è‡ªåŠ¨ä¿å­˜: æ¯25Kæ­¥")
            print("   - æ£€æŸ¥ç‚¹: æ¯100Kæ­¥")
            print("   - è¶…å‚æ•°: ä¸“ä¸ºé•¿æœŸè®­ç»ƒä¼˜åŒ–")
            print("   âš ï¸  è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¶é—´å’Œè®¡ç®—èµ„æº")
            confirm = input("ç¡®è®¤å¼€å§‹è¶…é•¿æ—¶é—´è®­ç»ƒ? (y/N): ").strip().lower()
            if confirm == 'y':
                model = train_with_stable_baselines3('PPO', total_timesteps=4000000, config=config, use_enhanced_reward=True)
            else:
                print("âŒ å–æ¶ˆè®­ç»ƒ")
                return
        elif choice == '5':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '6':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '7':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '8':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '9':
            model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
            if os.path.exists(model_path):
                evaluate_model(model_path)
            else:
                print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        elif choice == '10':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '11':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    else:
        print("ğŸ¯ é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("2. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        
        choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
        
        if choice == '1':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '2':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ é•¿æ—¶é—´è®­ç»ƒæç¤ºï¼š")
    print("  - ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    print("  - ğŸŒ TensorBoardè®¿é—®: http://localhost:6006")
    print("  - ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: ./models/")
    print("  - ğŸ’¾ æ£€æŸ¥ç‚¹ä½ç½®: ./checkpoints/")
    print("  - ğŸ”„ è‡ªåŠ¨ä¿å­˜: æ¯10Kæ­¥è‡ªåŠ¨ä¿å­˜ï¼Œé˜²æ­¢æ„å¤–ä¸¢å¤±")
    print("  - ğŸ“ˆ å­¦ä¹ ç‡è¡°å‡: é•¿æ—¶é—´è®­ç»ƒä¼šè‡ªåŠ¨ä¼˜åŒ–å­¦ä¹ ç‡")
    print("  - ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œæ¨èç”¨äºæ–°è®­ç»ƒ")
    print("  - ğŸŒ™ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼é€‚åˆä¸€æ•´æ™šè®­ç»ƒï¼Œè·å¾—æ›´å¥½çš„æ€§èƒ½")
    if not RICH_AVAILABLE:
        print("  - âœ¨ å»ºè®®å®‰è£… rich åº“äº«å—æ›´å¥½çš„è®­ç»ƒç•Œé¢ä½“éªŒ: pip install rich")

def test_optimized_reward_system():
    """
    ğŸ§ª æµ‹è¯•ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿçš„åŠŸèƒ½
    """
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–å¥–åŠ±è®¡ç®—å™¨...")
    
    calculator = OptimizedRewardCalculator()
    
    # æ¨¡æ‹Ÿä¸€äº›çŠ¶æ€ç”¨äºæµ‹è¯•
    class MockState:
        def __init__(self, score, position, food_count=0):
            self.score = score
            self.rectangle = [position[0]-10, position[1]-10, position[0]+10, position[1]+10]
            self.food = [f"food_{i}" for i in range(food_count)]
            self.clone = ["main_cell"]
    
    # æµ‹è¯•æ¢ç´¢å¥–åŠ±
    print("\nğŸ“ æµ‹è¯•æ¢ç´¢å¥–åŠ±:")
    state1 = MockState(1000, (100, 100), 2)  # æ–°ä½ç½®
    state2 = MockState(1100, (200, 100), 3)  # å¦ä¸€ä¸ªæ–°ä½ç½®
    state3 = MockState(1200, (100, 100), 1)  # å›åˆ°ç¬¬ä¸€ä¸ªä½ç½®
    
    action = [0.5, 0.3, 0]  # å‘å³ä¸Šç§»åŠ¨ï¼Œä¸åˆ†è£‚
    
    reward1, components1 = calculator.calculate_optimized_reward(state1, None, action, 10.0)
    print(f"ç¬¬ä¸€ä¸ªä½ç½®: æ€»å¥–åŠ±={reward1:.3f}, æ¢ç´¢å¥–åŠ±={components1.get('exploration_bonus', 0):.3f}")
    
    reward2, components2 = calculator.calculate_optimized_reward(state2, state1, action, 15.0)
    print(f"æ–°ä½ç½®: æ€»å¥–åŠ±={reward2:.3f}, æ¢ç´¢å¥–åŠ±={components2.get('exploration_bonus', 0):.3f}")
    
    reward3, components3 = calculator.calculate_optimized_reward(state3, state2, action, 12.0)
    print(f"å›åˆ°è€ä½ç½®: æ€»å¥–åŠ±={reward3:.3f}, æ¢ç´¢å¥–åŠ±={components3.get('exploration_bonus', 0):.3f}, å›é€€æƒ©ç½š={components3.get('backtrack_penalty', 0):.3f}")
    
    # æµ‹è¯•æ™ºèƒ½åˆ†è£‚
    print("\nğŸ”€ æµ‹è¯•æ™ºèƒ½åˆ†è£‚:")
    split_action = [0.1, 0.1, 2.0]  # åˆ†è£‚åŠ¨ä½œ
    state_with_food = MockState(3000, (300, 300), 5)  # é«˜åˆ†+å¤šé£Ÿç‰©
    reward_split, components_split = calculator.calculate_optimized_reward(state_with_food, state3, split_action, 8.0)
    print(f"æ™ºèƒ½åˆ†è£‚: æ€»å¥–åŠ±={reward_split:.3f}, åˆ†è£‚å¥–åŠ±={components_split.get('smart_split_bonus', 0):.3f}")
    
    # æµ‹è¯•æµªè´¹åˆ†è£‚
    bad_split_state = MockState(1500, (400, 400), 1)  # ä½åˆ†+å°‘é£Ÿç‰©
    reward_bad, components_bad = calculator.calculate_optimized_reward(bad_split_state, state_with_food, split_action, 5.0)
    print(f"æµªè´¹åˆ†è£‚: æ€»å¥–åŠ±={reward_bad:.3f}, åˆ†è£‚æƒ©ç½š={components_bad.get('waste_split_penalty', 0):.3f}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“Š æ¢ç´¢ç»Ÿè®¡:", calculator.get_exploration_stats())
    print("ğŸ“Š åˆ†è£‚ç»Ÿè®¡:", calculator.get_split_stats())
    
    print("\nâœ… ä¼˜åŒ–å¥–åŠ±ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œæµ‹è¯•
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test-reward":
        test_optimized_reward_system()
    else:
        main()
