#!/usr/bin/env python3
"""
ä½¿ç”¨GoBiggeræ ¸å¿ƒå¼•æ“è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
æ”¯æŒå¤šç§RLç®—æ³•ï¼šPPOã€DQNã€A2Cç­‰
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import json
from collections import deque
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥richåº“ç”¨äºç¾åŒ–ç•Œé¢
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.layout import Layout
    from rich.panel import Panel
    from rich.live import Live
    from rich.text import Text
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")

# è·¯å¾„è®¾ç½®ï¼šå®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from gobigger_gym_env import GoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
    from stable_baselines3.common.logger import configure
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒï¼ˆå¸¦Richç•Œé¢ï¼‰"""
    
    def __init__(self, eval_freq=1000, save_freq=5000, verbose=1, total_timesteps=50000):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.best_mean_reward = -np.inf
        self.episode_rewards = deque(maxlen=100)
        self.episode_scores = deque(maxlen=100)
        self.total_timesteps = total_timesteps
        
        # Richç•Œé¢ç»„ä»¶
        if RICH_AVAILABLE:
            self.console = Console()
            self.training_table = Table(title="ğŸ¤– GoBigger RL Training Status")
            self.training_table.add_column("Metric", style="cyan", no_wrap=True)
            self.training_table.add_column("Value", style="magenta")
            
            # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
            self.training_stats = {
                "ep_len_mean": 0,
                "ep_rew_mean": 0,
                "ep_score_mean": 0,
                "fps": 0,
                "iterations": 0,
                "time_elapsed": 0,
                "total_timesteps": 0,
                "approx_kl": 0,
                "clip_fraction": 0,
                "entropy_loss": 0,
                "learning_rate": 0,
                "loss": 0,
                "policy_gradient_loss": 0,
                "value_loss": 0,
                "episodes_completed": 0
            }
            
            self.start_time = time.time()
            self.last_table_update = 0
            self.table_update_interval = 5  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡è¡¨æ ¼
        
    def _update_training_table(self):
        """æ›´æ–°è®­ç»ƒçŠ¶æ€è¡¨æ ¼"""
        if not RICH_AVAILABLE:
            return
            
        # æ¸…é™¤æ—§è¡¨æ ¼å†…å®¹
        self.training_table = Table(title="ğŸ¤– GoBigger RL Training Status")
        self.training_table.add_column("Category", style="cyan", no_wrap=True)
        self.training_table.add_column("Metric", style="blue", no_wrap=True)
        self.training_table.add_column("Value", style="magenta")
        
        # Rollout metrics
        self.training_table.add_row("rollout/", "ep_len_mean", f"{self.training_stats['ep_len_mean']:.0f}")
        self.training_table.add_row("", "ep_rew_mean", f"{self.training_stats['ep_rew_mean']:.2f}")
        self.training_table.add_row("", "ep_score_mean", f"{self.training_stats['ep_score_mean']:.0f}")
        
        # Time metrics
        self.training_table.add_row("time/", "fps", f"{self.training_stats['fps']:.0f}")
        self.training_table.add_row("", "iterations", f"{self.training_stats['iterations']}")
        self.training_table.add_row("", "time_elapsed", f"{self.training_stats['time_elapsed']:.0f}")
        self.training_table.add_row("", "total_timesteps", f"{self.training_stats['total_timesteps']}")
        
        # Training metrics
        if self.training_stats['approx_kl'] > 0:
            self.training_table.add_row("train/", "approx_kl", f"{self.training_stats['approx_kl']:.6f}")
            self.training_table.add_row("", "clip_fraction", f"{self.training_stats['clip_fraction']:.4f}")
            self.training_table.add_row("", "entropy_loss", f"{self.training_stats['entropy_loss']:.2f}")
            self.training_table.add_row("", "learning_rate", f"{self.training_stats['learning_rate']:.6f}")
            self.training_table.add_row("", "loss", f"{self.training_stats['loss']:.6f}")
            self.training_table.add_row("", "policy_gradient_loss", f"{self.training_stats['policy_gradient_loss']:.6f}")
            self.training_table.add_row("", "value_loss", f"{self.training_stats['value_loss']:.6f}")
        
        # Episode info
        self.training_table.add_row("episodes/", "completed", f"{self.training_stats['episodes_completed']}")
        
    def _on_step(self) -> bool:
        # æ”¶é›†å¥–åŠ±å’Œåˆ†æ•°ç»Ÿè®¡
        for info in self.locals['infos']:
            if 'final_score' in info:
                final_score = info['final_score']
                score_delta = info.get('score_delta', 0)
                episode_length = info.get('episode_length', 0)
                
                self.episode_scores.append(final_score)
                self.training_stats['episodes_completed'] += 1
                
                if not RICH_AVAILABLE and self.verbose > 0:
                    print(f"ğŸ¯ Episode ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                          f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ­¥æ•°: {episode_length}")
            
            if 'episode' in info:
                episode_reward = info['episode']['r']
                self.episode_rewards.append(episode_reward)
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        if RICH_AVAILABLE:
            current_time = time.time()
            self.training_stats['time_elapsed'] = current_time - self.start_time
            self.training_stats['total_timesteps'] = self.num_timesteps
            
            # ä»loggerè·å–è®­ç»ƒæŒ‡æ ‡
            if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
                logger_data = self.model.logger.name_to_value
                
                # Rollout metrics
                if 'rollout/ep_len_mean' in logger_data:
                    self.training_stats['ep_len_mean'] = logger_data['rollout/ep_len_mean']
                if 'rollout/ep_rew_mean' in logger_data:
                    self.training_stats['ep_rew_mean'] = logger_data['rollout/ep_rew_mean']
                
                # Time metrics
                if 'time/fps' in logger_data:
                    self.training_stats['fps'] = logger_data['time/fps']
                if 'time/iterations' in logger_data:
                    self.training_stats['iterations'] = logger_data['time/iterations']
                
                # Training metrics
                if 'train/approx_kl' in logger_data:
                    self.training_stats['approx_kl'] = logger_data['train/approx_kl']
                if 'train/clip_fraction' in logger_data:
                    self.training_stats['clip_fraction'] = logger_data['train/clip_fraction']
                if 'train/entropy_loss' in logger_data:
                    self.training_stats['entropy_loss'] = logger_data['train/entropy_loss']
                if 'train/learning_rate' in logger_data:
                    self.training_stats['learning_rate'] = logger_data['train/learning_rate']
                if 'train/loss' in logger_data:
                    self.training_stats['loss'] = logger_data['train/loss']
                if 'train/policy_gradient_loss' in logger_data:
                    self.training_stats['policy_gradient_loss'] = logger_data['train/policy_gradient_loss']
                if 'train/value_loss' in logger_data:
                    self.training_stats['value_loss'] = logger_data['train/value_loss']
            
            # æ›´æ–°å¹³å‡åˆ†æ•°
            if len(self.episode_scores) > 0:
                self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
        
        # å®šæœŸè¯„ä¼°å’Œä¿å­˜ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¹²æ‰°ç•Œé¢ï¼‰
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards)
                mean_score = np.mean(self.episode_scores) if len(self.episode_scores) > 0 else 0
                
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    if not RICH_AVAILABLE and self.verbose > 0:
                        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.save_freq == 0:
            model_path = f"checkpoints/model_{self.num_timesteps}_steps.zip"
            self.model.save(model_path)
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
        
        return True

def create_env(config=None):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
    default_config = {
        'max_episode_steps': 2000,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    if config:
        default_config.update(config)
    
    return GoBiggerEnv(default_config)

def train_with_stable_baselines3(algorithm='PPO', total_timesteps=100000, config=None):
    """ä½¿ç”¨stable-baselines3è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆå¸¦Richç•Œé¢ï¼‰"""
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {algorithm} ç®—æ³•è®­ç»ƒ...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = make_vec_env(lambda: create_env(config), n_envs=1, vec_env_cls=DummyVecEnv)
    
    # åˆ›å»ºæ¨¡å‹
    if algorithm == 'PPO':
        model = PPO(
            "MlpPolicy", 
            env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=0 if RICH_AVAILABLE else 1,  # Richç•Œé¢æ—¶é™é»˜æ¨¡å¼
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'DQN':
        model = DQN(
            "MlpPolicy",
            env,
            learning_rate=1e-4,
            buffer_size=50000,
            learning_starts=1000,
            batch_size=32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            gradient_steps=1,
            target_update_interval=1000,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'A2C':
        model = A2C(
            "MlpPolicy",
            env,
            learning_rate=7e-4,
            n_steps=5,
            gamma=0.99,
            gae_lambda=1.0,
            ent_coef=0.01,
            vf_coef=0.25,
            max_grad_norm=0.5,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # åˆ›å»ºå›è°ƒ
    os.makedirs("checkpoints", exist_ok=True)
    
    if RICH_AVAILABLE:
        # åˆ›å»ºRichç•Œé¢ç‰ˆæœ¬çš„å›è°ƒ
        class RichTrainingCallback(TrainingCallback):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                
            def _on_step(self):
                # æ›´æ–°è®­ç»ƒç»Ÿè®¡
                current_time = time.time()
                self.training_stats['time_elapsed'] = current_time - self.start_time
                self.training_stats['total_timesteps'] = self.num_timesteps
                
                # ä»æ¨¡å‹è·å–è®­ç»ƒæŒ‡æ ‡
                if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                    logger_data = self.model.logger.name_to_value
                    
                    # æ›´æ–°å„é¡¹æŒ‡æ ‡
                    for key, value in logger_data.items():
                        if key in ['rollout/ep_len_mean', 'rollout/ep_rew_mean', 'time/fps', 
                                  'time/iterations', 'train/approx_kl', 'train/clip_fraction',
                                  'train/entropy_loss', 'train/learning_rate', 'train/loss',
                                  'train/policy_gradient_loss', 'train/value_loss']:
                            stat_name = key.split('/')[-1]
                            if stat_name in self.training_stats:
                                self.training_stats[stat_name] = value
                
                # æ›´æ–°å¹³å‡åˆ†æ•°
                if len(self.episode_scores) > 0:
                    self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
                
                # å®šæœŸæ›´æ–°æ˜¾ç¤º
                if current_time - self.last_table_update > self.table_update_interval:
                    self._update_training_table()
                    self.last_table_update = current_time
                    
                    # æ˜¾ç¤ºå½“å‰è¿›åº¦
                    progress_percent = (self.num_timesteps / self.total_timesteps) * 100
                    self.console.clear()
                    self.console.print(self.training_table)
                    self.console.print(f"\nğŸ“ˆ è®­ç»ƒè¿›åº¦: {progress_percent:.1f}% "
                                     f"({self.num_timesteps:,}/{self.total_timesteps:,} steps)")
                    
                    if len(self.episode_scores) > 0:
                        latest_score = self.episode_scores[-1] if self.episode_scores else 0
                        self.console.print(f"ğŸ¯ æœ€æ–°Episodeåˆ†æ•°: {latest_score:.0f}")
                
                return super()._on_step()
        
        callback = RichTrainingCallback(eval_freq=2000, save_freq=10000, total_timesteps=total_timesteps)
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
        print("ğŸ’¡ è®­ç»ƒç•Œé¢å°†å®šæœŸæ›´æ–°...")
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            tb_log_name=f"{algorithm}_gobigger"
        )
        
        train_time = time.time() - start_time
        callback.console.print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’", style="green bold")
        
    else:
        # ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢è®­ç»ƒ
        callback = TrainingCallback(eval_freq=2000, save_freq=10000, total_timesteps=total_timesteps)
        
        print(f"ğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            tb_log_name=f"{algorithm}_gobigger"
        )
        
        train_time = time.time() - start_time
        print(f"âœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f"models/{algorithm}_gobigger_final.zip"
    os.makedirs("models", exist_ok=True)
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def simple_random_training(episodes=100):
    """ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤ºï¼ˆå½“æ²¡æœ‰stable-baselines3æ—¶ï¼‰"""
    print("ğŸ® è¿è¡Œéšæœºç­–ç•¥æ¼”ç¤ºè®­ç»ƒ...")
    
    env = create_env({'max_episode_steps': 500})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # éšæœºåŠ¨ä½œ
            action = np.random.uniform(env.action_space_low, env.action_space_high)
            action[2] = int(action[2])  # åŠ¨ä½œç±»å‹ä¸ºæ•´æ•°
            
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    if (episode + 1) % 10 == 0:  # æ¯10ä¸ªepisodeæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                        print(f"  Episode {episode + 1} ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                              f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ€»å¥–åŠ±: {total_reward:.3f}")
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards)
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(episode_lengths)
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    
    plt.tight_layout()
    plt.savefig('random_training_results.png')
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° random_training_results.png")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-20:]):.3f}")

def evaluate_model(model_path, episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½å’Œè¯„ä¼°æ¨¡å‹")
        return
    
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    if 'PPO' in model_path:
        model = PPO.load(model_path)
    elif 'DQN' in model_path:
        model = DQN.load(model_path)
    elif 'A2C' in model_path:
        model = A2C.load(model_path)
    else:
        print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env({'max_episode_steps': 1000})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}, "
                          f"æœ€ç»ˆåˆ†æ•°={final_score:.2f}, åˆ†æ•°å˜åŒ–={score_delta:+.2f}")
                else:
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}")
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¤– GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨")
    print("=" * 50)
    
    if not RICH_AVAILABLE:
        print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")
        print("   å½“å‰ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢")
        print()
    else:
        console = Console()
        console.print("âœ¨ æ£€æµ‹åˆ° Rich åº“ï¼Œä½¿ç”¨ç¾åŒ–ç•Œé¢", style="green bold")
        print()
    
    # è®­ç»ƒé…ç½®
    config = {
        'max_episode_steps': 1500,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    
    if STABLE_BASELINES_AVAILABLE:
        print("ğŸ¯ é€‰æ‹©è®­ç»ƒç®—æ³•:")
        print("1. PPO (æ¨è) - Proximal Policy Optimization")
        print("2. DQN - Deep Q-Network") 
        print("3. A2C - Advantage Actor-Critic")
        print("4. è¯„ä¼°ç°æœ‰æ¨¡å‹")
        print("5. éšæœºç­–ç•¥æ¼”ç¤º")
        
        choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            model = train_with_stable_baselines3('PPO', total_timesteps=50000, config=config)
        elif choice == '2':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config)
        elif choice == '3':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config)
        elif choice == '4':
            model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
            if os.path.exists(model_path):
                evaluate_model(model_path)
            else:
                print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        elif choice == '5':
            simple_random_training(episodes=50)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    else:
        simple_random_training(episodes=50)
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼š")
    print("  - ä½¿ç”¨ tensorboard --logdir ./tensorboard_logs æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    print("  - æ¨¡å‹ä¿å­˜åœ¨ ./models/ ç›®å½•")
    print("  - æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ ./checkpoints/ ç›®å½•")
    if not RICH_AVAILABLE:
        print("  - å»ºè®®å®‰è£… rich åº“äº«å—æ›´å¥½çš„è®­ç»ƒç•Œé¢ä½“éªŒ: pip install rich")

if __name__ == "__main__":
    main()
