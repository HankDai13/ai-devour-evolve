#!/usr/bin/env python3
"""
ä½¿ç”¨GoBiggeræ ¸å¿ƒå¼•æ“è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
æ”¯æŒå¤šç§RLç®—æ³•ï¼šPPOã€DQNã€A2Cç­‰
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import json
from collections import deque
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥richåº“ç”¨äºç¾åŒ–ç•Œé¢
try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.layout import Layout
    from rich.panel import Panel
    from rich.live import Live
    from rich.text import Text
    from rich.align import Align
    from rich.columns import Columns
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")

def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºåŸç”ŸPythonç±»å‹ï¼Œè§£å†³JSONåºåˆ—åŒ–é—®é¢˜"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

# è·¯å¾„è®¾ç½®ï¼šå®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from gobigger_gym_env import GoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.monitor import Monitor
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±æ˜¾ç¤ºå’Œé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–ï¼‰"""
    
    def __init__(self, eval_freq=1000, save_freq=5000, verbose=1, total_timesteps=50000, 
                 auto_save_freq=10000, checkpoint_freq=25000, enable_lr_decay=False):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.auto_save_freq = auto_save_freq  # è‡ªåŠ¨ä¿å­˜é¢‘ç‡
        self.checkpoint_freq = checkpoint_freq  # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
        self.enable_lr_decay = enable_lr_decay  # å­¦ä¹ ç‡è¡°å‡
        self.best_mean_reward = -np.inf
        self.episode_rewards = deque(maxlen=100)
        self.episode_scores = deque(maxlen=100)
        self.total_timesteps = total_timesteps
        self.use_enhanced_reward = False  # é»˜è®¤å€¼ï¼Œä¼šåœ¨å¤–éƒ¨è®¾ç½®
        
        # é•¿æ—¶é—´è®­ç»ƒç›¸å…³
        self.initial_lr = None  # åˆå§‹å­¦ä¹ ç‡ï¼Œåœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®
        self.last_auto_save = 0
        self.last_checkpoint = 0
        
        # Richç•Œé¢ç»„ä»¶
        if RICH_AVAILABLE:
            self.console = Console()
            self.training_table = Table(title="ğŸ¤– GoBigger RL Training Status")
            self.training_table.add_column("Metric", style="cyan", no_wrap=True)
            self.training_table.add_column("Value", style="magenta")
            
            # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡
            self.training_stats = {
                "ep_len_mean": 0,
                "ep_rew_mean": 0,
                "ep_score_mean": 0,
                "fps": 0,
                "iterations": 0,
                "time_elapsed": 0,
                "total_timesteps": 0,
                "approx_kl": 0,
                "clip_fraction": 0,
                "entropy_loss": 0,
                "learning_rate": 0,
                "loss": 0,
                "policy_gradient_loss": 0,
                "value_loss": 0,
                "episodes_completed": 0
            }
            
            self.start_time = time.time()
            self.last_table_update = 0
            self.table_update_interval = 5  # æ¯5ç§’æ›´æ–°ä¸€æ¬¡è¡¨æ ¼
            
            # åˆ›å»ºè¿›åº¦æ¡
            self.progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=self.console,
                expand=True
            )
            self.progress_task = None
        
    def _update_training_table(self):
        """æ›´æ–°è®­ç»ƒçŠ¶æ€è¡¨æ ¼"""
        if not RICH_AVAILABLE:
            return
            
        # æ¸…é™¤æ—§è¡¨æ ¼å†…å®¹
        reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
        self.training_table = Table(title=f"ğŸ¤– GoBigger RL Training Status ({reward_type})")
        self.training_table.add_column("Category", style="cyan", no_wrap=True)
        self.training_table.add_column("Metric", style="blue", no_wrap=True)
        self.training_table.add_column("Value", style="magenta")
        
        # Rollout metrics
        self.training_table.add_row("rollout/", "ep_len_mean", f"{self.training_stats['ep_len_mean']:.0f}")
        self.training_table.add_row("", "ep_rew_mean", f"{self.training_stats['ep_rew_mean']:.2f}")
        self.training_table.add_row("", "ep_score_mean", f"{self.training_stats['ep_score_mean']:.0f}")
        
        # Time metrics
        self.training_table.add_row("time/", "fps", f"{self.training_stats['fps']:.0f}")
        self.training_table.add_row("", "iterations", f"{self.training_stats['iterations']}")
        self.training_table.add_row("", "time_elapsed", f"{self.training_stats['time_elapsed']:.0f}")
        self.training_table.add_row("", "total_timesteps", f"{self.training_stats['total_timesteps']}")
        
        # Training metrics
        if self.training_stats['approx_kl'] > 0:
            self.training_table.add_row("train/", "approx_kl", f"{self.training_stats['approx_kl']:.6f}")
            self.training_table.add_row("", "clip_fraction", f"{self.training_stats['clip_fraction']:.4f}")
            self.training_table.add_row("", "entropy_loss", f"{self.training_stats['entropy_loss']:.2f}")
            self.training_table.add_row("", "learning_rate", f"{self.training_stats['learning_rate']:.6f}")
            self.training_table.add_row("", "loss", f"{self.training_stats['loss']:.6f}")
            self.training_table.add_row("", "policy_gradient_loss", f"{self.training_stats['policy_gradient_loss']:.6f}")
            self.training_table.add_row("", "value_loss", f"{self.training_stats['value_loss']:.6f}")
        
        # Episode info
        self.training_table.add_row("episodes/", "completed", f"{self.training_stats['episodes_completed']}")
        
        # å¢å¼ºå¥–åŠ±ç³»ç»Ÿç‰¹æœ‰ä¿¡æ¯
        if self.use_enhanced_reward:
            self.training_table.add_row("", "", "")  # åˆ†éš”çº¿
            self.training_table.add_row("enhanced/", "system", "ğŸ¯ å¯†é›†å¥–åŠ±ä¿¡å·")
            self.training_table.add_row("", "components", "å¤šç»´åº¦å¥–åŠ±")
        
    def _on_step(self) -> bool:
        # æ”¶é›†å¥–åŠ±å’Œåˆ†æ•°ç»Ÿè®¡
        for info in self.locals['infos']:
            if 'final_score' in info:
                final_score = info['final_score']
                score_delta = info.get('score_delta', 0)
                episode_length = info.get('episode_length', 0)
                
                self.episode_scores.append(final_score)
                self.training_stats['episodes_completed'] += 1
                
                if not RICH_AVAILABLE and self.verbose > 0:
                    print(f"ğŸ¯ Episode ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                          f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ­¥æ•°: {episode_length}")
            
            # æ‰‹åŠ¨æ”¶é›†episode rewardä¿¡æ¯
            if 'episode' in info:
                episode_reward = info['episode']['r']
                episode_len = info['episode']['l']
                self.episode_rewards.append(episode_reward)
                
                # æ‰‹åŠ¨æ›´æ–°episodeç»Ÿè®¡
                if len(self.episode_rewards) > 0:
                    self.training_stats['ep_rew_mean'] = np.mean(self.episode_rewards)
                if len(self.episode_rewards) > 0:
                    # ä½¿ç”¨episodeé•¿åº¦ä¿¡æ¯
                    ep_lengths = [info['episode']['l'] for info in self.locals.get('infos', []) if 'episode' in info]
                    if ep_lengths:
                        self.training_stats['ep_len_mean'] = np.mean(ep_lengths)
                    else:
                        self.training_stats['ep_len_mean'] = episode_len
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        if RICH_AVAILABLE:
            current_time = time.time()
            self.training_stats['time_elapsed'] = current_time - self.start_time
            self.training_stats['total_timesteps'] = self.num_timesteps
            
            # ä»loggerè·å–è®­ç»ƒæŒ‡æ ‡
            if hasattr(self.model, 'logger') and self.model.logger.name_to_value:
                logger_data = self.model.logger.name_to_value
                
                # Rollout metrics
                if 'rollout/ep_len_mean' in logger_data:
                    self.training_stats['ep_len_mean'] = logger_data['rollout/ep_len_mean']
                if 'rollout/ep_rew_mean' in logger_data:
                    self.training_stats['ep_rew_mean'] = logger_data['rollout/ep_rew_mean']
                
                # Time metrics
                if 'time/fps' in logger_data:
                    self.training_stats['fps'] = logger_data['time/fps']
                if 'time/iterations' in logger_data:
                    self.training_stats['iterations'] = logger_data['time/iterations']
                
                # Training metrics
                if 'train/approx_kl' in logger_data:
                    self.training_stats['approx_kl'] = logger_data['train/approx_kl']
                if 'train/clip_fraction' in logger_data:
                    self.training_stats['clip_fraction'] = logger_data['train/clip_fraction']
                if 'train/entropy_loss' in logger_data:
                    self.training_stats['entropy_loss'] = logger_data['train/entropy_loss']
                if 'train/learning_rate' in logger_data:
                    self.training_stats['learning_rate'] = logger_data['train/learning_rate']
                if 'train/loss' in logger_data:
                    self.training_stats['loss'] = logger_data['train/loss']
                if 'train/policy_gradient_loss' in logger_data:
                    self.training_stats['policy_gradient_loss'] = logger_data['train/policy_gradient_loss']
                if 'train/value_loss' in logger_data:
                    self.training_stats['value_loss'] = logger_data['train/value_loss']
            
            # æ›´æ–°å¹³å‡åˆ†æ•°
            if len(self.episode_scores) > 0:
                self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
        
        # å®šæœŸè¯„ä¼°å’Œä¿å­˜ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¹²æ‰°ç•Œé¢ï¼‰
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards)
                mean_score = np.mean(self.episode_scores) if len(self.episode_scores) > 0 else 0
                
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    if not RICH_AVAILABLE and self.verbose > 0:
                        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.save_freq == 0:
            model_path = f"checkpoints/model_{self.num_timesteps}_steps.zip"
            self.model.save(model_path)
        
        # ğŸš€ é•¿æ—¶é—´è®­ç»ƒå¢å¼ºåŠŸèƒ½
        # è‡ªåŠ¨ä¿å­˜ï¼ˆæ›´é¢‘ç¹ï¼Œé˜²æ­¢æ„å¤–ä¸¢å¤±ï¼‰
        if self.num_timesteps - self.last_auto_save >= self.auto_save_freq:
            auto_save_path = f"models/auto_save_{self.num_timesteps}.zip"
            self.model.save(auto_save_path)
            self.last_auto_save = self.num_timesteps
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜: {auto_save_path}")
        
        # æ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆç”¨äºé•¿æ—¶é—´è®­ç»ƒæ¢å¤ï¼‰
        if self.num_timesteps - self.last_checkpoint >= self.checkpoint_freq:
            checkpoint_path = f"checkpoints/checkpoint_{self.num_timesteps}.zip"
            self.model.save(checkpoint_path)
            self.last_checkpoint = self.num_timesteps
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡ï¼ˆè½¬æ¢numpyç±»å‹ä»¥é¿å…JSONåºåˆ—åŒ–é”™è¯¯ï¼‰
            stats_path = f"checkpoints/stats_{self.num_timesteps}.json"
            stats_data = {
                'timesteps': self.num_timesteps,
                'best_reward': self.best_mean_reward,
                'episode_rewards': list(self.episode_rewards),
                'episode_scores': list(self.episode_scores),
                'training_stats': self.training_stats,
                'elapsed_hours': (time.time() - self.start_time) / 3600
            }
            # è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºåŸç”ŸPythonç±»å‹
            stats_data = convert_numpy_types(stats_data)
            
            with open(stats_path, 'w') as f:
                json.dump(stats_data, f, indent=2)
            
            if not RICH_AVAILABLE and self.verbose > 0:
                elapsed_hours = (time.time() - self.start_time) / 3600
                print(f"ğŸ“‹ æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_path} (è®­ç»ƒ {elapsed_hours:.1f} å°æ—¶)")
        
        # å­¦ä¹ ç‡è¡°å‡ï¼ˆé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–ï¼‰
        if self.enable_lr_decay and self.initial_lr is not None:
            progress = self.num_timesteps / self.total_timesteps
            # çº¿æ€§è¡°å‡åˆ°åˆå§‹å­¦ä¹ ç‡çš„10%
            new_lr = self.initial_lr * (1.0 - 0.9 * progress)
            if hasattr(self.model, 'lr_schedule'):
                # æ›´æ–°å­¦ä¹ ç‡
                if hasattr(self.model.policy.optimizer, 'param_groups'):
                    for param_group in self.model.policy.optimizer.param_groups:
                        param_group['lr'] = new_lr
            if not RICH_AVAILABLE and self.verbose > 0:
                print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
        
        return True

def create_env(config=None):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorï¼‰"""
    default_config = {
        'max_episode_steps': 2000,  # æ¯å±€æœ€å¤§æ­¥æ•°
        'use_enhanced_reward': False,  # é»˜è®¤ä½¿ç”¨ç®€å•å¥–åŠ±
        'debug_rewards': False,  # ğŸ”¥ æ–°å¢ï¼šå¥–åŠ±è°ƒè¯•é€‰é¡¹
    }
    if config:
        default_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(default_config)
    
    # ğŸ”¥ å¯ç”¨å¥–åŠ±è°ƒè¯•ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if default_config.get('debug_rewards', False):
        env.debug_rewards = True
        print("ğŸ” å¯ç”¨å¥–åŠ±è°ƒè¯•æ¨¡å¼ - å°†æ˜¾ç¤ºSplit/EjectåŠ¨ä½œå¥–åŠ±")
    
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

def create_enhanced_env(config=None):
    """åˆ›å»ºå¢å¼ºå¥–åŠ±è®­ç»ƒç¯å¢ƒï¼ˆå¸¦Monitorï¼‰"""
    enhanced_config = {
        'max_episode_steps': 2000,
        'use_enhanced_reward': True,  # å¯ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ
        'enhanced_reward_weights': {
            'score_growth': 2.0,        # åˆ†æ•°å¢é•¿å¥–åŠ±æƒé‡
            'efficiency': 1.5,          # æ•ˆç‡å¥–åŠ±æƒé‡
            'exploration': 0.8,         # æ¢ç´¢å¥–åŠ±æƒé‡
            'strategic_split': 2.0,     # æˆ˜ç•¥åˆ†è£‚å¥–åŠ±æƒé‡
            'food_density': 1.0,        # é£Ÿç‰©å¯†åº¦å¥–åŠ±æƒé‡
            'survival': 0.02,           # ç”Ÿå­˜å¥–åŠ±æƒé‡
            'time_penalty': -0.001,     # æ—¶é—´æƒ©ç½šæƒé‡
            'death_penalty': -20.0,     # æ­»äº¡æƒ©ç½šæƒé‡
        }
    }
    if config:
        enhanced_config.update(config)
    
    # åˆ›å»ºç¯å¢ƒå¹¶ç”¨MonitoråŒ…è£…ï¼ˆé‡è¦ï¼šè¿™æ˜¯episodeç»Ÿè®¡çš„å…³é”®ï¼‰
    env = GoBiggerEnv(enhanced_config)
    if STABLE_BASELINES_AVAILABLE:
        env = Monitor(env)
    return env

def train_with_stable_baselines3(algorithm='PPO', total_timesteps=100000, config=None, use_enhanced_reward=False):
    """ä½¿ç”¨stable-baselines3è®­ç»ƒæ™ºèƒ½ä½“ï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {algorithm} ç®—æ³•è®­ç»ƒ ({reward_type})...")
    
    # åˆ›å»ºç¯å¢ƒ - ä½¿ç”¨vectorized environmentä»¥æ­£ç¡®æ”¯æŒepisodeç»Ÿè®¡
    def make_env():
        if use_enhanced_reward:
            return create_enhanced_env(config)
        else:
            return create_env(config)
    
    if use_enhanced_reward:
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿ - æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")
    else:
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ")
    
    # ä½¿ç”¨make_vec_envæ¥æ­£ç¡®é›†æˆMonitorå’Œepisodeç»Ÿè®¡
    env = make_vec_env(make_env, n_envs=1, vec_env_cls=DummyVecEnv)
    
    # ğŸš€ é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„ç½‘ç»œé…ç½®
    def get_optimized_policy_kwargs(total_timesteps):
        """æ ¹æ®è®­ç»ƒè§„æ¨¡ä¼˜åŒ–ç½‘ç»œç»“æ„"""
        if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
            return dict(
                net_arch=[512, 512, 256, 128],  # æ›´æ·±æ›´å®½çš„ç½‘ç»œ
                activation_fn=torch.nn.ReLU,
                share_features_extractor=False
            )
        elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[256, 256, 128],  # æ›´æ·±çš„ç½‘ç»œ
                activation_fn=torch.nn.ReLU,
                share_features_extractor=False
            )
        elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[128, 128],
                activation_fn=torch.nn.ReLU
            )
        else:  # çŸ­æ—¶é—´è®­ç»ƒ
            return dict(
                net_arch=[64, 64],
                activation_fn=torch.nn.ReLU
            )
    
    try:
        import torch
        policy_kwargs = get_optimized_policy_kwargs(total_timesteps)
        print(f"ğŸ§  ç½‘ç»œç»“æ„: {policy_kwargs['net_arch']} (é€‚é… {total_timesteps:,} æ­¥è®­ç»ƒ)")
    except ImportError:
        policy_kwargs = None
        print("âš ï¸  æœªæ£€æµ‹åˆ°PyTorchï¼Œä½¿ç”¨é»˜è®¤ç½‘ç»œç»“æ„")
    
    # ğŸ¯ é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„è¶…å‚æ•°
    def get_optimized_hyperparams(algorithm, total_timesteps):
        """æ ¹æ®ç®—æ³•å’Œè®­ç»ƒè§„æ¨¡ä¼˜åŒ–è¶…å‚æ•°"""
        if algorithm == 'PPO':
            if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
                return {
                    "learning_rate": 2e-4,  # æ›´ä½çš„å­¦ä¹ ç‡ï¼Œæ›´ç¨³å®š
                    "n_steps": 2048,  # æ›´å¤§çš„rolloutï¼Œæ›´å¥½çš„é‡‡æ ·æ•ˆç‡
                    "batch_size": 256,  # æ›´å¤§çš„æ‰¹é‡ï¼Œæ›´ç¨³å®šçš„æ¢¯åº¦
                    "n_epochs": 6,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼(Geminiå»ºè®®4-10)
                    "gamma": 0.998,  # æ›´é«˜çš„æŠ˜æ‰£å› å­ï¼Œè€ƒè™‘é•¿æœŸå›æŠ¥
                    "gae_lambda": 0.99,  # æ›´é«˜çš„GAEå‚æ•°
                    "clip_range": 0.15,  # ç¨å¤§çš„è£å‰ªèŒƒå›´ï¼Œå¢åŠ å­¦ä¹ çµæ´»æ€§
                    "ent_coef": 0.015,  # ğŸ”¥ å¢åŠ ç†µç³»æ•°é¼“åŠ±æ¢ç´¢ï¼(Geminiå»ºè®®0.01-0.02)
                    "vf_coef": 0.25,  # ä»·å€¼å‡½æ•°ç³»æ•°
                    "max_grad_norm": 0.5  # ç¨å¾®æ”¾æ¾æ¢¯åº¦è£å‰ª
                }
            elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
                return {
                    "learning_rate": 2.5e-4,  # ç¨ä½çš„å­¦ä¹ ç‡
                    "n_steps": 1024,  # æ›´å¤§çš„rollout
                    "batch_size": 128,  # æ›´å¤§çš„æ‰¹é‡
                    "n_epochs": 8,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼(Geminiå»ºè®®4-10)
                    "gamma": 0.995,  # æ›´é«˜çš„æŠ˜æ‰£å› å­
                    "gae_lambda": 0.98,  # GAEå‚æ•°
                    "clip_range": 0.18,  # ç¨å¤§çš„è£å‰ªèŒƒå›´
                    "ent_coef": 0.012,  # ğŸ”¥ å¢åŠ ç†µç³»æ•°é¼“åŠ±æ¢ç´¢ï¼(Geminiå»ºè®®0.01-0.02)
                    "vf_coef": 0.5,  # ä»·å€¼å‡½æ•°ç³»æ•°
                    "max_grad_norm": 0.5  # æ¢¯åº¦è£å‰ª
                }
            elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
                return {
                    "learning_rate": 3e-4,
                    "n_steps": 512,
                    "batch_size": 64,
                    "n_epochs": 6,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.99,
                    "gae_lambda": 0.95,
                    "clip_range": 0.2,
                    "ent_coef": 0.01  # ğŸ”¥ å¢åŠ ç†µç³»æ•°é¼“åŠ±æ¢ç´¢ï¼
                }
            else:  # æ ‡å‡†è®­ç»ƒ
                return {
                    "learning_rate": 3e-4,
                    "n_steps": 256,
                    "batch_size": 32,
                    "n_epochs": 5,  # ğŸ”¥ å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆï¼
                    "gamma": 0.99,
                    "gae_lambda": 0.95,
                    "clip_range": 0.2,
                    "ent_coef": 0.01  # ğŸ”¥ å¢åŠ ç†µç³»æ•°é¼“åŠ±æ¢ç´¢ï¼
                }
        return {}
    
    hyperparams = get_optimized_hyperparams(algorithm, total_timesteps)
    
    # åˆ›å»ºæ¨¡å‹
    if algorithm == 'PPO':
        model = PPO(
            "MlpPolicy", 
            env,
            policy_kwargs=policy_kwargs,
            verbose=0 if RICH_AVAILABLE else 1,  # Richç•Œé¢æ—¶é™é»˜æ¨¡å¼
            tensorboard_log="./tensorboard_logs/",
            **hyperparams
        )
    elif algorithm == 'DQN':
        model = DQN(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=1e-4,
            buffer_size=min(100000, max(50000, total_timesteps // 10)),  # è‡ªé€‚åº”ç¼“å†²åŒº
            learning_starts=max(1000, total_timesteps // 100),
            batch_size=64 if total_timesteps >= 100000 else 32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            gradient_steps=1,
            target_update_interval=max(500, total_timesteps // 100),
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'A2C':
        model = A2C(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=7e-4,
            n_steps=8 if total_timesteps >= 100000 else 5,
            gamma=0.99,
            gae_lambda=1.0,
            ent_coef=0.01,
            vf_coef=0.25,
            max_grad_norm=0.5,
            verbose=0 if RICH_AVAILABLE else 1,
            tensorboard_log="./tensorboard_logs/"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # ğŸš€ åˆ›å»ºé•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–çš„å›è°ƒ
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs("models", exist_ok=True)
    
    # åŠ¨æ€è°ƒæ•´å›è°ƒé¢‘ç‡
    if total_timesteps >= 3000000:  # è¶…è¶…é•¿æ—¶é—´è®­ç»ƒï¼ˆ400ä¸‡æ­¥çº§åˆ«ï¼‰
        eval_freq = 10000
        save_freq = 50000
        auto_save_freq = 25000
        checkpoint_freq = 100000
        enable_lr_decay = True
        print(f"ğŸŒŸ è¶…é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}, è‡ªåŠ¨ä¿å­˜={auto_save_freq}")
        print(f"   é¢„è®¡è®­ç»ƒæ—¶é—´: 20-30å°æ—¶ (é€‚åˆå‘¨æœ«é•¿æ—¶é—´è®­ç»ƒ)")
    elif total_timesteps >= 1000000:  # è¶…é•¿æ—¶é—´è®­ç»ƒ
        eval_freq = 5000
        save_freq = 25000
        auto_save_freq = 10000
        checkpoint_freq = 50000
        enable_lr_decay = True
        print(f"ğŸ¯ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}, è‡ªåŠ¨ä¿å­˜={auto_save_freq}")
    elif total_timesteps >= 500000:  # é•¿æ—¶é—´è®­ç»ƒ
        eval_freq = 2500
        save_freq = 10000
        auto_save_freq = 5000
        checkpoint_freq = 25000
        enable_lr_decay = True
        print(f"ğŸ¯ ä¸­é•¿è®­ç»ƒæ¨¡å¼: è¯„ä¼°é—´éš”={eval_freq}, ä¿å­˜é—´éš”={save_freq}")
    else:  # æ ‡å‡†è®­ç»ƒ
        eval_freq = 1000
        save_freq = 5000
        auto_save_freq = 2500
        checkpoint_freq = 10000
        enable_lr_decay = False
    
    if RICH_AVAILABLE:
        # åˆ›å»ºRichç•Œé¢ç‰ˆæœ¬çš„å›è°ƒ
        class RichTrainingCallback(TrainingCallback):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.use_enhanced_reward = use_enhanced_reward  # åœ¨çˆ¶ç±»åˆå§‹åŒ–åè®¾ç½®
                
                # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼ˆç”¨äºå­¦ä¹ ç‡è¡°å‡ï¼‰
                if hasattr(model, 'learning_rate'):
                    if callable(model.learning_rate):
                        self.initial_lr = model.learning_rate(1.0)  # è·å–åˆå§‹å­¦ä¹ ç‡
                    else:
                        self.initial_lr = model.learning_rate
                    print(f"ğŸ“Š åˆå§‹å­¦ä¹ ç‡: {self.initial_lr}")
                
                # print(f"ğŸ” å›è°ƒåˆå§‹åŒ–è°ƒè¯•: use_enhanced_reward = {self.use_enhanced_reward}")
                
                # åˆå§‹åŒ–è¿›åº¦æ¡ä»»åŠ¡
                if hasattr(self, 'progress'):
                    reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                    if total_timesteps >= 3000000:
                        training_mode = "è¶…é•¿æ—¶é—´"
                    elif total_timesteps >= 1000000:
                        training_mode = "é•¿æ—¶é—´"
                    elif total_timesteps >= 500000:
                        training_mode = "ä¸­é•¿æ—¶é—´"
                    else:
                        training_mode = "æ ‡å‡†"
                    self.progress_task = self.progress.add_task(
                        f"[green]ğŸš€ {algorithm} {training_mode}è®­ç»ƒ ({reward_type})", 
                        total=total_timesteps
                    )
                
            def _on_step(self):
                # æ›´æ–°è®­ç»ƒç»Ÿè®¡
                current_time = time.time()
                self.training_stats['time_elapsed'] = current_time - self.start_time
                self.training_stats['total_timesteps'] = self.num_timesteps
                
                # ä»æ¨¡å‹è·å–è®­ç»ƒæŒ‡æ ‡
                if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                    logger_data = self.model.logger.name_to_value
                    
                    # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰loggeræ•°æ® (å¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨è°ƒè¯•)
                    # if self.num_timesteps % 1000 == 0:  # æ¯1000æ­¥æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                    #     print(f"\nğŸ” Loggerå®Œæ•´æ•°æ®è°ƒè¯• (æ­¥æ•°: {self.num_timesteps}):")
                    #     if logger_data:
                    #         for key, value in logger_data.items():
                    #             print(f"  {key}: {value}")
                    #     else:
                    #         print("  Loggeræ•°æ®ä¸ºç©º!")
                    #     
                    #     print(f"ğŸ” Training Statsè°ƒè¯•:")
                    #     for key, value in self.training_stats.items():
                    #         if value != 0:  # åªæ˜¾ç¤ºéé›¶å€¼
                    #             print(f"  {key}: {value}")
                    #     print()
                    
                    # æ›´æ–°å„é¡¹æŒ‡æ ‡
                    for key, value in logger_data.items():
                        if key in ['rollout/ep_len_mean', 'rollout/ep_rew_mean', 'time/fps', 
                                  'time/iterations', 'train/approx_kl', 'train/clip_fraction',
                                  'train/entropy_loss', 'train/learning_rate', 'train/loss',
                                  'train/policy_gradient_loss', 'train/value_loss']:
                            stat_name = key.split('/')[-1]
                            if stat_name in self.training_stats:
                                self.training_stats[stat_name] = value
                
                # æ›´æ–°å¹³å‡åˆ†æ•°
                if len(self.episode_scores) > 0:
                    self.training_stats['ep_score_mean'] = np.mean(self.episode_scores)
                
                # å®šæœŸæ›´æ–°æ˜¾ç¤º
                if current_time - self.last_table_update > self.table_update_interval:
                    self._update_training_table()
                    self.last_table_update = current_time
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if hasattr(self, 'progress') and self.progress_task is not None:
                        self.progress.update(self.progress_task, completed=self.num_timesteps)
                    
                    # åˆ›å»ºå¸ƒå±€ç»„ä»¶
                    reward_panel, tensorboard_panel, status_panel = self._create_training_layout()
                    
                    # æ¸…å±å¹¶æ˜¾ç¤º
                    self.console.clear()
                    
                    # æ˜¾ç¤ºä¸Šæ’é¢æ¿
                    top_row = Columns([reward_panel, tensorboard_panel], equal=True)
                    self.console.print(top_row)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡è¡¨æ ¼
                    stats_panel = Panel(
                        self.training_table,
                        title="è®­ç»ƒç»Ÿè®¡",
                        border_style="blue"
                    )
                    self.console.print(stats_panel)
                    self.console.print("")
                    
                    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
                    self.console.print(status_panel)
                
                return super()._on_step()
            
            def _create_training_layout(self):
                """åˆ›å»ºè®­ç»ƒç•Œé¢å¸ƒå±€"""
                # å¥–åŠ±ç³»ç»Ÿä¿¡æ¯é¢æ¿
                reward_type = "å¢å¼ºå¥–åŠ±" if self.use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
                reward_icon = "ğŸ¯" if self.use_enhanced_reward else "ğŸ“Š"
                reward_desc = "å¯†é›†å¥–åŠ±ä¿¡å·ï¼Œå¤šç»´åº¦åé¦ˆ" if self.use_enhanced_reward else "ç»å…¸å¼ºåŒ–å­¦ä¹ å¥–åŠ±"
                
                reward_panel = Panel(
                    f"[bold green]{reward_icon} {reward_type}ç³»ç»Ÿ[/bold green]\n[dim]{reward_desc}[/dim]",
                    title="å¥–åŠ±ç³»ç»Ÿ",
                    border_style="green"
                )
                
                # TensorBoardæç¤º
                tensorboard_panel = Panel(
                    "[bold cyan]ğŸ“Š TensorBoardç›‘æ§[/bold cyan]\n" +
                    "[dim]åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/dim]\n" +
                    "[yellow]tensorboard --logdir ./tensorboard_logs[/yellow]\n" +
                    "[dim]ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:6006[/dim]",
                    title="ç›‘æ§å·¥å…·",
                    border_style="cyan"
                )
                
                # çŠ¶æ€ä¿¡æ¯
                progress_percent = (self.num_timesteps / self.total_timesteps) * 100
                status_lines = []
                status_lines.append(f"[bold blue]è®­ç»ƒè¿›åº¦[/bold blue]: {progress_percent:.1f}% ({self.num_timesteps:,}/{self.total_timesteps:,} steps)")
                
                if len(self.episode_scores) > 0:
                    latest_score = self.episode_scores[-1]
                    avg_score = np.mean(self.episode_scores)
                    status_lines.append(f"[bold yellow]Episodeä¿¡æ¯[/bold yellow]: æœ€æ–°åˆ†æ•°={latest_score:.0f}, å¹³å‡åˆ†æ•°={avg_score:.0f}")
                
                if self.use_enhanced_reward:
                    status_lines.append("[dim]ğŸ’¡ å¢å¼ºå¥–åŠ±æä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·[/dim]")
                
                status_panel = Panel(
                    "\n".join(status_lines),
                    title="çŠ¶æ€ä¿¡æ¯",
                    border_style="magenta"
                )
                
                # åˆ›å»ºå®Œæ•´å¸ƒå±€å¹¶è¿”å›æ‰€æœ‰ç»„ä»¶
                return reward_panel, tensorboard_panel, status_panel
        
        callback = RichTrainingCallback(
            eval_freq=eval_freq, 
            save_freq=save_freq, 
            total_timesteps=total_timesteps,
            auto_save_freq=auto_save_freq,
            checkpoint_freq=checkpoint_freq,
            enable_lr_decay=enable_lr_decay
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸ“ˆ å¼€å§‹ä½¿ç”¨ {algorithm} è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
        print(f"ğŸ¯ å¥–åŠ±ç³»ç»Ÿ: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}")
        print("ï¿½ TensorBoardæ—¥å¿—: ./tensorboard_logs/")
        print("ï¿½ğŸ’¡ è®­ç»ƒç•Œé¢å°†æ¯5ç§’æ›´æ–°ä¸€æ¬¡...")
        print("\nğŸš€ å¯åŠ¨ TensorBoard ç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   ç„¶åè®¿é—®: http://localhost:6006")
        print("\n" + "="*60)
        start_time = time.time()
        
        # å¯åŠ¨è¿›åº¦æ¡
        with callback.progress:
            model.learn(
                total_timesteps=total_timesteps,
                callback=callback,
                tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
            )
        
        train_time = time.time() - start_time
        
        # è®­ç»ƒå®Œæˆä¿¡æ¯
        callback.console.clear()
        completion_panel = Panel(
            f"[bold green]âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼[/bold green]\n\n" +
            f"[yellow]è®­ç»ƒæ—¶é—´[/yellow]: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)\n" +
            f"[yellow]æ€»æ­¥æ•°[/yellow]: {total_timesteps:,}\n" +
            f"[yellow]å¥–åŠ±ç³»ç»Ÿ[/yellow]: {'å¢å¼ºå¥–åŠ±' if use_enhanced_reward else 'æ ‡å‡†å¥–åŠ±'}\n" +
            f"[yellow]ç®—æ³•[/yellow]: {algorithm}\n\n" +
            f"[cyan]ğŸ“Š æŸ¥çœ‹è¯¦ç»†è®­ç»ƒæ›²çº¿:[/cyan]\n" +
            f"[dim]tensorboard --logdir ./tensorboard_logs[/dim]\n\n" +
            f"[cyan]ğŸ“ æ¨¡å‹æ–‡ä»¶:[/cyan]\n" +
            f"[dim]./models/ å’Œ ./checkpoints/[/dim]",
            title="ğŸ‰ è®­ç»ƒå®Œæˆ",
            border_style="green"
        )
        callback.console.print(completion_panel)
        
    else:
        # ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢è®­ç»ƒ
        callback = TrainingCallback(
            eval_freq=eval_freq, 
            save_freq=save_freq, 
            total_timesteps=total_timesteps,
            auto_save_freq=auto_save_freq,
            checkpoint_freq=checkpoint_freq,
            enable_lr_decay=enable_lr_decay
        )
        callback.use_enhanced_reward = use_enhanced_reward
        
        # è®¾ç½®åˆå§‹å­¦ä¹ ç‡ï¼ˆç”¨äºå­¦ä¹ ç‡è¡°å‡ï¼‰
        if hasattr(model, 'learning_rate'):
            if callable(model.learning_rate):
                callback.initial_lr = model.learning_rate(1.0)
            else:
                callback.initial_lr = model.learning_rate
        
        reward_info = "å¢å¼ºå¥–åŠ±ç³»ç»Ÿ" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±ç³»ç»Ÿ"
        if total_timesteps >= 3000000:
            training_mode = "è¶…é•¿æ—¶é—´"
        elif total_timesteps >= 1000000:
            training_mode = "é•¿æ—¶é—´"
        elif total_timesteps >= 500000:
            training_mode = "ä¸­é•¿æ—¶é—´"
        else:
            training_mode = "æ ‡å‡†"
        print(f"ğŸ“ˆ å¼€å§‹{training_mode}è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps:,} ({reward_info})")
        if enable_lr_decay:
            print(f"ğŸ“Š å­¦ä¹ ç‡è¡°å‡: å¯ç”¨ (åˆå§‹LR: {callback.initial_lr})")
        print(f"ğŸ’¾ ä¿å­˜è®¾ç½®: æ£€æŸ¥ç‚¹æ¯{checkpoint_freq:,}æ­¥, è‡ªåŠ¨ä¿å­˜æ¯{auto_save_freq:,}æ­¥")
        if total_timesteps >= 3000000:
            print(f"â° é¢„è®¡è®­ç»ƒæ—¶é—´: 20-30å°æ—¶ (å»ºè®®å‘¨æœ«è¿›è¡Œ)")
        print("ğŸ“Š TensorBoardç›‘æ§:")
        print("   åœ¨æ–°ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./tensorboard_logs")
        print("   è®¿é—®: http://localhost:6006")
        print("="*50)
        start_time = time.time()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            tb_log_name=f"{algorithm}_gobigger_{'enhanced' if use_enhanced_reward else 'standard'}"
        )
        
        train_time = time.time() - start_time
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’ ({train_time/60:.1f}åˆ†é’Ÿ)")
        print("ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    reward_suffix = "_enhanced" if use_enhanced_reward else "_standard"
    final_model_path = f"models/{algorithm}_gobigger{reward_suffix}_final.zip"
    os.makedirs("models", exist_ok=True)
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def simple_random_training(episodes=100, use_enhanced_reward=False):
    """ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤ºï¼ˆæ”¯æŒå¢å¼ºå¥–åŠ±ç³»ç»Ÿï¼‰"""
    reward_type = "å¢å¼ºå¥–åŠ±" if use_enhanced_reward else "æ ‡å‡†å¥–åŠ±"
    print(f"ğŸ® è¿è¡Œéšæœºç­–ç•¥æ¼”ç¤ºè®­ç»ƒ ({reward_type})...")
    
    if use_enhanced_reward:
        env = create_enhanced_env({'max_episode_steps': 500})
        print("âœ¨ ä½¿ç”¨å¢å¼ºå¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    else:
        env = create_env({'max_episode_steps': 500})
        print("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¥–åŠ±ç³»ç»Ÿè¿›è¡Œæ¼”ç¤º")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # éšæœºåŠ¨ä½œ
            action = env.action_space.sample()
            action[2] = int(action[2])  # åŠ¨ä½œç±»å‹ä¸ºæ•´æ•°
            
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    if (episode + 1) % 10 == 0:  # æ¯10ä¸ªepisodeæ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                        print(f"  Episode {episode + 1} ç»“æŸ - æœ€ç»ˆåˆ†æ•°: {final_score:.2f}, "
                              f"åˆ†æ•°å˜åŒ–: {score_delta:+.2f}, æ€»å¥–åŠ±: {total_reward:.3f}")
                        
                        # å¦‚æœä½¿ç”¨å¢å¼ºå¥–åŠ±ï¼Œæ˜¾ç¤ºå¥–åŠ±ç»„ä»¶ä¿¡æ¯
                        if use_enhanced_reward and hasattr(env, 'reward_components_history') and env.reward_components_history:
                            latest_components = env.reward_components_history[-1]['components']
                            if latest_components:
                                print(f"    å¥–åŠ±ç»„ä»¶: ", end="")
                                for comp, val in latest_components.items():
                                    if abs(val) > 0.001:
                                        print(f"{comp}={val:.3f} ", end="")
                                print()
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards)
    title = f'Episode Rewards ({reward_type})'
    plt.title(title)
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(episode_lengths)
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    
    plt.tight_layout()
    filename = f'random_training_results_{reward_type.lower().replace(" ", "_")}.png'
    plt.savefig(filename)
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° {filename}")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-20:]):.3f}")
    
    if use_enhanced_reward:
        print("ğŸ’¡ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›äº†æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·")

def evaluate_model(model_path, episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½å’Œè¯„ä¼°æ¨¡å‹")
        return
    
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    if 'PPO' in model_path:
        model = PPO.load(model_path)
    elif 'DQN' in model_path:
        model = DQN.load(model_path)
    elif 'A2C' in model_path:
        model = A2C.load(model_path)
    else:
        print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env({'max_episode_steps': 1000})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                # æ˜¾ç¤ºepisodeç»“æŸä¿¡æ¯
                if 'final_score' in info:
                    final_score = info['final_score']
                    score_delta = info.get('score_delta', 0)
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}, "
                          f"æœ€ç»ˆåˆ†æ•°={final_score:.2f}, åˆ†æ•°å˜åŒ–={score_delta:+.2f}")
                else:
                    print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}")
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¤– GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ (æ”¯æŒå¢å¼ºå¥–åŠ±)")
    print("ğŸ”¥ Geminiä¼˜åŒ–ç‰ˆæœ¬ - è§£å†³ç­–ç•¥å´©å¡Œé—®é¢˜")
    print("=" * 60)
    
    # æ˜¾ç¤ºGeminiä¼˜åŒ–ä¿¡æ¯
    if not RICH_AVAILABLE:
        print("ğŸ¯ Geminiåˆ†æä¼˜åŒ–äº®ç‚¹:")
        print("  âœ… é™ä½n_epochs (20â†’6-8) - é˜²æ­¢è¿‡æ‹Ÿåˆå¯¼è‡´çš„ç­–ç•¥å´©å¡Œ")
        print("  âœ… æé«˜ent_coef (0.005â†’0.01+) - å¢åŠ ç­–ç•¥å¤šæ ·æ€§å’Œæ¢ç´¢")
        print("  âœ… äº‹ä»¶é©±åŠ¨å¥–åŠ± - å¤§å¹…å¥–åŠ±Split/Ejecté«˜çº§åŠ¨ä½œ")
        print("  âœ… å¹³è¡¡clip_range - æé«˜å­¦ä¹ çµæ´»æ€§")
        print("  ğŸ”¥ ç›®æ ‡ï¼šæ™ºèƒ½ä½“å­¦ä¼šå¤šæ ·åŒ–ç­–ç•¥ï¼Œä½¿ç”¨æ‰€æœ‰åŠ¨ä½œç±»å‹")
        print()
    else:
        console = Console()
        optimization_panel = Panel(
            "[bold green]ğŸ”¥ Gemini AI æ·±åº¦åˆ†æä¼˜åŒ–[/bold green]\n\n" +
            "[yellow]ğŸ¯ ç­–ç•¥å´©å¡Œé—®é¢˜è¯Šæ–­[/yellow]:\n" +
            "  â€¢ è¶…é«˜n_epochs(20)å¯¼è‡´ä¸¥é‡è¿‡æ‹Ÿåˆ\n" +
            "  â€¢ ä½ent_coef(0.005)æŠ‘åˆ¶æ¢ç´¢å¤šæ ·æ€§\n" +
            "  â€¢ æ™ºèƒ½ä½“åªä¼šå•ä¸€æ–¹å‘ç§»åŠ¨+ä»ä¸Split/Eject\n\n" +
            "[yellow]âœ¨ ä¼˜åŒ–æªæ–½[/yellow]:\n" +
            "  â€¢ [green]é™ä½n_epochs(20â†’6-8)[/green] - é˜²æ­¢è¿‡æ‹Ÿåˆ\n" +
            "  â€¢ [green]æé«˜ent_coef(0.005â†’0.01+)[/green] - å¢åŠ ç­–ç•¥å¤šæ ·æ€§\n" +
            "  â€¢ [green]äº‹ä»¶é©±åŠ¨å¥–åŠ±[/green] - é‡å¥–Split(+2.0)/Eject(+1.5)åŠ¨ä½œ\n" +
            "  â€¢ [green]å¹³è¡¡clip_range[/green] - æé«˜å­¦ä¹ çµæ´»æ€§\n\n" +
            "[bold cyan]ğŸ¯ é¢„æœŸæ•ˆæœ[/bold cyan]: æ™ºèƒ½ä½“å­¦ä¼šå¤šæ ·åŒ–ç­–ç•¥ï¼Œä¸»åŠ¨ä½¿ç”¨æ‰€æœ‰åŠ¨ä½œç±»å‹",
            title="Gemini AI ä¼˜åŒ–ç‰ˆæœ¬",
            border_style="red"
        )
        console.print(optimization_panel)
        print()
    
    if not RICH_AVAILABLE:
        print("ğŸ’¡ å»ºè®®å®‰è£… rich åº“è·å¾—æ›´å¥½çš„è®­ç»ƒç•Œé¢: pip install rich")
        print("   å½“å‰ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬ç•Œé¢")
        print()
    else:
        console = Console()
        welcome_panel = Panel(
            "[bold green]ğŸ‰ æ¬¢è¿ä½¿ç”¨ GoBigger RL è®­ç»ƒå™¨[/bold green]\n\n" +
            "[yellow]âœ¨ Richç•Œé¢æ”¯æŒ[/yellow]: ç¾åŒ–ç•Œé¢ã€è¿›åº¦æ¡ã€å®æ—¶ç»Ÿè®¡\n" +
            "[yellow]ğŸ“Š TensorBoardé›†æˆ[/yellow]: è¯¦ç»†è®­ç»ƒæ›²çº¿ç›‘æ§\n" +
            "[yellow]ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿ[/yellow]: å¤šç»´åº¦å¯†é›†å¥–åŠ±ä¿¡å·\n" +
            "[yellow]ğŸš€ å¤šç®—æ³•æ”¯æŒ[/yellow]: PPOã€DQNã€A2C\n" +
            "[yellow]â° é•¿æ—¶é—´è®­ç»ƒä¼˜åŒ–[/yellow]: è‡ªåŠ¨ä¿å­˜ã€å­¦ä¹ ç‡è¡°å‡ã€ç½‘ç»œä¼˜åŒ–",
            title="åŠŸèƒ½ç‰¹è‰²",
            border_style="green"
        )
        console.print(welcome_panel)
        print()
    
    # è®­ç»ƒé…ç½®
    config = {
        'max_episode_steps': 1500,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    
    if STABLE_BASELINES_AVAILABLE:
        print("ğŸ¯ é€‰æ‹©è®­ç»ƒæ¨¡å¼ (Geminiä¼˜åŒ–ç‰ˆæœ¬):")
        print("1. PPO + æ ‡å‡†å¥–åŠ± - ç»å…¸å¼ºåŒ–å­¦ä¹  + äº‹ä»¶é©±åŠ¨å¥–åŠ±")
        print("2. PPO + å¢å¼ºå¥–åŠ± - å¯†é›†å¥–åŠ±ä¿¡å· + Split/Ejectæ¿€åŠ± (æ¨è)")
        print("3. ğŸŒ™ PPO + å¢å¼ºå¥–åŠ± - é•¿æ—¶é—´è®­ç»ƒ (2Mæ­¥, ä¸€æ•´æ™š)")
        print("4. ğŸŒŸ PPO + å¢å¼ºå¥–åŠ± - è¶…é•¿æ—¶é—´è®­ç»ƒ (4Mæ­¥, å‘¨æœ«è®­ç»ƒ)")
        print("5. DQN + æ ‡å‡†å¥–åŠ±")
        print("6. DQN + å¢å¼ºå¥–åŠ±")
        print("7. A2C + æ ‡å‡†å¥–åŠ±")
        print("8. A2C + å¢å¼ºå¥–åŠ±") 
        print("9. è¯„ä¼°ç°æœ‰æ¨¡å‹")
        print("10. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("11. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        print("")
        print("ğŸ’¡ Geminiä¼˜åŒ–è¯´æ˜:")
        print("   - æ‰€æœ‰PPOæ¨¡å¼å‡å·²ä¼˜åŒ–è¶…å‚æ•°ï¼Œé˜²æ­¢ç­–ç•¥å´©å¡Œ")
        print("   - å¢åŠ äº†Split(+2.0)å’ŒEject(+1.5)åŠ¨ä½œçš„äº‹ä»¶å¥–åŠ±")
        print("   - æé«˜æ¢ç´¢å¤šæ ·æ€§ï¼Œé¿å…å•ä¸€æ–¹å‘ç§»åŠ¨")
        
        choice = input("\nè¯·é€‰æ‹© (1-11): ").strip()
        
        if choice == '1':
            model = train_with_stable_baselines3('PPO', total_timesteps=1000000, config=config, use_enhanced_reward=False)
        elif choice == '2':
            model = train_with_stable_baselines3('PPO', total_timesteps=1000000, config=config, use_enhanced_reward=True)
        elif choice == '3':
            print("ğŸŒ™ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼ - é€‚åˆä¸€æ•´æ™šè®­ç»ƒ")
            print("   - è®­ç»ƒæ­¥æ•°: 2,000,000 (çº¦8-12å°æ—¶)")
            print("   - ç½‘ç»œç»“æ„: æ·±åº¦ä¼˜åŒ– [256, 256, 128]")
            print("   - å­¦ä¹ ç‡è¡°å‡: å¯ç”¨")
            print("   - è‡ªåŠ¨ä¿å­˜: æ¯10Kæ­¥") 
            print("   - æ£€æŸ¥ç‚¹: æ¯50Kæ­¥")
            confirm = input("ç¡®è®¤å¼€å§‹é•¿æ—¶é—´è®­ç»ƒ? (y/N): ").strip().lower()
            if confirm == 'y':
                model = train_with_stable_baselines3('PPO', total_timesteps=2000000, config=config, use_enhanced_reward=True)
            else:
                print("âŒ å–æ¶ˆè®­ç»ƒ")
                return
        elif choice == '4':
            print("ğŸŒŸ è¶…é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼ - é€‚åˆå‘¨æœ«è®­ç»ƒ")
            print("   - è®­ç»ƒæ­¥æ•°: 4,000,000 (çº¦20-30å°æ—¶)")
            print("   - ç½‘ç»œç»“æ„: æ·±åº¦ä¼˜åŒ– [512, 512, 256, 128]")
            print("   - å­¦ä¹ ç‡è¡°å‡: å¯ç”¨ (æ›´ä¿å®ˆçš„è¡°å‡)")
            print("   - è‡ªåŠ¨ä¿å­˜: æ¯25Kæ­¥")
            print("   - æ£€æŸ¥ç‚¹: æ¯100Kæ­¥")
            print("   - è¶…å‚æ•°: ä¸“ä¸ºé•¿æœŸè®­ç»ƒä¼˜åŒ–")
            print("   âš ï¸  è¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ—¶é—´å’Œè®¡ç®—èµ„æº")
            confirm = input("ç¡®è®¤å¼€å§‹è¶…é•¿æ—¶é—´è®­ç»ƒ? (y/N): ").strip().lower()
            if confirm == 'y':
                model = train_with_stable_baselines3('PPO', total_timesteps=4000000, config=config, use_enhanced_reward=True)
            else:
                print("âŒ å–æ¶ˆè®­ç»ƒ")
                return
        elif choice == '5':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '6':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '7':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=False)
        elif choice == '8':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config, use_enhanced_reward=True)
        elif choice == '9':
            model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
            if os.path.exists(model_path):
                evaluate_model(model_path)
            else:
                print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        elif choice == '10':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '11':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    else:
        print("ğŸ¯ é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. éšæœºç­–ç•¥æ¼”ç¤º (æ ‡å‡†å¥–åŠ±)")
        print("2. éšæœºç­–ç•¥æ¼”ç¤º (å¢å¼ºå¥–åŠ±)")
        
        choice = input("\nè¯·é€‰æ‹© (1-2): ").strip()
        
        if choice == '1':
            simple_random_training(episodes=50, use_enhanced_reward=False)
        elif choice == '2':
            simple_random_training(episodes=50, use_enhanced_reward=True)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ é•¿æ—¶é—´è®­ç»ƒæç¤ºï¼š")
    print("  - ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ›²çº¿: tensorboard --logdir ./tensorboard_logs")
    print("  - ğŸŒ TensorBoardè®¿é—®: http://localhost:6006")
    print("  - ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: ./models/")
    print("  - ğŸ’¾ æ£€æŸ¥ç‚¹ä½ç½®: ./checkpoints/")
    print("  - ğŸ”„ è‡ªåŠ¨ä¿å­˜: æ¯10Kæ­¥è‡ªåŠ¨ä¿å­˜ï¼Œé˜²æ­¢æ„å¤–ä¸¢å¤±")
    print("  - ğŸ“ˆ å­¦ä¹ ç‡è¡°å‡: é•¿æ—¶é—´è®­ç»ƒä¼šè‡ªåŠ¨ä¼˜åŒ–å­¦ä¹ ç‡")
    print("  - ğŸ¯ å¢å¼ºå¥–åŠ±ç³»ç»Ÿæä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œæ¨èç”¨äºæ–°è®­ç»ƒ")
    print("  - ğŸŒ™ é•¿æ—¶é—´è®­ç»ƒæ¨¡å¼é€‚åˆä¸€æ•´æ™šè®­ç»ƒï¼Œè·å¾—æ›´å¥½çš„æ€§èƒ½")
    if not RICH_AVAILABLE:
        print("  - âœ¨ å»ºè®®å®‰è£… rich åº“äº«å—æ›´å¥½çš„è®­ç»ƒç•Œé¢ä½“éªŒ: pip install rich")

if __name__ == "__main__":
    main()
