#!/usr/bin/env python3
"""
ä½¿ç”¨GoBiggeræ ¸å¿ƒå¼•æ“è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
æ”¯æŒå¤šç§RLç®—æ³•ï¼šPPOã€DQNã€A2Cç­‰
"""
import sys
import os
from pathlib import Path
import numpy as np
import time
import json
from collections import deque
import matplotlib.pyplot as plt

# è·¯å¾„è®¾ç½®ï¼šå®šä½åˆ°é¡¹ç›®æ ¹ç›®å½•
root_dir = Path(__file__).parent.parent
build_dir = root_dir / "build" / "Release"
sys.path.insert(0, str(build_dir))
os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"

import gobigger_env
from gobigger_gym_env import GoBiggerEnv

try:
    # å°è¯•å¯¼å…¥stable-baselines3 (å¦‚æœå·²å®‰è£…)
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.callbacks import EvalCallback, BaseCallback
    from stable_baselines3.common.logger import configure
    STABLE_BASELINES_AVAILABLE = True
    print("âœ… æ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ä¸“ä¸šRLç®—æ³•")
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("âš ï¸  æœªæ£€æµ‹åˆ° stable-baselines3ï¼Œå°†ä½¿ç”¨ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤º")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install stable-baselines3[extra]")

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒ"""
    
    def __init__(self, eval_freq=1000, save_freq=5000, verbose=1):
        super().__init__(verbose)
        self.eval_freq = eval_freq
        self.save_freq = save_freq
        self.best_mean_reward = -np.inf
        self.episode_rewards = deque(maxlen=100)
        
    def _on_step(self) -> bool:
        # æ”¶é›†å¥–åŠ±ç»Ÿè®¡
        if 'episode' in self.locals['infos'][0]:
            episode_reward = self.locals['infos'][0]['episode']['r']
            self.episode_rewards.append(episode_reward)
            
        # å®šæœŸè¯„ä¼°å’Œä¿å­˜
        if self.num_timesteps % self.eval_freq == 0:
            if len(self.episode_rewards) > 0:
                mean_reward = np.mean(self.episode_rewards)
                if self.verbose > 0:
                    print(f"Steps: {self.num_timesteps}, Mean reward (last 100 eps): {mean_reward:.2f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    if self.verbose > 0:
                        print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.save_freq == 0:
            model_path = f"checkpoints/model_{self.num_timesteps}_steps.zip"
            self.model.save(model_path)
            if self.verbose > 0:
                print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹: {model_path}")
        
        return True

def create_env(config=None):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
    default_config = {
        'max_episode_steps': 2000,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    if config:
        default_config.update(config)
    
    return GoBiggerEnv(default_config)

def train_with_stable_baselines3(algorithm='PPO', total_timesteps=100000, config=None):
    """ä½¿ç”¨stable-baselines3è®­ç»ƒæ™ºèƒ½ä½“"""
    print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {algorithm} ç®—æ³•è®­ç»ƒ...")
    
    # åˆ›å»ºç¯å¢ƒ
    env = make_vec_env(lambda: create_env(config), n_envs=1, vec_env_cls=DummyVecEnv)
    
    # åˆ›å»ºæ¨¡å‹
    if algorithm == 'PPO':
        model = PPO(
            "MlpPolicy", 
            env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'DQN':
        model = DQN(
            "MlpPolicy",
            env,
            learning_rate=1e-4,
            buffer_size=50000,
            learning_starts=1000,
            batch_size=32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            gradient_steps=1,
            target_update_interval=1000,
            verbose=1,
            tensorboard_log="./tensorboard_logs/"
        )
    elif algorithm == 'A2C':
        model = A2C(
            "MlpPolicy",
            env,
            learning_rate=7e-4,
            n_steps=5,
            gamma=0.99,
            gae_lambda=1.0,
            ent_coef=0.01,
            vf_coef=0.25,
            max_grad_norm=0.5,
            verbose=1,
            tensorboard_log="./tensorboard_logs/"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
    
    # åˆ›å»ºå›è°ƒ
    os.makedirs("checkpoints", exist_ok=True)
    callback = TrainingCallback(eval_freq=2000, save_freq=10000)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"ğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {total_timesteps}")
    start_time = time.time()
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        tb_log_name=f"{algorithm}_gobigger"
    )
    
    train_time = time.time() - start_time
    print(f"âœ… è®­ç»ƒå®Œæˆï¼ç”¨æ—¶: {train_time:.2f}ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f"models/{algorithm}_gobigger_final.zip"
    os.makedirs("models", exist_ok=True)
    model.save(final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    return model

def simple_random_training(episodes=100):
    """ç®€å•çš„éšæœºç­–ç•¥æ¼”ç¤ºï¼ˆå½“æ²¡æœ‰stable-baselines3æ—¶ï¼‰"""
    print("ğŸ® è¿è¡Œéšæœºç­–ç•¥æ¼”ç¤ºè®­ç»ƒ...")
    
    env = create_env({'max_episode_steps': 500})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            # éšæœºåŠ¨ä½œ
            action = np.random.uniform(env.action_space_low, env.action_space_high)
            action[2] = int(action[2])  # åŠ¨ä½œç±»å‹ä¸ºæ•´æ•°
            
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_length = np.mean(episode_lengths[-10:])
            print(f"Episode {episode + 1}: å¹³å‡å¥–åŠ±={avg_reward:.3f}, å¹³å‡é•¿åº¦={avg_length:.1f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(episode_rewards)
    plt.title('Episode Rewards')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    
    plt.subplot(1, 2, 2)
    plt.plot(episode_lengths)
    plt.title('Episode Lengths')
    plt.xlabel('Episode')
    plt.ylabel('Steps')
    
    plt.tight_layout()
    plt.savefig('random_training_results.png')
    plt.show()
    
    print(f"ğŸ“Š è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° random_training_results.png")
    print(f"ğŸ“ˆ æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-20:]):.3f}")

def evaluate_model(model_path, episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not STABLE_BASELINES_AVAILABLE:
        print("âŒ éœ€è¦ stable-baselines3 æ¥åŠ è½½å’Œè¯„ä¼°æ¨¡å‹")
        return
    
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    if 'PPO' in model_path:
        model = PPO.load(model_path)
    elif 'DQN' in model_path:
        model = DQN.load(model_path)
    elif 'A2C' in model_path:
        model = A2C.load(model_path)
    else:
        print("âŒ æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹")
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_env({'max_episode_steps': 1000})
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        steps = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        print(f"Episode {episode + 1}: å¥–åŠ±={total_reward:.3f}, æ­¥æ•°={steps}")
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ¤– GoBigger å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨")
    print("=" * 50)
    
    # è®­ç»ƒé…ç½®
    config = {
        'max_episode_steps': 1500,  # æ¯å±€æœ€å¤§æ­¥æ•°
    }
    
    if STABLE_BASELINES_AVAILABLE:
        print("ğŸ¯ é€‰æ‹©è®­ç»ƒç®—æ³•:")
        print("1. PPO (æ¨è) - Proximal Policy Optimization")
        print("2. DQN - Deep Q-Network") 
        print("3. A2C - Advantage Actor-Critic")
        print("4. è¯„ä¼°ç°æœ‰æ¨¡å‹")
        print("5. éšæœºç­–ç•¥æ¼”ç¤º")
        
        choice = input("\nè¯·é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            model = train_with_stable_baselines3('PPO', total_timesteps=50000, config=config)
        elif choice == '2':
            model = train_with_stable_baselines3('DQN', total_timesteps=50000, config=config)
        elif choice == '3':
            model = train_with_stable_baselines3('A2C', total_timesteps=50000, config=config)
        elif choice == '4':
            model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
            if os.path.exists(model_path):
                evaluate_model(model_path)
            else:
                print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        elif choice == '5':
            simple_random_training(episodes=50)
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    else:
        simple_random_training(episodes=50)
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ’¡ æç¤ºï¼š")
    print("  - ä½¿ç”¨ tensorboard --logdir ./tensorboard_logs æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    print("  - æ¨¡å‹ä¿å­˜åœ¨ ./models/ ç›®å½•")
    print("  - æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ ./checkpoints/ ç›®å½•")

if __name__ == "__main__":
    main()
