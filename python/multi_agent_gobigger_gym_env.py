#!/usr/bin/env python3
"""
Multi-Agent GoBigger Gymnasium Environment
æ”¯æŒRLæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIæœºå™¨äººæ··åˆè®­ç»ƒçš„å¤šæ™ºèƒ½ä½“ç¯å¢ƒ
"""
import sys
import os
import time
from pathlib import Path
import numpy as np
from typing import Dict, Any, List, Tuple, Optional

# æ™ºèƒ½è·¯å¾„æœç´¢ï¼šå¯»æ‰¾æœ‰æ•ˆçš„æ„å»ºç›®å½•
root_dir = Path(__file__).parent.parent
build_dir_candidates = [
    root_dir / "build" / "Release",
    root_dir / "build" / "Debug", 
    root_dir / "build",
    root_dir / "build-msvc" / "Release",
    root_dir / "build-msvc" / "Debug",
]

build_dir_found = None
for build_dir in build_dir_candidates:
    if build_dir.exists() and any(build_dir.iterdir()):
        sys.path.insert(0, str(build_dir))
        os.environ["PATH"] = f"{str(build_dir)};{os.environ['PATH']}"
        build_dir_found = build_dir
        print(f"âœ… æ‰¾åˆ°æ„å»ºç›®å½•: {build_dir}")
        break

if not build_dir_found:
    raise ImportError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ„å»ºç›®å½•ã€‚è¯·ç¡®ä¿é¡¹ç›®å·²ç¼–è¯‘ã€‚")

try:
    import gobigger_env
    print("âœ… æˆåŠŸå¯¼å…¥ gobigger_env æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ gobigger_env å¤±è´¥: {e}")
    print(f"æœç´¢è·¯å¾„: {build_dir_found}")
    raise

try:
    import gymnasium as gym
    from gymnasium import spaces
    GYMNASIUM_AVAILABLE = True
    print("âœ… ä½¿ç”¨ gymnasium åº“")
except ImportError:
    try:
        import gym
        from gym import spaces
        GYMNASIUM_AVAILABLE = False
        print("âœ… ä½¿ç”¨ gym åº“")
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… gymnasium æˆ– gym")
        raise

class MultiAgentGoBiggerEnv(gym.Env if GYMNASIUM_AVAILABLE else gym.Env):
    """
    å¤šæ™ºèƒ½ä½“GoBiggerç¯å¢ƒ
    
    è¿™ä¸ªç¯å¢ƒæ”¯æŒä¸€ä¸ªRLæ™ºèƒ½ä½“ä¸å¤šä¸ªä¼ ç»ŸAIç­–ç•¥æœºå™¨äººå¯¹æˆ˜ã€‚
    ä¼ ç»ŸAIä½¿ç”¨æ‚¨åœ¨src_newä¸­å·²å®ç°çš„ç­–ç•¥ï¼ˆFOOD_HUNTER, AGGRESSIVE, RANDOMï¼‰ï¼Œ
    è€ŒRLæ™ºèƒ½ä½“é€šè¿‡æ ‡å‡†çš„Gymnasiumæ¥å£è¿›è¡Œè®­ç»ƒã€‚
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒå›¢é˜Ÿæ’åä½œä¸ºå¥–åŠ±ä¿¡å·
    - ä¼ ç»ŸAIæä¾›ç¨³å®šçš„å¯¹æ‰‹
    - å¤šæ™ºèƒ½ä½“ç¯å¢ƒå¢åŠ è®­ç»ƒå¤æ‚åº¦
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“ç¯å¢ƒ"""
        super().__init__()
        
        # ç¯å¢ƒé…ç½®
        self.config = config or {}
        
        # åˆ›å»ºC++å¤šæ™ºèƒ½ä½“å¼•æ“é…ç½®
        engine_config = gobigger_env.MultiAgentConfig()
        engine_config.maxFoodCount = self.config.get('max_food_count', 3000)
        engine_config.initFoodCount = self.config.get('init_food_count', 2500)
        engine_config.maxThornsCount = self.config.get('max_thorns_count', 12)
        engine_config.initThornsCount = self.config.get('init_thorns_count', 9)
        engine_config.maxFrames = self.config.get('max_frames', 3000)
        engine_config.aiOpponentCount = self.config.get('ai_opponent_count', 3)
        engine_config.gameUpdateInterval = self.config.get('update_interval', 16)
        
        # åˆ›å»ºC++å¼•æ“
        self.engine = gobigger_env.MultiAgentGameEngine(engine_config)
        
        # å®šä¹‰RLæ™ºèƒ½ä½“çš„åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´
        # åŠ¨ä½œ: [move_x, move_y, action_type]
        # move_x, move_y: [-1.0, 1.0] ç§»åŠ¨æ–¹å‘
        # action_type: 0=ç§»åŠ¨, 1=åˆ†è£‚, 2=å–·å°„
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0, 0], dtype=np.float32),
            high=np.array([1.0, 1.0, 2], dtype=np.float32),
            dtype=np.float32
        )
        
        # è§‚å¯Ÿç©ºé—´ï¼šç‰¹å¾å‘é‡ + å›¢é˜Ÿæ’åä¿¡æ¯
        self.observation_space_shape = (450,)  # å¢åŠ ç©ºé—´ä»¥åŒ…å«æ’åä¿¡æ¯
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=self.observation_space_shape,
            dtype=np.float32
        )
        
        # ç¯å¢ƒçŠ¶æ€
        self.current_obs = None
        self.episode_step = 0
        self.max_episode_steps = self.config.get('max_episode_steps', 3000)
        
        # å¥–åŠ±è¿½è¸ª
        self.last_score = 0.0
        self.initial_score = 0.0
        self.last_team_rank = 1
        self.episode_reward = 0.0
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = self.config.get('debug_mode', False)
        
        print(f"ğŸ® å¤šæ™ºèƒ½ä½“ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   AIå¯¹æ‰‹æ•°é‡: {engine_config.aiOpponentCount}")
        print(f"   æœ€å¤§å¸§æ•°: {engine_config.maxFrames}")
        print(f"   è§‚å¯Ÿç©ºé—´: {self.observation_space.shape}")
        print(f"   åŠ¨ä½œç©ºé—´: {self.action_space.shape}")
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None):
        """é‡ç½®ç¯å¢ƒ"""
        if seed is not None:
            np.random.seed(seed)
        
        # é‡ç½®C++å¼•æ“
        self.current_obs = self.engine.reset()
        self.episode_step = 0
        self.episode_reward = 0.0
        
        # åˆå§‹åŒ–å¥–åŠ±è¿½è¸ª
        reward_info = self.engine.get_reward_info()
        self.last_score = reward_info.get('score', 0.0)
        self.initial_score = self.last_score
        self.last_team_rank = reward_info.get('team_rank', 1)
        
        if self.debug_mode:
            print(f"ğŸ”„ ç¯å¢ƒé‡ç½® - åˆå§‹åˆ†æ•°: {self.initial_score}, åˆå§‹æ’å: {self.last_team_rank}")
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_features(), {}
        else:
            return self._extract_features()
    
    def step(self, action):
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        # ç¡®ä¿åŠ¨ä½œæ ¼å¼æ­£ç¡®å¹¶è¿›è¡Œè£å‰ª
        if isinstance(action, (list, tuple, np.ndarray)):
            if len(action) >= 3:
                move_x = float(np.clip(action[0], -1.0, 1.0))
                move_y = float(np.clip(action[1], -1.0, 1.0))
                action_type = int(np.round(np.clip(action[2], 0, 2)))
                rl_action = [move_x, move_y, action_type]
            else:
                rl_action = [0.0, 0.0, 0]
        else:
            rl_action = [0.0, 0.0, 0]
        
        # æ„é€ å¤šæ™ºèƒ½ä½“åŠ¨ä½œå­—å…¸
        actions_dict = {"rl_agent": rl_action}
        
        # æ‰§è¡ŒåŠ¨ä½œ
        self.current_obs = self.engine.step(actions_dict)
        self.episode_step += 1
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_multi_agent_reward()
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        terminated = self.engine.is_done()
        truncated = self.episode_step >= self.max_episode_steps
        
        # ç´¯ç§¯å¥–åŠ±
        self.episode_reward += reward
        
        # å‡†å¤‡ä¿¡æ¯å­—å…¸
        info = self._prepare_info_dict(terminated or truncated)
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym
        if GYMNASIUM_AVAILABLE:
            return self._extract_features(), reward, terminated, truncated, info
        else:
            return self._extract_features(), reward, terminated or truncated, info
    
    def _extract_features(self) -> np.ndarray:
        """ä»å¤šæ™ºèƒ½ä½“è§‚å¯Ÿä¸­æå–ç‰¹å¾å‘é‡"""
        if not self.current_obs:
            return np.zeros(self.observation_space_shape, dtype=np.float32)
        
        features = []
        
        # 1. å…¨å±€çŠ¶æ€ç‰¹å¾ (10ç»´)
        global_state = self.current_obs.get('global_state', {})
        features.extend([
            global_state.get('frame', 0) / self.max_episode_steps,
            global_state.get('total_players', 0) / 10.0,
            global_state.get('food_count', 0) / 3000.0,
            global_state.get('thorns_count', 0) / 15.0,
        ])
        features.extend([0.0] * 6)  # é¢„ç•™6ä¸ªå…¨å±€ç‰¹å¾
        
        # 2. RLæ™ºèƒ½ä½“çŠ¶æ€ç‰¹å¾ (20ç»´)
        rl_obs = self.current_obs.get('rl_agent', {})
        if rl_obs:
            # ä½ç½®å’ŒåŸºç¡€å±æ€§
            pos = rl_obs.get('position', (0, 0))
            vel = rl_obs.get('velocity', (0, 0))
            features.extend([
                pos[0] / 2000.0,  # å½’ä¸€åŒ–ä½ç½®
                pos[1] / 2000.0,
                rl_obs.get('radius', 0) / 100.0,
                rl_obs.get('score', 0) / 10000.0,
                vel[0] / 1000.0,  # å½’ä¸€åŒ–é€Ÿåº¦
                vel[1] / 1000.0,
                float(rl_obs.get('can_split', False)),
                float(rl_obs.get('can_eject', False)),
            ])
            features.extend([0.0] * 12)  # é¢„ç•™12ä¸ªRLç‰¹å¾
        else:
            features.extend([0.0] * 20)  # RLæ™ºèƒ½ä½“æ­»äº¡æ—¶å¡«0
        
        # 3. å›¢é˜Ÿæ’åç‰¹å¾ (20ç»´)
        team_ranking = global_state.get('team_ranking', [])
        ranking_features = [0.0] * 20
        
        for i, team_info in enumerate(team_ranking[:5]):  # æœ€å¤š5ä¸ªé˜Ÿä¼
            base_idx = i * 4
            if base_idx + 3 < len(ranking_features):
                ranking_features[base_idx] = team_info.get('team_id', 0) / 10.0
                ranking_features[base_idx + 1] = team_info.get('score', 0) / 10000.0
                ranking_features[base_idx + 2] = team_info.get('rank', 1) / 5.0
                ranking_features[base_idx + 3] = 1.0 if team_info.get('team_id', -1) == 0 else 0.0  # æ˜¯å¦æ˜¯RLé˜Ÿä¼
        
        features.extend(ranking_features)
        
        # 4. è§†é‡å†…é£Ÿç‰©ç‰¹å¾ (200ç»´: 50ä¸ªé£Ÿç‰© Ã— 4ç»´)
        nearby_food = rl_obs.get('nearby_food', [])
        food_features = []
        for i in range(50):
            if i < len(nearby_food):
                food = nearby_food[i]
                food_features.extend([
                    food[0] / 2000.0,  # x
                    food[1] / 2000.0,  # y
                    food[2] / 10.0,    # radius
                    food[3] / 100.0,   # score
                ])
            else:
                food_features.extend([0.0, 0.0, 0.0, 0.0])
        features.extend(food_features)
        
        # 5. è§†é‡å†…å…¶ä»–ç©å®¶ç‰¹å¾ (200ç»´: 20ä¸ªç©å®¶ Ã— 10ç»´)
        nearby_players = rl_obs.get('nearby_players', [])
        player_features = []
        for i in range(20):
            if i < len(nearby_players):
                player = nearby_players[i]
                player_features.extend([
                    player[0] / 2000.0,  # x
                    player[1] / 2000.0,  # y
                    player[2] / 100.0,   # radius
                    player[3] / 10000.0, # score
                    player[4] / 10.0,    # team_id
                    player[5] / 10.0,    # player_id
                ])
                player_features.extend([0.0] * 4)  # é¢„ç•™4ç»´
            else:
                player_features.extend([0.0] * 10)
        features.extend(player_features)
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        target_length = self.observation_space_shape[0]
        if len(features) > target_length:
            features = features[:target_length]
        elif len(features) < target_length:
            features.extend([0.0] * (target_length - len(features)))
        
        return np.array(features, dtype=np.float32)
    
    def _calculate_multi_agent_reward(self) -> float:
        """è®¡ç®—å¤šæ™ºèƒ½ä½“å¥–åŠ±"""
        reward_info = self.engine.get_reward_info()
        
        # 1. åˆ†æ•°å¢é•¿å¥–åŠ±
        current_score = reward_info.get('score', 0.0)
        score_delta = current_score - self.last_score
        score_reward = score_delta / 100.0
        
        # 2. å›¢é˜Ÿæ’åå¥–åŠ±ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        current_rank = reward_info.get('team_rank', 1)
        total_teams = reward_info.get('total_teams', 1)
        
        # æ’åå¥–åŠ±ï¼šæ’åè¶Šé«˜å¥–åŠ±è¶Šå¤§
        rank_reward = (total_teams - current_rank + 1) / total_teams * 0.5
        
        # æ’åå˜åŒ–å¥–åŠ±ï¼šæ’åæå‡ç»™äºˆé¢å¤–å¥–åŠ±
        rank_change_reward = 0.0
        if current_rank < self.last_team_rank:
            rank_change_reward = 2.0  # æ’åæå‡å¥–åŠ±
        elif current_rank > self.last_team_rank:
            rank_change_reward = -1.0  # æ’åä¸‹é™æƒ©ç½š
        
        # 3. ç”Ÿå­˜å¥–åŠ±
        is_alive = reward_info.get('alive', False)
        survival_reward = 0.01 if is_alive else -10.0
        
        # 4. æ—¶é—´æƒ©ç½šï¼ˆé¼“åŠ±å¿«é€Ÿå†³ç­–ï¼‰
        time_penalty = -0.001
        
        # 5. é«˜çº§åŠ¨ä½œå¥–åŠ±ï¼ˆä¿æŒåŸæœ‰çš„Split/Ejectå¥–åŠ±æœºåˆ¶ï¼‰
        advanced_action_reward = 0.0
        # è¿™é‡Œå¯ä»¥æ ¹æ®actionç±»å‹æ·»åŠ å¥–åŠ±ï¼Œä½†éœ€è¦ä»C++ç«¯ä¼ é€’æ›´å¤šä¿¡æ¯
        
        # æ€»å¥–åŠ±
        total_reward = (score_reward + rank_reward + rank_change_reward + 
                       survival_reward + time_penalty + advanced_action_reward)
        
        # æ›´æ–°çŠ¶æ€
        self.last_score = current_score
        self.last_team_rank = current_rank
        
        if self.debug_mode and self.episode_step % 100 == 0:
            print(f"ğŸ¯ Step {self.episode_step}: "
                  f"Score={current_score:.1f}(Î”{score_delta:+.1f}), "
                  f"Rank={current_rank}/{total_teams}, "
                  f"Reward={total_reward:.3f}")
        
        return total_reward
    
    def _prepare_info_dict(self, episode_done: bool) -> Dict[str, Any]:
        """å‡†å¤‡ä¿¡æ¯å­—å…¸"""
        info = {}
        
        if episode_done:
            reward_info = self.engine.get_reward_info()
            final_score = reward_info.get('score', 0.0)
            final_rank = reward_info.get('team_rank', 1)
            total_teams = reward_info.get('total_teams', 1)
            
            # Monitoræ ‡å‡†æ ¼å¼
            info['episode'] = {
                'r': self.episode_reward,
                'l': self.episode_step,
                't': time.time()
            }
            
            # å¤šæ™ºèƒ½ä½“ç‰¹å®šä¿¡æ¯
            info['final_score'] = final_score
            info['score_delta'] = final_score - self.initial_score
            info['final_rank'] = final_rank
            info['total_teams'] = total_teams
            info['rank_progress'] = (total_teams - final_rank + 1) / total_teams
            
            # AIå¯¹æ‰‹çŠ¶æ€
            ai_states = self.current_obs.get('ai_states', {})
            info['ai_opponents_alive'] = len([s for s in ai_states.values() 
                                            if s.get('alive_balls_count', 0) > 0])
            
            if self.debug_mode:
                print(f"ğŸ Episodeç»“æŸ:")
                print(f"   æœ€ç»ˆåˆ†æ•°: {final_score:.1f} (å¢é•¿: {final_score - self.initial_score:+.1f})")
                print(f"   æœ€ç»ˆæ’å: {final_rank}/{total_teams}")
                print(f"   Episodeå¥–åŠ±: {self.episode_reward:.3f}")
                print(f"   å­˜æ´»AIå¯¹æ‰‹: {info['ai_opponents_alive']}")
        
        return info
    
    def render(self, mode: str = 'human'):
        """æ¸²æŸ“ç¯å¢ƒ"""
        if mode == 'human' and self.current_obs:
            global_state = self.current_obs.get('global_state', {})
            rl_obs = self.current_obs.get('rl_agent', {})
            
            print(f"Frame: {global_state.get('frame', 0)}")
            if rl_obs:
                print(f"RL Agent - Score: {rl_obs.get('score', 0):.1f}, "
                      f"Pos: ({rl_obs.get('position', (0, 0))[0]:.0f}, "
                      f"{rl_obs.get('position', (0, 0))[1]:.0f})")
            
            # æ˜¾ç¤ºå›¢é˜Ÿæ’å
            team_ranking = global_state.get('team_ranking', [])
            print("Team Ranking:")
            for i, team in enumerate(team_ranking):
                marker = "ğŸ‘‘" if team.get('team_id') == 0 else "ğŸ¤–"
                print(f"  {i+1}. {marker} Team {team.get('team_id', 0)}: {team.get('score', 0):.1f}")
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        print("ğŸ”š å¤šæ™ºèƒ½ä½“ç¯å¢ƒå…³é—­")

def demo_multi_agent_usage():
    """æ¼”ç¤ºå¤šæ™ºèƒ½ä½“ç¯å¢ƒä½¿ç”¨"""
    print("\n" + "="*60)
    print("ğŸ® å¤šæ™ºèƒ½ä½“ GoBigger ç¯å¢ƒæ¼”ç¤º")
    print("="*60 + "\n")
    
    # åˆ›å»ºç¯å¢ƒ
    config = {
        'max_episode_steps': 200,
        'ai_opponent_count': 3,
        'debug_mode': True,
        'max_frames': 1000
    }
    env = MultiAgentGoBiggerEnv(config)
    
    # æ£€æŸ¥ç¯å¢ƒï¼ˆå¦‚æœå®‰è£…äº†stable-baselines3ï¼‰
    try:
        from stable_baselines3.common.env_checker import check_env
        print("ğŸ” ä½¿ç”¨ stable-baselines3 æ£€æŸ¥ç¯å¢ƒ...")
        check_env(env)
        print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼\n")
    except ImportError:
        print("âš ï¸ æœªå®‰è£… stable-baselines3ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥\n")
    
    # é‡ç½®ç¯å¢ƒ
    if GYMNASIUM_AVAILABLE:
        obs, info = env.reset()
    else:
        obs = env.reset()
        info = {}
    
    print(f"ğŸ ç¯å¢ƒé‡ç½®å®Œæˆ")
    print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"   è§‚å¯Ÿç»´åº¦: {obs.shape}")
    print()
    
    # è¿è¡Œå‡ æ­¥
    total_reward = 0
    for step in range(10):
        # éšæœºåŠ¨ä½œ
        action = env.action_space.sample()
        
        if GYMNASIUM_AVAILABLE:
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        else:
            obs, reward, done, info = env.step(action)
            terminated, truncated = done, False
        
        total_reward += reward
        
        print(f"æ­¥éª¤ {step+1:02d}: "
              f"åŠ¨ä½œ=[{action[0]:.2f}, {action[1]:.2f}, {int(np.round(action[2]))}], "
              f"å¥–åŠ±={reward:.4f}, "
              f"ç´¯è®¡å¥–åŠ±={total_reward:.4f}, "
              f"ç»“æŸ={done}")
        
        if step % 3 == 0:  # æ¯3æ­¥æ¸²æŸ“ä¸€æ¬¡
            env.render()
            print()
        
        if done:
            print(f"\nğŸ å›åˆç»“æŸ (ç¬¬{step+1}æ­¥)")
            if 'final_rank' in info:
                print(f"   æœ€ç»ˆæ’å: {info['final_rank']}/{info.get('total_teams', 1)}")
            break
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print(f"   æœ€ç»ˆç´¯è®¡å¥–åŠ±: {total_reward:.4f}")
    print(f"   å¹³å‡æ­¥éª¤å¥–åŠ±: {total_reward/(step+1):.4f}")
    
    env.close()

if __name__ == "__main__":
    demo_multi_agent_usage()
