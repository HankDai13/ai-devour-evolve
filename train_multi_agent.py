#!/usr/bin/env python3
"""
å¤šæ™ºèƒ½ä½“GoBiggerå¼ºåŒ–å­¦ä¹ è®­ç»ƒç¤ºä¾‹
ä½¿ç”¨PPOç®—æ³•è®­ç»ƒRLæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAI ROBOTå¯¹æˆ˜
"""
import sys
import os
from pathlib import Path

# æ·»åŠ scriptsç›®å½•åˆ°è·¯å¾„
scripts_dir = Path(__file__).parent / "scripts"
sys.path.insert(0, str(scripts_dir))

try:
    from gobigger_gym_env import MultiAgentGoBiggerEnv, MULTI_AGENT_AVAILABLE
except ImportError as e:
    print(f"âŒ å¯¼å…¥ç¯å¢ƒå¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²è¿è¡Œ build_multi_agent.bat ç¼–è¯‘å¤šæ™ºèƒ½ä½“æ¨¡å—")
    sys.exit(1)

# æ£€æŸ¥å¤šæ™ºèƒ½ä½“æ”¯æŒ
if not MULTI_AGENT_AVAILABLE:
    print("âŒ å¤šæ™ºèƒ½ä½“åŠŸèƒ½ä¸å¯ç”¨")
    print("è¯·è¿è¡Œ build_multi_agent.bat ç¼–è¯‘å¤šæ™ºèƒ½ä½“æ¨¡å—")
    sys.exit(1)

def setup_qt_environment():
    """Setup Qt environment variables for proper plugin loading"""
    qt_dir = r"D:\qt\6.9.1\msvc2022_64"
    
    # Set Qt environment variables
    os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = os.path.join(qt_dir, "plugins", "platforms")
    os.environ['QT_PLUGIN_PATH'] = os.path.join(qt_dir, "plugins")
    
    # Add Qt bin to PATH
    qt_bin = os.path.join(qt_dir, "bin")
    if qt_bin not in os.environ.get('PATH', ''):
        os.environ['PATH'] = qt_bin + os.pathsep + os.environ.get('PATH', '')
    
    print(f"Qt environment configured:")
    print(f"  QT_QPA_PLATFORM_PLUGIN_PATH = {os.environ.get('QT_QPA_PLATFORM_PLUGIN_PATH')}")
    print(f"  QT_PLUGIN_PATH = {os.environ.get('QT_PLUGIN_PATH')}")
    print()

# Setup Qt environment before importing any Qt-dependent modules
setup_qt_environment()

def train_multi_agent_ppo():
    """ä½¿ç”¨PPOè®­ç»ƒå¤šæ™ºèƒ½ä½“ç¯å¢ƒ"""
    print("ğŸš€ å¯åŠ¨å¤šæ™ºèƒ½ä½“PPOè®­ç»ƒ")
    
    try:
        from stable_baselines3 import PPO
        from stable_baselines3.common.env_util import make_vec_env
        from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
        import torch
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… stable-baselines3")
        print("è¿è¡Œ: pip install stable-baselines3[extra]")
        return
    
    # ç¯å¢ƒé…ç½®
    env_config = {
        'max_episode_steps': 2000,      # æ›´é•¿çš„episode
        'ai_opponent_count': 3,         # 3ä¸ªAIå¯¹æ‰‹
        'max_frames': 4000,             # æ›´å¤šå¸§æ•°
        'ai_strategies': ['FOOD_HUNTER', 'AGGRESSIVE', 'RANDOM'],
        'max_food_count': 3000,
        'init_food_count': 2500,
    }
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    def make_env():
        return MultiAgentGoBiggerEnv(env_config)
    
    # ä½¿ç”¨å¤šä¸ªå¹¶è¡Œç¯å¢ƒåŠ é€Ÿè®­ç»ƒ
    n_envs = 4
    env = make_vec_env(make_env, n_envs=n_envs)
    
    print(f"âœ… åˆ›å»ºäº† {n_envs} ä¸ªå¹¶è¡Œå¤šæ™ºèƒ½ä½“ç¯å¢ƒ")
    print(f"   æ¯ä¸ªç¯å¢ƒåŒ…å« 1ä¸ªRLæ™ºèƒ½ä½“ vs {env_config['ai_opponent_count']}ä¸ªä¼ ç»ŸAI")
    
    # PPOé…ç½®ï¼ˆé’ˆå¯¹å¤šæ™ºèƒ½ä½“ä¼˜åŒ–ï¼‰
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        # ç½‘ç»œæ¶æ„
        policy_kwargs=dict(
            net_arch=dict(pi=[512, 512, 256], vf=[512, 512, 256]),
            activation_fn=torch.nn.ReLU,
        ),
        # å­¦ä¹ å‚æ•°
        learning_rate=3e-4,
        n_steps=2048,           # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        batch_size=256,         # æ‰¹æ¬¡å¤§å°
        n_epochs=10,            # æ¯æ¬¡æ›´æ–°çš„epochæ•°
        gamma=0.99,             # æŠ˜æ‰£å› å­
        gae_lambda=0.95,        # GAE lambda
        clip_range=0.2,         # PPO clip range
        ent_coef=0.01,          # ç†µç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        vf_coef=0.5,            # ä»·å€¼å‡½æ•°ç³»æ•°
        max_grad_norm=0.5,      # æ¢¯åº¦è£å‰ª
        # è®¾å¤‡
        device='cuda' if torch.cuda.is_available() else 'cpu',
    )
    
    print(f"âœ… PPOæ¨¡å‹åˆ›å»ºå®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {model.device}")
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,  # æ¯10kæ­¥ä¿å­˜ä¸€æ¬¡
        save_path='./models/multi_agent_checkpoints/',
        name_prefix='multi_agent_ppo'
    )
    
    # è¯„ä¼°å›è°ƒ
    eval_env = make_env()
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./models/multi_agent_best/',
        log_path='./logs/multi_agent_eval/',
        eval_freq=5000,    # æ¯5kæ­¥è¯„ä¼°ä¸€æ¬¡
        deterministic=True,
        render=False
    )
    
    # åˆ›å»ºç›®å½•
    os.makedirs('./models/multi_agent_checkpoints/', exist_ok=True)
    os.makedirs('./models/multi_agent_best/', exist_ok=True)
    os.makedirs('./logs/multi_agent_eval/', exist_ok=True)
    
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"   æ€»è®­ç»ƒæ­¥æ•°: 100,000")
    print(f"   å¹¶è¡Œç¯å¢ƒæ•°: {n_envs}")
    print(f"   æ¯ä¸ªç¯å¢ƒçš„AIå¯¹æ‰‹: {env_config['ai_opponent_count']}")
    print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {env.observation_space.shape}")
    print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=100000,  # æ€»è®­ç»ƒæ­¥æ•°
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save("./models/multi_agent_ppo_final")
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")
    
    # æ¸…ç†ç¯å¢ƒ
    env.close()
    eval_env.close()
    
    return model

def test_trained_model():
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„å¤šæ™ºèƒ½ä½“æ¨¡å‹")
    
    try:
        from stable_baselines3 import PPO
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… stable-baselines3")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = "./models/multi_agent_ppo_final.zip"
    if not os.path.exists(model_path):
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒ")
        return
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    print(f"âœ… åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    env_config = {
        'max_episode_steps': 1000,
        'ai_opponent_count': 3,
        'ai_strategies': ['FOOD_HUNTER', 'AGGRESSIVE', 'RANDOM'],
        'debug_mode': True,  # å¯ç”¨è°ƒè¯•æ¨¡å¼
    }
    env = MultiAgentGoBiggerEnv(env_config)
    
    # è¿è¡Œæµ‹è¯•episodes
    n_test_episodes = 5
    total_rewards = []
    final_ranks = []
    
    for episode in range(n_test_episodes):
        print(f"\nğŸ® æµ‹è¯•Episode {episode + 1}/{n_test_episodes}")
        
        obs = env.reset()
        episode_reward = 0
        step = 0
        
        while True:
            # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward
            step += 1
            
            # æ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
            if step % 50 == 0:
                env.render()
            
            if done:
                total_rewards.append(episode_reward)
                final_rank = info.get('final_rank', 1)
                final_ranks.append(final_rank)
                
                print(f"ğŸ“Š Episode {episode + 1} ç»“æœ:")
                print(f"   æ€»å¥–åŠ±: {episode_reward:.2f}")
                print(f"   æœ€ç»ˆæ’å: {final_rank}/{info.get('total_teams', 1)}")
                print(f"   åˆ†æ•°å˜åŒ–: {info.get('score_delta', 0):+.1f}")
                print(f"   å­˜æ´»AI: {info.get('ai_opponents_alive', 0)}")
                break
    
    # ç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡ ({n_test_episodes} episodes):")
    print(f"   å¹³å‡å¥–åŠ±: {sum(total_rewards)/len(total_rewards):.2f}")
    print(f"   å¹³å‡æ’å: {sum(final_ranks)/len(final_ranks):.1f}")
    print(f"   æœ€ä½³æ’å: {min(final_ranks)}")
    print(f"   æ’å1çš„æ¬¡æ•°: {final_ranks.count(1)}/{n_test_episodes}")
    
    env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– å¤šæ™ºèƒ½ä½“GoBiggerå¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("="*50)
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--train":
            train_multi_agent_ppo()
        elif sys.argv[1] == "--test":
            test_trained_model()
        else:
            print("âŒ æœªçŸ¥å‚æ•°")
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python train_multi_agent.py --train   # è®­ç»ƒæ¨¡å‹")
            print("  python train_multi_agent.py --test    # æµ‹è¯•æ¨¡å‹")
    else:
        print("ğŸ¯ ä½¿ç”¨æ–¹æ³•:")
        print("  python train_multi_agent.py --train   # è®­ç»ƒRLæ™ºèƒ½ä½“ä¸ä¼ ç»ŸAIå¯¹æˆ˜")
        print("  python train_multi_agent.py --test    # æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹")
        print()
        print("ğŸ¤– å¤šæ™ºèƒ½ä½“ç‰¹ç‚¹:")
        print("  â€¢ RLæ™ºèƒ½ä½“ vs 3ä¸ªä¼ ç»ŸAI ROBOT")
        print("  â€¢ AIç­–ç•¥: FOOD_HUNTER, AGGRESSIVE, RANDOM")  
        print("  â€¢ å›¢é˜Ÿæ’åä½œä¸ºå¥–åŠ±ä¿¡å·")
        print("  â€¢ å¢å¼ºçš„è§‚å¯Ÿç©ºé—´ï¼ˆ450ç»´ï¼‰")
        print("  â€¢ æ”¯æŒå¹¶è¡Œè®­ç»ƒåŠ é€Ÿ")

if __name__ == "__main__":
    main()
